+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames della-j16g1,della-j17g1,della-k11g1,della-k12g3,della-k13g1,della-k15g3,della-k16g2,della-k18g1
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=4 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames della-j16g1,della-j17g1,della-k11g1,della-k12g3,della-k13g1,della-k15g3,della-k16g2,della-k18g1
++ scontrol show hostnames della-j16g1,della-j17g1,della-k11g1,della-k12g3,della-k13g1,della-k15g3,della-k16g2,della-k18g1
++ scontrol show hostnames della-j16g1,della-j17g1,della-k11g1,della-k12g3,della-k13g1,della-k15g3,della-k16g2,della-k18g1
++ scontrol show hostnames della-j16g1,della-j17g1,della-k11g1,della-k12g3,della-k13g1,della-k15g3,della-k16g2,della-k18g1
++ head -n 1
++ head -n 1
++ head -n 1
++ head -n 1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=3 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
++ scontrol show hostnames della-j16g1,della-j17g1,della-k11g1,della-k12g3,della-k13g1,della-k15g3,della-k16g2,della-k18g1
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=2 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=7 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=5 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames della-j16g1,della-j17g1,della-k11g1,della-k12g3,della-k13g1,della-k15g3,della-k16g2,della-k18g1
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=1 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=6 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames della-j16g1,della-j17g1,della-k11g1,della-k12g3,della-k13g1,della-k15g3,della-k16g2,della-k18g1
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=0 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
W1003 22:46:21.569000 3290636 site-packages/torch/distributed/run.py:774] 
W1003 22:46:21.569000 3290636 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 3290636 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 22:46:21.569000 3290636 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 3055446 site-packages/torch/distributed/run.py:774] 
W1003 22:46:21.569000 3055446 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 3055446 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 22:46:21.569000 3055446 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 3207214 site-packages/torch/distributed/run.py:774] 
W1003 22:46:21.569000 3207214 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 3207214 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 22:46:21.569000 3207214 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 3876934 site-packages/torch/distributed/run.py:774] 
W1003 22:46:21.569000 3876934 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 3876934 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 22:46:21.569000 3876934 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 1409120 site-packages/torch/distributed/run.py:774] 
W1003 22:46:21.569000 1409120 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 1409120 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 22:46:21.569000 1409120 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 1912487 site-packages/torch/distributed/run.py:774] 
W1003 22:46:21.569000 1912487 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 1912487 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 22:46:21.569000 1912487 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 1345197 site-packages/torch/distributed/run.py:774] 
W1003 22:46:21.569000 1345197 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 1345197 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 22:46:21.569000 1345197 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 228818 site-packages/torch/distributed/run.py:774] 
W1003 22:46:21.569000 228818 site-packages/torch/distributed/run.py:774] *****************************************
W1003 22:46:21.569000 228818 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 22:46:21.569000 228818 site-packages/torch/distributed/run.py:774] *****************************************
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,513 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,513 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,513 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,513 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,513 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,513 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,516 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,516 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,516 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,516 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,516 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,516 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,518 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,518 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,518 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,518 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,518 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,518 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,520 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,520 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,520 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,520 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,520 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,520 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,523 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,523 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,523 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,523 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,523 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,523 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,523 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,523 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,523 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,523 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,523 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,523 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,524 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,525 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,525 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,525 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,525 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,525 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,525 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,525 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,525 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,525 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,525 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:05,525 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,151 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 22:47:06,152 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,157 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 22:47:06,158 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 22:47:06,158 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,159 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,159 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,159 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,159 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,159 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,159 >> loading file chat_template.jinja
[INFO|configuration_utils.py:839] 2025-10-03 22:47:06,163 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,164 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,164 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,164 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,164 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,164 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,164 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,186 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 22:47:06,186 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 22:47:06,189 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,189 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,189 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,189 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,189 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,189 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,189 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,198 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,198 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 22:47:06,198 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 22:47:06,199 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 22:47:06,201 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,202 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,202 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,202 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,202 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,202 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,202 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,202 >> loading file chat_template.jinja
[INFO|configuration_utils.py:763] 2025-10-03 22:47:06,202 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,203 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 22:47:06,203 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 22:47:06,204 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,204 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,204 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,204 >> loading file added_tokens.json
[INFO|configuration_utils.py:839] 2025-10-03 22:47:06,204 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,204 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,204 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,204 >> loading file chat_template.jinja
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,205 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,205 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,205 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,205 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,205 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,205 >> loading file chat_template.jinja
[INFO|configuration_utils.py:839] 2025-10-03 22:47:06,208 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,208 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,208 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,208 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,208 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,208 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,208 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,218 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 22:47:06,219 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 22:47:06,221 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,222 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,222 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,222 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,222 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,222 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 22:47:06,222 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,801 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,804 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,875 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,902 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,906 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,910 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,912 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 22:47:06,919 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 354/186688 [00:01<09:51, 315.28 examples/s]Converting format of dataset (num_proc=64):  13%|█▎        | 24927/186688 [00:01<00:05, 28010.79 examples/s]Converting format of dataset (num_proc=64):  26%|██▌       | 48253/186688 [00:01<00:02, 56362.86 examples/s]Converting format of dataset (num_proc=64):  38%|███▊      | 71655/186688 [00:01<00:01, 85697.41 examples/s]Converting format of dataset (num_proc=64):  49%|████▉     | 92065/186688 [00:01<00:00, 107530.32 examples/s]Converting format of dataset (num_proc=64):  60%|██████    | 112226/186688 [00:01<00:00, 124900.75 examples/s]Converting format of dataset (num_proc=64):  71%|███████   | 132570/186688 [00:01<00:00, 142601.28 examples/s]Converting format of dataset (num_proc=64):  82%|████████▏ | 152477/186688 [00:01<00:00, 1540Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 400/186688 [00:01<09:17, 333.87 examples/s]Converting format of dataset (num_proc=64):  12%|█▏        | 23218/186688 [00:01<00:06, 24520.15 examples/s]Converting format of dataset (num_proc=64):  24%|██▍       | 45510/186688 [00:01<00:02, 50594.26 examples/s]Converting format of dataset (num_proc=64):  37%|███▋      | 69220/186688 [00:01<00:01, 80097.14 examples/s]Converting format of dataset (num_proc=64):  49%|████▊     | 90863/186688 [00:01<00:00, 104992.16 examples/s]Converting format of dataset (num_proc=64):  60%|█████▉    | 111286/186688 [00:01<00:00, 122567.00 examples/s]Converting format of dataset (num_proc=64):  70%|███████   | 131099/186688 [00:01<00:00, 136946.08 examples/s]Converting format of dataset (num_proc=64):  81%|████████  | 150308/186688 [00:01<00:00, 147387Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 424/186688 [00:01<09:05, 341.39 examples/s]Converting format of dataset (num_proc=64):  13%|█▎        | 24424/186688 [00:01<00:06, 24976.92 examples/s]Converting format of dataset (num_proc=64):  25%|██▌       | 47252/186688 [00:01<00:02, 50722.93 examples/s]Converting format of dataset (num_proc=64):  39%|███▉      | 72710/186688 [00:01<00:01, 81998.16 examples/s]Converting format of dataset (num_proc=64):  50%|█████     | 93437/186688 [00:01<00:00, 104554.12 examples/s]Converting format of dataset (num_proc=64):  61%|██████    | 114333/186688 [00:01<00:00, 120356.33 examples/s]Converting format of dataset (num_proc=64):  72%|███████▏  | 133885/186688 [00:01<00:00, 135339.40 examples/s]Converting format of dataset (num_proc=64):  82%|████████▏ | 153172/186688 [00:01<00:00, 14Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 311/186688 [00:01<12:54, 240.70 examples/s]Converting format of dataset (num_proc=64):  11%|█         | 19896/186688 [00:01<00:08, 19666.73 examples/s]Converting format of dataset (num_proc=64):  23%|██▎       | 43490/186688 [00:01<00:03, 46366.51 examples/s]Converting format of dataset (num_proc=64):  36%|███▋      | 67679/186688 [00:01<00:01, 75483.71 examples/s]Converting format of dataset (num_proc=64):  48%|████▊     | 90513/186688 [00:01<00:00, 101942.66 examples/s]Converting format of dataset (num_proc=64):  60%|█████▉    | 111164/186688 [00:01<00:00, 121458.33 examples/s]Converting format of dataset (num_proc=64):  70%|███████   | 131443/186688 [00:01<00:00, 132188.49 examples/s]Converting format of dataset (num_proc=64):  81%|████████  | 151115/186688 [00:02<00:00, 146702.7Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 490/186688 [00:01<08:52, 349.90 examples/s]Converting format of dataset (num_proc=64):  14%|█▍        | 26636/186688 [00:01<00:06, 24438.01 examples/s]Converting format of dataset (num_proc=64):  26%|██▌       | 48276/186688 [00:01<00:02, 46734.06 examples/s]Converting format of dataset (num_proc=64):  39%|███▊      | 71880/186688 [00:01<00:01, 73551.19 examples/s]Converting format of dataset (num_proc=64):  54%|█████▎    | 100180/186688 [00:01<00:00, 108876.38 examples/s]Converting format of dataset (num_proc=64):  66%|██████▌   | 123148/186688 [00:01<00:00, 130401.20 examples/s]Converting format of dataset (num_proc=64):  78%|███████▊  | 145761/186688 [00:02<00:00, 147094.01 examples/s]Converting format of dataset (num_proc=64):  90%|████████▉ | 167640/186688 [00:02<00:0Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 532/186688 [00:01<08:24, 368.67 examples/s]Converting format of dataset (num_proc=64):  11%|█         | 20697/186688 [00:01<00:08, 18499.49 examples/s]Converting format of dataset (num_proc=64):  22%|██▏       | 41878/186688 [00:01<00:03, 40424.65 examples/s]Converting format of dataset (num_proc=64):  39%|███▊      | 72152/186688 [00:01<00:01, 77048.23 examples/s]Converting format of dataset (num_proc=64):  50%|█████     | 93965/186688 [00:01<00:00, 96136.51 examples/s]Converting format of dataset (num_proc=64):  63%|██████▎   | 117362/186688 [00:01<00:00, 120792.34 examples/s]Converting format of dataset (num_proc=64):  75%|███████▍  | 139642/186688 [00:02<00:00, 141528.41 examples/s]Converting format of dataset (num_proc=64):  86%|████████▌ | 160917/186688 [00:02<00:00, 147Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 288/186688 [00:01<16:08, 192.42 examples/s]Converting format of dataset (num_proc=64):  12%|█▏        | 21586/186688 [00:01<00:08, 18712.85 examples/s]Converting format of dataset (num_proc=64):  23%|██▎       | 42724/186688 [00:01<00:03, 39948.87 examples/s]Converting format of dataset (num_proc=64):  34%|███▍      | 63726/186688 [00:01<00:01, 63022.05 examples/s]Converting format of dataset (num_proc=64):  48%|████▊     | 88946/186688 [00:01<00:01, 93511.17 examples/s]Converting format of dataset (num_proc=64):  59%|█████▊    | 109499/186688 [00:02<00:00, 110070.42 examples/s]Converting format of dataset (num_proc=64):  69%|██████▉   | 128779/186688 [00:02<00:00, 126894.59 examples/s]Converting format of dataset (num_proc=64):  81%|████████  | 151494/186688 [00:02<00:00, 149551.Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 546/186688 [00:01<08:11, 379.07 examples/s]Converting format of dataset (num_proc=64):  10%|█         | 18995/186688 [00:01<00:09, 16933.35 examples/s]Converting format of dataset (num_proc=64):  22%|██▏       | 41446/186688 [00:01<00:03, 40430.86 examples/s]Converting format of dataset (num_proc=64):  39%|███▉      | 73235/186688 [00:01<00:01, 78953.37 examples/s]Converting format of dataset (num_proc=64):  51%|█████     | 95172/186688 [00:01<00:00, 97739.51 examples/s]Converting format of dataset (num_proc=64):  62%|██████▏   | 115506/186688 [00:01<00:00, 116625.99 examples/s]Converting format of dataset (num_proc=64):  73%|███████▎  | 136650/186688 [00:02<00:00, 135481.75 examples/s]Converting format of dataset (num_proc=64):  84%|████████▍ | 156853/186688 [00:03<00:00, 42881.89 examples/s]Converting format of dataset (num_proc=64):  92%|█████████▏| 172128/186688 [00:02<00:00, 130682.44 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:03<00:00, 55953.64 examples/s] 
.59 examples/s]Converting format of dataset (num_proc=64):  91%|█████████ | 169203/186688 [00:02<00:00, 138298.47 examples/s]Converting format of dataset (num_proc=64): 100%|█████████▉| 185909/186688 [00:02<00:00, 77933.08 examples/s] Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:03<00:00, 51267.17 examples/s]
09 examples/s]Converting format of dataset (num_proc=64):  92%|█████████▏| 172174/186688 [00:02<00:00, 144766.92 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:03<00:00, 46736.68 examples/s] 
8 examples/s]Converting format of dataset (num_proc=64):  91%|█████████ | 170318/186688 [00:02<00:00, 134175.93 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 46492.41 examples/s] 
0, 140018.11 examples/s]Converting format of dataset (num_proc=64): 100%|█████████▉| 186517/186688 [00:02<00:00, 84550.43 examples/s] Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 46271.99 examples/s]
8343.46 examples/s]Converting format of dataset (num_proc=64):  92%|█████████▏| 172392/186688 [00:02<00:00, 122120.95 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 45384.52 examples/s] 
763.28 examples/s]Converting format of dataset (num_proc=64):  97%|█████████▋| 180843/186688 [00:02<00:00, 104622.16 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 44750.61 examples/s] 
54.48 examples/s] Converting format of dataset (num_proc=64):  92%|█████████▏| 171291/186688 [00:03<00:00, 38523.44 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 44500.83 examples/s]
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:12<39:51, 77.65 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:13<17:41, 173.93 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:13<09:52, 309.84 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:14<04:38, 652.32 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:14<02:45, 1085.11 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▍         | 8000/186688 [00:15<02:25, 1228.66 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:15<02:20, 1265.44 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:16<02:25, 1210.44 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▌         | 11Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<42:03, 73.59 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<18:04, 170.24 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:14<10:25, 293.56 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 6000/186688 [00:15<03:48, 790.38 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:15<03:12, 932.28 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▍         | 8000/186688 [00:15<02:44, 1083.43 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:16<02:33, 1154.98 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▌         | 11000/186688 [00:17<01:44, 1683.15 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 120Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<41:43, 74.17 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<18:51, 163.27 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:14<07:24, 410.98 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:15<05:32, 546.71 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 6000/186688 [00:16<04:29, 669.77 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:16<03:32, 844.93 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▍         | 8000/186688 [00:17<02:52, 1034.37 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:17<01:51, 1579.98 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 1200Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:14<45:06, 68.61 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:15<19:25, 158.48 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:15<11:08, 274.81 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:16<07:19, 415.64 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:16<05:12, 581.80 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▍         | 8000/186688 [00:16<02:14, 1329.95 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▌         | 11000/186688 [00:17<01:23, 2108.67 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:17<01:00, 2855.17 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<40:50, 75.79 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:13<11:03, 276.98 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:14<05:46, 524.64 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:15<03:54, 764.94 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:15<02:37, 1130.95 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:16<01:40, 1738.59 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 13000/186688 [00:16<01:34, 1843.53 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:17<01:46, 1617.80 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<42:54, 72.12 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:14<12:02, 254.18 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:15<06:27, 468.99 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:15<03:54, 766.35 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▍         | 8000/186688 [00:16<03:17, 904.45 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:16<02:44, 1081.29 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:17<02:11, 1348.72 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:17<01:26, 2019.46 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<42:40, 72.52 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<18:22, 167.53 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:15<11:21, 269.64 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<07:27, 407.84 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 6000/186688 [00:16<04:10, 721.20 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:17<01:51, 1585.81 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 13000/186688 [00:17<01:18, 2217.87 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:18<01:00, 2838.89 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 2Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<43:08, 71.73 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<18:35, 165.50 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<07:49, 389.50 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:15<03:34, 839.36 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:16<02:08, 1373.48 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 13000/186688 [00:16<01:27, 1984.18 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:18<01:08, 2463.58 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 20000/186688 [00:18<01:02, 2664.00 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         |00/186688 [00:17<01:39, 1761.73 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:17<01:08, 2537.90 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         | 15000/186688 [00:18<01:08, 2491.95 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:18<01:14, 2300.56 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         | 17000/186688 [00:19<01:16, 2231.82 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:19<01:00, 2792.91 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         | 19000/186688 [00:21<01:54, 1468.52 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 20000/186688 [00:21<01:28, 1889.73 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:21<01:11, 2302.74 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:21<00:55, 2965.69 000/186688 [00:18<00:42, 3923.98 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         | 19000/186688 [00:18<00:48, 3471.44 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:19<00:39, 4131.56 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:19<00:46, 3511.48 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:20<00:38, 4179.26 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:20<00:38, 4153.27 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:20<00:38, 4122.71 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:21<00:37, 4134.29 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:21<00:44, 3502.86 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:22<13000/186688 [00:18<02:00, 1446.71 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:19<01:52, 1536.99 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         | 15000/186688 [00:19<01:38, 1741.48 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:20<01:40, 1692.36 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         | 17000/186688 [00:20<01:17, 2190.33 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:20<01:12, 2329.24 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         | 19000/186688 [00:21<01:06, 2532.88 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 20000/186688 [00:21<01:27, 1900.69 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:22<01:08, 2424.11 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:22<01:01, 2673.0/186688 [00:18<01:24, 2077.73 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:18<01:04, 2677.23 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         | 17000/186688 [00:18<00:47, 3542.77 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:19<00:35, 4661.31 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:20<00:42, 3849.91 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:21<00:41, 3902.71 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:21<00:38, 4105.73 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:21<00:25, 6097.43 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:23<00:56, 2734.18 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:23<00:4000/186688 [00:17<02:18, 1264.22 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:18<02:30, 1159.63 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:18<01:34, 1820.42 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:19<01:07, 2520.02 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:19<01:06, 2548.01 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         | 19000/186688 [00:20<01:07, 2492.19 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 20000/186688 [00:22<02:14, 1236.06 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:23<02:06, 1308.17 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:23<01:43, 1592.93 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:23<01:24, 1928.| 15000/186688 [00:18<02:07, 1351.35 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:19<02:01, 1405.92 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         | 17000/186688 [00:20<02:10, 1301.65 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         | 19000/186688 [00:20<01:24, 1983.96 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 20000/186688 [00:21<01:23, 1999.01 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:21<01:25, 1942.22 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:22<01:26, 1908.68 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:22<01:32, 1769.94 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:23<01:37, 1661.00 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:23<01:2examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:21<00:53, 3049.44 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:21<00:43, 3746.33 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:22<00:39, 4107.30 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:22<00:33, 4824.45 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:22<00:29, 5430.74 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:22<00:34, 4546.00 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:23<00:33, 4626.88 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:23<00:35, 4335.20 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:24<01:12, 2111.87 examples/s]Runn 21000/186688 [00:19<01:04, 2562.53 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:19<00:49, 3268.24 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:20<01:05, 2465.79 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:21<00:57, 2798.35 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:21<00:50, 3092.21 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:22<00:39, 3893.23 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:24<01:15, 2024.45 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:24<01:23, 1826.40 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:25<01:14, 2037.14 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00000/186688 [00:18<00:43, 3815.45 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:19<00:49, 3367.72 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:19<00:41, 3931.62 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:20<00:41, 3905.40 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:20<00:46, 3402.80 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:21<00:48, 3203.46 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:22<01:05, 2384.59 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:24<01:47, 1441.17 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:25<01:31, 1670.94 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:2670 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:22<00:56, 2904.44 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:22<00:50, 3217.79 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:23<01:00, 2693.28 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:23<00:42, 3744.96 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:24<01:00, 2644.15 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:24<00:32, 4771.29 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:25<00:44, 3488.91 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:26<00:46, 3239.61 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:26<00:44, 3384.05 examples/s]R55 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:23<01:13, 2226.97 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:24<01:03, 2541.06 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:24<00:49, 3221.08 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:25<00:57, 2780.80 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:26<01:16, 2052.10 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:26<01:04, 2441.49 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:26<00:59, 2624.86 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:27<01:12, 2132.81 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:27<00:41, 3646.79 examples/s]R2, 1962.35 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:24<01:08, 2338.99 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:24<00:58, 2697.70 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:25<01:28, 1779.58 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:26<01:20, 1945.43 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:26<01:12, 2147.70 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:26<01:05, 2350.20 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:27<00:59, 2596.34 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:27<01:05, 2328.10 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:28<01:37, 1561.56 examping tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:25<01:07, 2254.34 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:27<01:35, 1577.21 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:27<01:17, 1924.77 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:27<01:09, 2151.59 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:28<01:34, 1568.99 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:29<01:34, 1551.23 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:29<01:22, 1773.63 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:29<01:10, 2043.31 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:31<01:43, 1388.45 examples/s]Running toke<02:01, 1258.25 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:26<01:45, 1435.19 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:28<02:14, 1121.59 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:29<02:15, 1108.60 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:29<01:51, 1339.07 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:29<01:06, 2205.39 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:30<01:02, 2313.21 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:31<01:25, 1700.42 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:31<00:51, 2747.17 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:31<00:409, 3093.94 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:24<01:10, 2156.85 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:25<01:23, 1815.33 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:28<02:25, 1026.90 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:29<02:25, 1022.27 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:29<01:53, 1302.14 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:29<01:38, 1495.20 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:29<01:17, 1876.15 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:31<02:09, 1120.71 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:31<01:39, 1450.00:39, 3931.36 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:22<00:34, 4452.88 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:25<01:59, 1273.92 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:25<01:30, 1655.55 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:28<02:47, 886.05 examples/s] Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:30<02:49, 869.64 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:31<02:38, 925.92 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:31<02:07, 1142.94 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:31<01:42, 1412.47 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:32<01:38, 1460:26<01:31, 1644.78 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:26<01:21, 1838.04 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:29<02:42, 912.70 examples/s] Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:29<01:37, 1500.42 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:29<01:33, 1560.50 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:30<01:28, 1640.19 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:30<01:25, 1687.24 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:32<01:42, 1387.69 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<01:25, 1662.11 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:unning tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:26<00:46, 3225.61 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:27<01:18, 1891.03 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:28<01:02, 2351.66 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:28<01:09, 2111.36 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:29<01:06, 2204.76 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:29<01:01, 2349.60 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:29<00:40, 3559.20 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:31<01:31, 1545.31 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:32<01:50, 1276.00 examples/s]Runniunning tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:27<00:36, 4099.31 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:28<00:44, 3373.00 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:29<01:04, 2290.75 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:29<01:00, 2438.44 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:30<01:25, 1696.19 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:31<01:26, 1674.17 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:31<01:19, 1815.94 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:32<01:24, 1680.99 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<01:07, 2090.69 examples/s]Runniles/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:29<01:26, 1735.90 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:30<02:11, 1139.63 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:31<01:59, 1239.68 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:31<01:11, 2065.77 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:32<01:31, 1600.88 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:32<01:10, 2043.72 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:33<01:14, 1925.22 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:34<01:24, 1679.88 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:34<01:05, 2160.54 examples/nizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:31<01:17, 1848.25 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:31<00:44, 3136.33 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:31<00:41, 3381.35 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:32<00:31, 4369.80 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:32<00:41, 3273.26 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:33<00:49, 2700.97 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:34<01:01, 2184.05 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:34<00:48, 2701.17 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:35<00:54, 2380.51 examples/s]Running tong tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:33<00:53, 2640.98 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<00:49, 2831.40 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:33<00:49, 2795.53 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:34<00:53, 2564.49 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:34<00:39, 3397.69 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:34<00:36, 3722.39 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:35<00:38, 3422.05 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:35<00:43, 3026.52 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:36<00:40, 3190.57 examples/s]Run0.57 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:32<01:36, 1474.10 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:33<01:12, 1944.26 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:33<01:18, 1782.88 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<01:08, 2041.29 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:35<01:35, 1454.09 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:35<01:18, 1761.00 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:35<01:01, 2210.83 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:35<00:57, 2364.64 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<00:45, 279 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:32<01:25, 1675.65 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<01:27, 1622.65 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:33<01:11, 1980.41 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:34<01:46, 1316.64 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:34<01:22, 1690.40 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:35<01:34, 1455.96 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:35<01:18, 1735.03 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:36<01:00, 2242.09 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:36<00:56, 238s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:34<00:52, 2702.23 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:35<00:45, 3064.63 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:35<00:39, 3467.47 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:35<00:35, 3896.51 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:35<00:36, 3654.87 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<00:31, 4212.41 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:36<00:41, 3165.54 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:37<00:29, 4378.30 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:37<00:30, 4203.59 example32<01:05, 2153.48 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<01:14, 1881.05 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:34<01:10, 1962.15 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:34<00:56, 2395.05 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:35<01:15, 1772.58 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<01:09, 1924.38 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:36<01:07, 1963.93 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:36<00:53, 2451.82 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:37<00:44, 2945.29 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 , 3474.72 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<01:22, 1686.49 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:34<01:09, 1975.32 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:34<01:06, 2042.46 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:35<00:59, 2279.38 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<01:15, 1780.65 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:36<00:49, 2674.78 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:36<00:48, 2678.22 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:37<00:53, 2437.72 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:37<00:kenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:35<00:54, 2400.39 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:35<00:45, 2855.27 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:35<00:36, 3521.60 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:36<00:33, 3763.10 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:37<01:14, 1698.02 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:37<01:02, 2007.27 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:37<00:52, 2336.00 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:38<00:31, 3877.68 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:38<00:21, 5630.09 examplng tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:32<01:24, 1651.98 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:33<01:48, 1278.48 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:35<02:11, 1046.34 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:35<01:50, 1239.23 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:35<01:04, 2087.77 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<01:09, 1930.50 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:37<01:13, 1813.14 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:37<01:18, 1680.93 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:38<00:50, 2543.28 examples/s]Run0.34 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<00:45, 2922.35 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:37<01:19, 1660.37 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:37<00:59, 2208.18 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:38<00:56, 2331.36 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:38<00:34, 3702.66 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:38<00:30, 4235.59 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:39<00:48, 2621.79 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:39<00:38, 3244.18 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:39<ning tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:36<00:35, 3643.51 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:36<00:51, 2475.08 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:37<00:43, 2914.13 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:37<00:48, 2610.90 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:38<00:43, 2856.50 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:38<00:35, 3437.86 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:39<00:38, 3163.75 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:39<00:52, 2314.17 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:40<00:44, 2708.18946.77 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:36<00:41, 3167.63 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:37<00:49, 2667.43 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:37<01:01, 2113.56 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:38<00:37, 3386.54 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:38<00:31, 4001.16 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:39<00:53, 2355.58 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:39<00:53, 2354.36 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:39<00:50, 2485.88 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00[00:37<00:40, 3127.47 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:37<00:35, 3590.68 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:38<00:26, 4710.81 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:38<00:39, 3124.01 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:38<00:33, 3640.39 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:39<00:32, 3683.17 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:39<00:26, 4548.44 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:40<00:34, 3422.89 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:40<00:31, 3663.82 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉ ning tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:38<00:46, 2754.53 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:38<00:42, 3036.74 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:38<00:34, 3708.97 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:38<00:24, 5018.60 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:39<00:43, 2861.77 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:40<00:47, 2568.03 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:40<00:31, 3801.92 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:40<00:37, 3203.41 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:41<00:33, 3538.s/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:37<00:37, 3362.71 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:38<00:39, 3196.00 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:38<00:45, 2733.96 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:39<00:34, 3558.33 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:39<00:24, 4881.24 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:39<00:38, 3166.73 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:40<00:40, 2985.82 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:41<00:50, 2365.85 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:41<0057, 2253.86 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:38<00:50, 2550.44 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:38<00:58, 2152.78 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:38<00:48, 2592.60 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:39<00:52, 2368.17 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:39<00:49, 2474.91 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:40<00:49, 2459.91 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:40<00:51, 2365.04 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:41<00:43, 2764.28 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 680:40<00:45, 2710.91 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:40<00:37, 3291.57 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:40<00:29, 4061.79 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:40<00:37, 3187.37 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:41<00:41, 2872.24 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:41<00:20, 5615.94 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:41<00:21, 5323.59 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:41<00:20, 5686.26 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:41<00:18, 6136.71 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉    es/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:39<00:31, 3758.25 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:39<00:22, 5074.25 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:39<00:18, 6119.91 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:41<00:44, 2537.58 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:41<00:40, 2735.18 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76917/186688 [00:41<00:41, 2640.93 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 77917/186688 [00:42<00:43, 2509.19 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78917/186688 [00:43<00:54, 1962.30 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80917/186688 [000:40, 3078.89 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:40<00:32, 3772.63 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:40<00:37, 3314.19 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:40<00:27, 4443.61 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:41<00:51, 2315.30 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:41<00:33, 3565.77 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:42<00:30, 3825.64 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:42<00:29, 3934.27 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:43<00:42, 2644.15 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 18 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:41<00:44, 2629.00 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:41<00:36, 3158.73 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:42<00:31, 3652.91 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:42<00:21, 5204.55 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:43<00:25, 4328.51 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:43<00:24, 4543.13 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78000/186688 [00:43<00:28, 3881.22 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 79000/186688 [00:43<00:24, 4443.19 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80000/00/186688 [00:41<00:50, 2345.53 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:42<00:43, 2722.90 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:42<00:44, 2618.83 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:43<00:40, 2855.78 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:43<00:35, 3200.31 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 75917/186688 [00:43<00:20, 5535.32 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76917/186688 [00:43<00:25, 4316.01 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 77917/186688 [00:44<00:30, 3534.13 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78917/186688 [00:44<00:26, 4096.13 examples/s]Running tokenizer on dataset (num_proc=64):  43% examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:40<00:53, 2223.44 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:41<01:06, 1759.93 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:41<00:53, 2166.04 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:42<00:49, 2332.64 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:42<00:31, 3605.68 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:42<00:29, 3802.33 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:43<00:27, 4017.05 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:43<00:30, 3597.67 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78000/186688     | 73000/186688 [00:40<00:23, 4905.25 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:41<00:19, 5780.65 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:41<00:18, 5915.79 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:42<00:30, 3580.11 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78000/186688 [00:42<00:28, 3839.38 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80000/186688 [00:42<00:25, 4172.10 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 81000/186688 [00:44<01:05, 1617.66 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82000/186688 [00:44<00:56, 1851.54 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 83000/186688 [00:45<01:05, 1578.46 examples/s]Running tokenizer on dataset (nu74000/186688 [00:43<00:37, 2976.95 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:43<00:31, 3592.29 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 77834/186688 [00:44<00:24, 4359.61 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 79834/186688 [00:44<00:18, 5883.46 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 81834/186688 [00:44<00:17, 6066.96 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84751/186688 [00:45<00:21, 4852.17 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85751/186688 [00:45<00:24, 4075.95 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▋     | 86751/186688 [00:46<00:22, 4394.83 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87751/186688 [00:46<00:25, 3921.80 examples/s]Running tokenizer on dataset (num_p:45, 2574.58 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:41<00:52, 2221.34 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:42<00:40, 2790.81 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:44<01:00, 1849.73 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:45<01:05, 1697.24 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:45<00:56, 1957.93 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78000/186688 [00:46<00:55, 1951.57 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80000/186688 [00:46<00:46, 2308.75 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 81000/186688 [00:46<00:38, 2712.87 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍ 186688 [00:43<00:21, 4871.21 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 81000/186688 [00:44<00:24, 4261.73 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82000/186688 [00:44<00:35, 2979.34 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 84000/186688 [00:45<00:25, 4025.16 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85000/186688 [00:45<00:24, 4149.29 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 86000/186688 [00:45<00:21, 4690.87 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87000/186688 [00:47<01:02, 1590.41 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 88000/186688 [00:48<01:11, 1381.25 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89000/186688 [00:48<00:55, 1759.97 examples/s]Running tokenizer on dataset (num_proc=0:43<00:41, 2542.50 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 81917/186688 [00:44<00:48, 2164.03 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82917/186688 [00:44<00:40, 2552.44 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83917/186688 [00:44<00:38, 2642.64 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84917/186688 [00:45<00:38, 2654.40 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85834/186688 [00:45<00:47, 2115.28 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 86834/186688 [00:46<00:54, 1825.27 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87834/186688 [00:47<00:58, 1687.50 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 88834/186688 [00:47<00:52, 1875.28 examples/s]Running tokenizer on dataset (num_proc=64):  48%  | 74000/186688 [00:42<00:24, 4615.45 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:42<00:27, 4040.94 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:42<00:21, 5088.95 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80000/186688 [00:43<00:12, 8313.92 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82000/186688 [00:46<00:57, 1815.93 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 83000/186688 [00:46<00:52, 1980.50 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 84000/186688 [00:47<01:11, 1430.41 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85000/186688 [00:48<01:20, 1267.87 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85917/186688 [00:49<01:05, 1536.66 examples/s]Running tokenizer on dataset (num|████▎     | 79917/186688 [00:44<00:22, 4695.21 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80917/186688 [00:44<00:25, 4138.35 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 81917/186688 [00:45<00:43, 2418.60 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82917/186688 [00:46<00:48, 2147.71 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83917/186688 [00:46<00:40, 2518.01 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85917/186688 [00:47<00:35, 2840.25 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 86917/186688 [00:47<00:35, 2795.37 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87834/186688 [00:48<00:38, 2579.81 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89751/186688 [00:49<00:48, 2002.51 examples/s]Running to [00:44<01:03, 1707.14 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 79000/186688 [00:45<00:57, 1861.84 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 81000/186688 [00:47<01:15, 1407.86 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82000/186688 [00:47<01:16, 1375.21 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 83000/186688 [00:48<01:02, 1650.21 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 84000/186688 [00:49<01:09, 1470.42 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 86000/186688 [00:49<00:45, 2226.61 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87000/186688 [00:49<00:46, 2154.95 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 88000/186688 [00:50<00:37, 2629.00 examples/s]Running tokenizer on dataset (num_proc=64):  m_proc=64):  45%|████▌     | 84917/186688 [00:46<00:51, 1988.23 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85834/186688 [00:46<00:50, 1998.70 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87834/186688 [00:47<00:44, 2198.24 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89834/186688 [00:48<00:40, 2405.66 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90751/186688 [00:49<00:55, 1727.27 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92585/186688 [00:49<00:37, 2534.62 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93502/186688 [00:50<00:39, 2377.53 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94502/186688 [00:50<00:38, 2374.91 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95502/186688 [00:50<00:33, 2684.27 examplroc=64):  48%|████▊     | 88751/186688 [00:46<00:26, 3632.99 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89751/186688 [00:48<00:52, 1843.02 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90751/186688 [00:48<00:51, 1879.13 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91751/186688 [00:49<01:08, 1392.68 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92668/186688 [00:50<01:03, 1472.43 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93668/186688 [00:51<01:05, 1412.60 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94668/186688 [00:51<00:52, 1737.80 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96585/186688 [00:51<00:31, 2833.31 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97502/186688 [00:51<00:26, 3377.42 examp    | 82000/186688 [00:47<00:39, 2650.67 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 83000/186688 [00:48<00:55, 1877.35 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 84000/186688 [00:49<01:01, 1674.49 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84917/186688 [00:49<00:50, 2013.11 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85917/186688 [00:49<00:45, 2193.50 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87834/186688 [00:50<00:54, 1802.86 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 88834/186688 [00:51<00:51, 1894.85 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89834/186688 [00:51<00:48, 1983.92 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90834/186688 [00:51<00:37, 2529.47 examples/s]Running tokenizer on datase|████▊     | 89834/186688 [00:48<01:11, 1349.01 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90751/186688 [00:49<01:09, 1378.50 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91668/186688 [00:49<01:00, 1565.98 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92585/186688 [00:50<00:55, 1693.57 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95502/186688 [00:51<00:44, 2030.00 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96502/186688 [00:51<00:43, 2074.70 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97502/186688 [00:52<00:50, 1780.08 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99336/186688 [00:52<00:32, 2718.11 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100336/186688 [00:53<00:28, 3039.91 examples/s]R48%|████▊     | 88917/186688 [00:50<00:38, 2509.69 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89917/186688 [00:51<00:51, 1884.62 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90917/186688 [00:51<00:55, 1737.17 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91834/186688 [00:52<00:50, 1878.43 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92834/186688 [00:52<00:39, 2367.63 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93751/186688 [00:52<00:31, 2926.20 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94751/186688 [00:53<00:41, 2236.64 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95668/186688 [00:53<00:32, 2816.87 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97585/186688 [00:53<00:19, 4470.54 examples/s]Runni_proc=64):  48%|████▊     | 88834/186688 [00:49<00:37, 2610.99 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89751/186688 [00:49<00:32, 2965.42 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91668/186688 [00:50<00:31, 3001.63 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92668/186688 [00:50<00:34, 2753.23 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93585/186688 [00:50<00:29, 3141.42 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94502/186688 [00:51<00:48, 1894.89 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95502/186688 [00:52<00:56, 1607.52 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96419/186688 [00:52<00:45, 1985.27 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97336/186688 [00:53<00:54, 1628.58 exa64):  48%|████▊     | 90000/186688 [00:48<00:49, 1954.47 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 91000/186688 [00:49<00:42, 2246.55 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93917/186688 [00:49<00:27, 3386.24 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94917/186688 [00:50<00:32, 2788.83 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████▏    | 95917/186688 [00:51<00:46, 1945.69 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96917/186688 [00:51<00:47, 1873.93 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98917/186688 [00:53<00:52, 1676.13 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 99917/186688 [00:53<00:51, 1690.64 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 100834/186688 [00:54<00:45, 1876.18 exkenizer on dataset (num_proc=64):  49%|████▊     | 90668/186688 [00:49<00:54, 1766.84 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91668/186688 [00:51<01:07, 1408.70 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92668/186688 [00:51<01:10, 1331.21 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93668/186688 [00:52<00:55, 1685.78 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94668/186688 [00:52<00:50, 1837.20 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95585/186688 [00:52<00:42, 2151.39 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96585/186688 [00:52<00:32, 2762.97 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97502/186688 [00:53<00:44, 2013.46 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99419/186688 es/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96419/186688 [00:51<00:44, 2028.03 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98336/186688 [00:52<00:43, 2045.03 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100170/186688 [00:52<00:31, 2754.86 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101087/186688 [00:53<00:32, 2636.45 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103004/186688 [00:53<00:24, 3445.18 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 103921/186688 [00:54<00:46, 1775.01 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104838/186688 [00:55<00:38, 2115.18 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106672/186688 [00:55<00:25, 3152.01 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█unning tokenizer on dataset (num_proc=64):  55%|█████▍    | 102336/186688 [00:53<00:18, 4559.53 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104253/186688 [00:53<00:15, 5239.54 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▋    | 105253/186688 [00:53<00:18, 4379.45 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106170/186688 [00:54<00:23, 3447.13 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 107170/186688 [00:54<00:28, 2810.63 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108087/186688 [00:55<00:32, 2396.03 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 109087/186688 [00:55<00:27, 2789.15 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110004/186688 [00:55<00:27, 2804.95 examples/s]Running tokenizer on dataset (num_proc=64):  59%|██t (num_proc=64):  50%|████▉     | 92751/186688 [00:52<00:24, 3773.77 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93668/186688 [00:54<01:06, 1393.43 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95585/186688 [00:54<00:51, 1757.92 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97502/186688 [00:54<00:33, 2625.53 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99419/186688 [00:55<00:37, 2355.30 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 100419/186688 [00:56<00:34, 2467.41 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101419/186688 [00:56<00:32, 2595.27 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103336/186688 [00:56<00:22, 3663.94 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104336/186688 [00:56<ng tokenizer on dataset (num_proc=64):  53%|█████▎    | 99502/186688 [00:53<00:14, 6042.66 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101419/186688 [00:53<00:10, 7923.81 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103419/186688 [00:54<00:14, 5645.15 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▋    | 105253/186688 [00:54<00:15, 5261.63 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106170/186688 [00:54<00:15, 5133.36 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108087/186688 [00:55<00:11, 6905.56 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110004/186688 [00:55<00:18, 4099.68 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110921/186688 [00:56<00:25, 2932.35 examples/s]Running tokenizer on dataset (num_proc=64):  60%|████amples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 101751/186688 [00:54<00:43, 1960.04 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102668/186688 [00:55<00:45, 1853.77 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103585/186688 [00:55<00:48, 1721.96 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104585/186688 [00:55<00:39, 2071.59 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 105502/186688 [00:56<00:33, 2448.44 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 107419/186688 [00:56<00:24, 3224.05 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108419/186688 [00:56<00:22, 3554.44 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▊    | 109336/186688 [00:57<00:25, 3030.47 examples/s]Running tokenizer on dataset (num_proc=64):  6mples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98253/186688 [00:54<00:46, 1905.31 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100087/186688 [00:54<00:36, 2385.64 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101004/186688 [00:55<00:47, 1805.71 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102004/186688 [00:55<00:46, 1815.29 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104921/186688 [00:56<00:25, 3230.82 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106755/186688 [00:56<00:19, 4065.31 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 107672/186688 [00:56<00:22, 3442.77 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108672/186688 [00:57<00:19, 4018.05 examples/s]Running tokenizer on dataset (num_proc=64):  59%les/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98502/186688 [00:51<00:25, 3482.93 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99502/186688 [00:51<00:21, 4130.28 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 100502/186688 [00:52<00:19, 4437.23 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101502/186688 [00:52<00:17, 4984.56 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102502/186688 [00:53<00:30, 2778.14 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103502/186688 [00:53<00:28, 2900.57 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104419/186688 [00:54<00:38, 2121.05 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106253/186688 [00:55<00:46, 1742.75 examples/s]Running tokenizer on dataset (num_proc=64):  58%|[00:54<00:44, 1972.22 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101336/186688 [00:55<00:34, 2465.28 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102253/186688 [00:55<00:29, 2906.91 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104087/186688 [00:55<00:19, 4207.72 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 105004/186688 [00:56<00:27, 2978.91 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106004/186688 [00:56<00:38, 2117.02 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106921/186688 [00:57<00:36, 2174.37 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 107921/186688 [00:57<00:30, 2570.72 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109838/186688 [00:57<00:20, 3819.80 examples/s]Running tokenizer on d████▊    | 107672/186688 [00:55<00:32, 2427.45 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108589/186688 [00:56<00:29, 2646.06 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▊    | 109589/186688 [00:56<00:23, 3213.38 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110589/186688 [00:56<00:21, 3517.33 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112423/186688 [00:56<00:16, 4561.57 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113340/186688 [00:57<00:23, 3098.96 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115257/186688 [00:58<00:25, 2752.71 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116174/186688 [00:58<00:22, 3150.00 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117091/186688 [00:58<00:23, 2██▉    | 110921/186688 [00:56<00:23, 3261.79 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111921/186688 [00:56<00:18, 3981.72 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113838/186688 [00:56<00:11, 6188.98 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115672/186688 [00:56<00:09, 7391.11 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117506/186688 [00:56<00:12, 5464.01 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120257/186688 [00:57<00:09, 7074.78 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121257/186688 [00:57<00:09, 6686.97 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122174/186688 [00:57<00:13, 4838.58 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 123091/186688 [00:58<00:25, ▉    | 111838/186688 [00:57<00:29, 2568.30 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112755/186688 [00:57<00:24, 3061.10 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113672/186688 [00:57<00:23, 3147.55 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114589/186688 [00:57<00:21, 3286.84 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115506/186688 [00:58<00:25, 2777.16 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116423/186688 [00:58<00:26, 2633.18 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117340/186688 [00:58<00:21, 3234.19 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118257/186688 [00:58<00:18, 3757.42 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121091/186688 [00:59<00:14, 4632.0%|█████▉    | 111170/186688 [00:57<00:16, 4562.71 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113004/186688 [00:57<00:20, 3561.62 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 114838/186688 [00:58<00:20, 3439.49 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116672/186688 [00:58<00:15, 4432.30 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117589/186688 [00:59<00:18, 3709.41 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118506/186688 [00:59<00:20, 3278.59 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119506/186688 [00:59<00:18, 3538.30 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 120506/186688 [00:59<00:16, 3981.59 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 121423/186688 [00:22, 3645.53 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106253/186688 [00:57<00:20, 3938.59 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 107170/186688 [00:58<00:34, 2300.22 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108170/186688 [00:58<00:27, 2808.46 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110087/186688 [00:58<00:18, 4040.24 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 111004/186688 [00:58<00:17, 4294.67 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112838/186688 [00:59<00:15, 4827.28 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114672/186688 [00:59<00:12, 5668.58 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115589/186688 [00:59<00:17, 3992.90 examples/s]Running tokenizer on data████▊    | 108087/186688 [00:57<01:01, 1272.93 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 109087/186688 [00:57<00:50, 1544.44 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 111004/186688 [00:58<00:36, 2087.30 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112838/186688 [00:58<00:36, 2012.88 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113755/186688 [00:59<00:31, 2319.37 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114672/186688 [00:59<00:30, 2373.41 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115589/186688 [00:59<00:30, 2326.70 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116506/186688 [01:00<00:27, 2583.13 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117423/186688 [01:00<00:24ataset (num_proc=64):  60%|█████▉    | 111755/186688 [00:57<00:14, 5003.22 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112672/186688 [00:58<00:13, 5411.78 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113672/186688 [00:58<00:19, 3841.56 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114672/186688 [00:58<00:22, 3179.80 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116506/186688 [00:59<00:16, 4320.03 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118340/186688 [00:59<00:18, 3693.39 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120174/186688 [01:00<00:14, 4527.75 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121091/186688 [01:00<00:13, 4857.05 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████2507.99 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 124008/186688 [00:59<00:26, 2389.86 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124925/186688 [00:59<00:21, 2936.28 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126759/186688 [00:59<00:14, 4175.86 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127676/186688 [00:59<00:15, 3733.06 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128593/186688 [01:00<00:14, 4114.78 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130427/186688 [01:00<00:10, 5273.55 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131427/186688 [01:00<00:10, 5519.12 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132344/186688 [01:00<00:12, 4479.24 examples/s]Running tokenizer on976.45 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119925/186688 [00:59<00:16, 3933.52 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 121759/186688 [00:59<00:12, 5245.15 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 122676/186688 [00:59<00:12, 5282.31 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 123593/186688 [00:59<00:13, 4835.53 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124593/186688 [01:00<00:25, 2440.16 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125593/186688 [01:00<00:21, 2822.20 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127427/186688 [01:01<00:16, 3639.57 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128427/186688 [01:01<00:16, 3618.70 examples/s]Running tokenizer on |█████▉    | 110589/186688 [00:57<00:15, 4771.40 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112423/186688 [00:57<00:11, 6362.51 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 114340/186688 [00:57<00:08, 8053.99 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116174/186688 [00:58<00:17, 4102.13 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▎   | 118925/186688 [00:58<00:13, 4862.10 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 120842/186688 [00:59<00:18, 3478.28 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 122842/186688 [01:00<00:16, 3834.36 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 123759/186688 [01:01<00:26, 2366.16 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125676/186688 [01:001:00<00:14, 4361.67 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 122423/186688 [01:00<00:14, 4398.61 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125174/186688 [01:00<00:09, 6589.11 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127008/186688 [01:00<00:07, 8225.00 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128842/186688 [01:00<00:06, 9000.39 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130676/186688 [01:01<00:08, 6515.82 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 131676/186688 [01:01<00:08, 6635.74 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132593/186688 [01:01<00:08, 6092.81 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 133510/186688 [01:01<00:10, 5056.77 examples/s]Runni71 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122091/186688 [00:59<00:15, 4297.56 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 123008/186688 [00:59<00:15, 4012.07 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 123925/186688 [01:00<00:17, 3581.24 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124842/186688 [01:00<00:15, 3898.07 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125759/186688 [01:00<00:15, 3831.76 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127676/186688 [01:01<00:13, 4420.98 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129510/186688 [01:01<00:14, 3938.71 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130427/186688 [01:01<00:13, 4096.49 examples/s]Running tokenizer on data, 2843.27 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118340/186688 [01:00<00:21, 3125.52 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119340/186688 [01:00<00:18, 3621.24 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121257/186688 [01:01<00:15, 4267.43 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 123091/186688 [01:01<00:10, 5867.38 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124925/186688 [01:01<00:08, 7095.05 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125842/186688 [01:01<00:12, 4914.01 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126759/186688 [01:02<00:12, 4747.48 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128593/186688 [01:02<00:12, 4485.70 examples/s]Running tokenizer set (num_proc=64):  62%|██████▏   | 116589/186688 [01:00<00:19, 3647.77 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118506/186688 [01:00<00:14, 4739.59 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119423/186688 [01:00<00:14, 4764.21 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121257/186688 [01:01<00:13, 4715.42 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122174/186688 [01:01<00:24, 2645.92 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 123174/186688 [01:02<00:20, 3080.94 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125091/186688 [01:02<00:14, 4221.59 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126925/186688 [01:02<00:11, 5384.02 examples/s]Running tokenizer on dataset (num_proc=64):  68%|█████▌   | 122925/186688 [01:00<00:16, 3949.64 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 123842/186688 [01:00<00:14, 4362.54 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125759/186688 [01:01<00:14, 4123.46 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126759/186688 [01:01<00:12, 4623.91 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127676/186688 [01:01<00:13, 4497.82 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128593/186688 [01:01<00:12, 4506.50 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129510/186688 [01:02<00:19, 2911.31 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130427/186688 [01:02<00:17, 3293.18 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131344/186688 [01:03<00:21, 2529.dataset (num_proc=64):  70%|██████▉   | 130344/186688 [01:01<00:11, 4793.10 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131261/186688 [01:01<00:10, 5051.72 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132178/186688 [01:01<00:10, 5075.99 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133178/186688 [01:02<00:11, 4466.15 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134095/186688 [01:02<00:14, 3728.91 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135012/186688 [01:02<00:13, 3855.70 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 135929/186688 [01:03<00:12, 3979.46 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136846/186688 [01:03<00:16, 2972.87 examples/s]Running tokenizer on dataset (num_proc=64):  74%|ng tokenizer on dataset (num_proc=64):  72%|███████▏  | 134427/186688 [01:02<00:10, 5012.50 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136261/186688 [01:02<00:10, 4741.94 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 137178/186688 [01:02<00:11, 4335.77 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138095/186688 [01:02<00:11, 4222.40 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139929/186688 [01:03<00:08, 5394.13 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140846/186688 [01:03<00:12, 3688.73 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141763/186688 [01:03<00:13, 3349.05 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142680/186688 [01:04<00:11, 3858.61 examples/s]Running tokenizer on dataset ▊   | 127842/186688 [01:03<00:16, 3624.74 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128759/186688 [01:03<00:17, 3319.49 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130593/186688 [01:03<00:12, 4607.25 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131510/186688 [01:03<00:11, 4669.62 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132427/186688 [01:04<00:12, 4186.77 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133344/186688 [01:04<00:11, 4457.65 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134261/186688 [01:04<00:12, 4272.73 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136178/186688 [01:04<00:08, 5784.50 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 137095/186688 [01:04<0 dataset (num_proc=64):  72%|███████▏  | 134178/186688 [01:01<00:13, 3808.08 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135178/186688 [01:01<00:12, 4237.67 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136095/186688 [01:02<00:16, 2992.66 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 137012/186688 [01:02<00:17, 2918.49 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138929/186688 [01:02<00:11, 4285.80 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141763/186688 [01:03<00:15, 2929.18 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142680/186688 [01:04<00:15, 2900.72 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143597/186688 [01:04<00:19, 2245.67 examples/s]Running tokenizer on dataset (num_proc=64): set (num_proc=64):  70%|███████   | 131344/186688 [01:02<00:18, 3069.62 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132261/186688 [01:02<00:16, 3354.38 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133178/186688 [01:02<00:15, 3473.18 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135095/186688 [01:03<00:17, 2920.11 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136012/186688 [01:03<00:14, 3398.56 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 137929/186688 [01:04<00:12, 3965.40 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138846/186688 [01:04<00:17, 2796.74 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140763/186688 [01:04<00:11, 4050.34 examples/s]Running tokenizer on dataset (num_proc=64):  76%|█1<00:22, 2733.78 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127510/186688 [01:01<00:16, 3678.22 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128427/186688 [01:02<00:14, 4039.19 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129344/186688 [01:02<00:14, 3937.64 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130344/186688 [01:03<00:22, 2520.96 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131261/186688 [01:03<00:20, 2768.21 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132178/186688 [01:03<00:16, 3253.67 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133178/186688 [01:04<00:28, 1879.87 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134095/186688 [01:05<00:29, 1754.87 examples/s]Runningon dataset (num_proc=64):  69%|██████▉   | 129593/186688 [01:02<00:12, 4638.37 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130510/186688 [01:02<00:13, 4237.90 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132344/186688 [01:03<00:10, 5179.04 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134178/186688 [01:03<00:08, 6549.44 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135095/186688 [01:03<00:08, 5865.99 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 137846/186688 [01:04<00:08, 5962.76 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139763/186688 [01:04<00:07, 6484.25 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140680/186688 [01:04<00:09, 4763.94 examples/s]Running tokenizer on dataset (num_proc=64):  76%48 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133178/186688 [01:03<00:13, 3949.29 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134178/186688 [01:04<00:20, 2615.07 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135178/186688 [01:04<00:16, 3057.67 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136095/186688 [01:05<00:21, 2375.14 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 137095/186688 [01:05<00:27, 1820.69 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139846/186688 [01:06<00:14, 3171.66 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140763/186688 [01:06<00:13, 3299.86 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141680/186688 [01:06<00:14, 3195.66 examples/s]Running t█████▌  | 141680/186688 [01:05<00:12, 3703.47 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142597/186688 [01:05<00:13, 3340.81 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144514/186688 [01:05<00:09, 4314.04 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145431/186688 [01:06<00:09, 4360.02 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146431/186688 [01:06<00:12, 3296.93 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148265/186688 [01:06<00:08, 4750.98 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149182/186688 [01:07<00:09, 3984.60 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151099/186688 [01:07<00:06, 5116.25 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████▏  77%|███████▋  | 144514/186688 [01:05<00:16, 2610.57 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145431/186688 [01:05<00:18, 2205.62 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147265/186688 [01:06<00:14, 2637.23 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149265/186688 [01:06<00:09, 3901.70 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151099/186688 [01:06<00:07, 4791.02 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████▏ | 152099/186688 [01:07<00:09, 3462.52 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153099/186688 [01:07<00:08, 3986.51 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 154016/186688 [01:07<00:07, 4121.05 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████0:08, 5951.44 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138012/186688 [01:04<00:08, 5895.04 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139846/186688 [01:05<00:09, 5136.08 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140846/186688 [01:05<00:09, 4909.06 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141846/186688 [01:05<00:09, 4504.93 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142763/186688 [01:06<00:16, 2642.31 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143680/186688 [01:06<00:13, 3229.62 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144680/186688 [01:07<00:16, 2558.74 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145597/186688 [01:07<00:18, 2225.18 examples/s██████▍  | 137846/186688 [01:04<00:23, 2072.79 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139763/186688 [01:04<00:14, 3309.30 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140680/186688 [01:05<00:27, 1656.69 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141597/186688 [01:06<00:23, 1920.46 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142597/186688 [01:06<00:19, 2253.22 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143597/186688 [01:06<00:19, 2172.46 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145431/186688 [01:07<00:12, 3313.21 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147348/186688 [01:07<00:10, 3814.04 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  (num_proc=64):  77%|███████▋  | 143597/186688 [01:04<00:12, 3556.45 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144514/186688 [01:04<00:10, 4049.24 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146431/186688 [01:05<00:11, 3555.26 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149348/186688 [01:05<00:06, 6195.26 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151182/186688 [01:06<00:12, 2822.79 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████▏ | 152099/186688 [01:07<00:11, 2946.92 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153016/186688 [01:07<00:11, 2870.09 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154933/186688 [01:07<00:08, 3577.51 examples/s]Running tokenizer on dataset (num_proc=64):  84 tokenizer on dataset (num_proc=64):  73%|███████▎  | 136012/186688 [01:05<00:18, 2701.97 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138846/186688 [01:05<00:12, 3857.26 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140763/186688 [01:06<00:10, 4568.22 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141763/186688 [01:06<00:13, 3231.95 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142680/186688 [01:07<00:12, 3449.54 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143597/186688 [01:07<00:13, 3265.71 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144597/186688 [01:07<00:10, 3846.53 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145514/186688 [01:07<00:10, 3753.60 examples/s]Running tokenizer on dataset (n|███████▌  | 141597/186688 [01:06<00:25, 1737.66 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142514/186688 [01:06<00:21, 2079.15 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144431/186688 [01:07<00:16, 2626.72 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145348/186688 [01:07<00:18, 2230.66 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146265/186688 [01:08<00:17, 2324.05 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147182/186688 [01:08<00:13, 2850.34 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148099/186688 [01:08<00:13, 2851.16 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150016/186688 [01:09<00:11, 3161.44 examples/s]Running tokenizer on dataset (num_proc=64):  81%|███████okenizer on dataset (num_proc=64):  76%|███████▋  | 142680/186688 [01:06<00:11, 3708.55 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143597/186688 [01:07<00:14, 2987.46 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144597/186688 [01:07<00:13, 3017.93 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145514/186688 [01:07<00:11, 3499.05 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146431/186688 [01:08<00:14, 2841.79 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148265/186688 [01:08<00:09, 4214.09 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149182/186688 [01:08<00:08, 4237.77 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150182/186688 [01:09<00:12, 2925.96 examples/s]Running tokenizer on dataset (num]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146514/186688 [01:08<00:16, 2411.55 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 149431/186688 [01:08<00:09, 3848.90 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151348/186688 [01:09<00:08, 4196.17 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153265/186688 [01:09<00:07, 4502.76 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154265/186688 [01:09<00:08, 3890.42 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 155182/186688 [01:10<00:08, 3514.51 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 157099/186688 [01:10<00:08, 3458.30 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 158099/186688 [01:11<00:09, 3173.35 examples/s]Running tokeum_proc=64):  78%|███████▊  | 146431/186688 [01:08<00:13, 2881.85 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148348/186688 [01:08<00:09, 3916.04 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149348/186688 [01:08<00:09, 3892.50 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151265/186688 [01:09<00:12, 2820.08 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153182/186688 [01:10<00:11, 2856.69 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154099/186688 [01:10<00:11, 2890.37 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 155099/186688 [01:10<00:09, 3375.61 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 156016/186688 [01:11<00:11, 2759.25 examples/s]Running tokenizer on dataset (num_proc=64):  84| 148348/186688 [01:08<00:12, 3079.96 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149348/186688 [01:08<00:14, 2592.78 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150265/186688 [01:08<00:12, 2857.28 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151265/186688 [01:08<00:09, 3543.53 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 152265/186688 [01:09<00:12, 2706.10 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154099/186688 [01:10<00:16, 1950.23 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 155016/186688 [01:11<00:13, 2268.60 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 155933/186688 [01:11<00:17, 1728.48 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156850/18668  | 151016/186688 [01:09<00:12, 2797.60 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████▏ | 152016/186688 [01:11<00:24, 1436.44 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153850/186688 [01:11<00:14, 2222.32 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154850/186688 [01:12<00:20, 1584.26 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 155850/186688 [01:12<00:16, 1905.57 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156850/186688 [01:13<00:19, 1504.68 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:14<00:18, 1594.84 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:14<00:10, 2488.00 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160_proc=64):  81%|████████  | 151099/186688 [01:09<00:14, 2541.08 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████▏ | 152016/186688 [01:10<00:15, 2240.94 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154933/186688 [01:11<00:14, 2143.04 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 155850/186688 [01:12<00:14, 2136.00 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156850/186688 [01:13<00:17, 1700.54 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:13<00:14, 2027.43 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:16<00:30, 916.08 examples/s] Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:17<00:34, 776.05 examples/s]Running tokenizer on dataset (num_proc=64): %|████████▎ | 155933/186688 [01:08<00:09, 3084.62 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156850/186688 [01:08<00:08, 3376.65 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:08<00:06, 4162.54 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:09<00:08, 3322.46 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:12<00:25, 1027.19 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:20<01:11, 345.42 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:22<01:05, 367.71 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:23<00:52, 440.06 examples/s]Running tokenizer on dataset (num_proc=64):  88%|██▖███▎ | 154933/186688 [01:07<00:06, 4568.46 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156850/186688 [01:09<00:17, 1750.39 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:09<00:14, 2024.30 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:10<00:15, 1794.54 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:11<00:14, 1880.44 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:11<00:16, 1580.70 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161767/186688 [01:21<01:17, 323.54 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:22<01:03, 379.09 examples/s]Running tokenizer on dataset (num_proc=64):  88%|███████| 152016/186688 [01:07<00:07, 4639.91 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 155016/186688 [01:08<00:08, 3915.63 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156933/186688 [01:09<00:08, 3564.23 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:11<00:19, 1503.12 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:12<00:17, 1539.17 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:16<00:32, 791.65 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:22<01:01, 403.68 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:25<01:02, 385.53 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/1nizer on dataset (num_proc=64):  85%|████████▌ | 159016/186688 [01:11<00:10, 2593.00 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159933/186688 [01:11<00:09, 2935.31 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:12<00:14, 1786.64 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:20<01:04, 385.17 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:26<01:23, 284.80 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:26<00:58, 393.15 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:29<01:02, 350.74 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:30<00:44, 468.04 examples/s]Running tokenizer on dat8 [01:12<00:17, 1720.44 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:13<00:19, 1494.10 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:14<00:25, 1103.40 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:15<00:18, 1452.05 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:18<00:38, 664.13 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:28<01:41, 245.64 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162850/186688 [01:28<01:09, 343.91 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:29<00:51, 444.55 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:32<00%|████████▍ | 156933/186688 [01:12<00:14, 2120.72 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:13<00:19, 1464.68 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:13<00:16, 1648.83 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:15<00:22, 1206.17 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:15<00:18, 1411.62 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:28<01:49, 227.34 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:31<01:35, 251.15 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:33<01:16, 301.39 examples/s]Running tokenizer on dataset (num_proc=64):  88%|██86688 [01:25<00:44, 514.77 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:26<00:37, 584.14 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:31<00:51, 406.56 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:31<00:36, 543.64 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:34<00:39, 484.94 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:34<00:28, 619.92 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:35<00:25, 661.32 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:36<00:18, 863.90 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:36█████▊ | 164767/186688 [01:24<00:41, 529.27 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:25<00:32, 645.74 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:26<00:29, 680.51 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:30<00:43, 438.33 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:32<00:38, 465.41 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:34<00:33, 505.88 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:35<00:27, 585.96 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:36<00:21, 683.19 examples/s]Running tokenizer on dataset (num_proc=64):  93%|██████▖▊ | 164767/186688 [01:25<00:45, 478.55 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:28<00:50, 412.29 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:30<00:46, 428.08 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:34<00:48, 390.03 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:35<00:37, 477.68 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:35<00:29, 582.97 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:36<00:24, 659.52 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:37<00:18, 827.17 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 17 86%|████████▌ | 160850/186688 [01:18<00:26, 983.86 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:28<01:33, 266.60 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162850/186688 [01:29<01:06, 356.03 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163850/186688 [01:32<01:06, 344.37 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:33<00:54, 404.95 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:34<00:43, 481.36 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:35<00:32, 612.17 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:37<00:35, 529.48 examples/s]Running tokenizer on dataset (num_proc=64):  90%|███850/186688 [01:15<00:12, 2100.15 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:26<01:22, 302.46 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162850/186688 [01:28<01:09, 342.30 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:33<01:18, 293.07 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:33<00:56, 387.18 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:34<00:43, 481.90 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:39<00:56, 354.42 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:39<00:39, 482.43 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01aset (num_proc=64):  89%|████████▉ | 166767/186688 [01:32<00:42, 473.85 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:33<00:35, 536.71 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:33<00:23, 746.98 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:34<00:17, 974.48 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:35<00:14, 1021.25 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:37<00:14, 932.42 examples/s] Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:38<00:13, 930.42 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:39<00:11, 1041.38 examples/s]Running tokenizer on dataset █████▊ | 164767/186688 [01:33<00:56, 388.62 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:35<00:45, 458.75 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:38<00:49, 404.98 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:38<00:36, 522.05 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:39<00:25, 712.23 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:40<00:23, 709.35 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:41<00:19, 809.61 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:43<00:16, 819.08 examples/s]Running tokenizer on dataset (num_proc=64):  93%|██████:58, 375.19 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:34<00:51, 410.21 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:38<00:56, 351.36 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:40<00:47, 399.48 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:41<00:27, 604.89 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:41<00:15, 947.57 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:42<00:12, 1156.15 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:44<00:15, 808.75 examples/s] Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:49<00:26, 4██▎| 172767/186688 [01:37<00:18, 760.66 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:38<00:14, 864.37 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:38<00:11, 1082.25 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:39<00:11, 936.14 examples/s] Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:45<00:24, 407.33 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:49<00:28, 323.23 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:51<00:24, 336.59 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [01:52<00:16, 457.65 examples/s]Running tokenizer on dataset (num_proc=64):  97%|██████2767/186688 [01:37<00:12, 1122.98 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:37<00:09, 1299.38 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:38<00:08, 1326.60 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:44<00:25, 438.10 examples/s] Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:45<00:21, 464.51 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:50<00:27, 339.44 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:53<00:25, 321.08 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [01:54<00:16, 433.59 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋<00:13, 1085.00 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:37<00:13, 1045.22 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:39<00:15, 846.07 examples/s] Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:43<00:25, 463.03 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:44<00:20, 532.94 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:47<00:21, 465.78 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:52<00:27, 331.71 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:52<00:19, 430.89 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [01█████ | 168767/186688 [01:38<00:25, 712.88 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:40<00:25, 651.64 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:41<00:24, 653.16 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:45<00:34, 435.89 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:46<00:24, 568.62 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:49<00:29, 432.33 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:51<00:25, 476.21 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:52<00:20, 535.49 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████(num_proc=64):  94%|█████████▍| 175684/186688 [01:40<00:13, 833.28 examples/s] Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:46<00:27, 370.66 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:52<00:33, 276.60 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:56<00:32, 253.68 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [01:57<00:21, 336.56 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [01:57<00:13, 468.46 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [01:59<00:11, 462.02 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [01:59<00:07, 597.01 examples/s]Running tokenizer on dataset:39<00:28, 637.13 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:41<00:28, 593.67 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:43<00:27, 571.27 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:44<00:22, 654.07 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:46<00:22, 630.69 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:46<00:11, 1079.03 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:49<00:15, 709.04 examples/s] Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:57<00:33, 300.91 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [02███▋| 180269/186688 [01:52<00:10, 602.75 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [01:55<00:11, 477.87 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:01<00:16, 281.39 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:02<00:09, 388.75 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:02<00:02, 699.40 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:02<00:01, 786.44 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:03<00:00, 998.89 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:03<00:00, 1506.85 examples/s]
| 180269/186688 [01:54<00:11, 557.84 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:02<00:20, 263.56 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:02<00:07, 466.92 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:02<00:04, 598.19 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:03<00:02, 789.74 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:04<00:01, 823.76 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:04<00:00, 957.09 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:05<00:00, 1489.67 examples/s]
 (num_proc=64):  98%|█████████▊| 183020/186688 [02:00<00:04, 803.95 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:01<00:03, 700.34 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:01<00:01, 954.57 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:04<00:01, 618.99 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:04<00:00, 848.58 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:05<00:00, 1487.02 examples/s]
██▎| 173767/186688 [01:45<00:17, 720.04 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:45<00:12, 918.11 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:52<00:27, 400.70 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:57<00:34, 292.73 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [02:02<00:36, 251.71 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:03<00:25, 324.35 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:03<00:16, 435.46 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:05<00:14, 441.64 examples/s]Running tokenizer on dataset (num_proc=64):  97%|██████47.74 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:51<00:23, 471.13 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:52<00:19, 521.27 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [02:02<00:39, 230.57 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:03<00:28, 294.03 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:06<00:24, 298.58 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:07<00:17, 371.24 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:08<00:13, 418.44 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:09<00:08, 5:55<00:17, 408.81 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [01:57<00:14, 439.16 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [01:59<00:13, 408.89 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:02<00:11, 389.70 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:02<00:06, 541.16 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:03<00:04, 580.64 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:04<00:02, 647.74 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:05<00:01, 784.09 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:09<00:00, 452.06 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:10<00:00, 1434.58 examples/s]
████▍| 176601/186688 [01:55<00:23, 429.73 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:57<00:19, 476.06 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:59<00:19, 434.15 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:01<00:16, 442.86 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:07<00:21, 301.06 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:09<00:16, 328.08 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:09<00:10, 429.78 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:10<00:07, 519.74 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████:00<00:29, 306.89 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:01<00:21, 378.29 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:06<00:24, 294.71 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:08<00:19, 332.03 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:08<00:12, 441.97 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:08<00:07, 604.83 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:11<00:06, 524.32 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:12<00:05, 534.86 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02██▋| 181186/186688 [02:07<00:11, 475.61 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:10<00:11, 389.52 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:11<00:07, 493.97 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:11<00:04, 592.65 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:12<00:02, 688.91 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:13<00:01, 794.46 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:14<00:00, 933.23 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:14<00:00, 1383.70 examples/s]
:13<00:02, 684.11 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:14<00:01, 760.03 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:14<00:00, 844.36 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:15<00:00, 1375.21 examples/s]
55.28 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:09<00:05, 728.60 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:10<00:03, 704.24 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:12<00:02, 740.24 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:12<00:01, 912.49 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:15<00:00, 593.25 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:16<00:00, 1371.91 examples/s]
████▊| 183937/186688 [02:12<00:04, 562.69 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:14<00:03, 522.66 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:15<00:01, 568.01 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:18<00:00, 462.68 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:19<00:00, 1342.91 examples/s]
[INFO|configuration_utils.py:763] 2025-10-03 22:49:31,322 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 22:49:31,322 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 22:49:31,322 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 22:49:31,322 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 22:49:31,322 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-03 22:49:31,322 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|configuration_utils.py:839] 2025-10-03 22:49:31,322 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-03 22:49:31,323 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 22:49:31,323 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|configuration_utils.py:763] 2025-10-03 22:49:31,323 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-03 22:49:31,323 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-03 22:49:31,323 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 22:49:31,324 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-03 22:49:31,324 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-03 22:49:31,332 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 22:49:31,333 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|modeling_utils.py:1277] 2025-10-03 22:49:32,042 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-03 22:49:32,042 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-03 22:49:32,042 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-03 22:49:32,042 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 22:49:32,043 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-03 22:49:32,043 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-03 22:49:32,043 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-03 22:49:32,043 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 22:49:32,043 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-03 22:49:32,043 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 22:49:32,044 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-03 22:49:32,044 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 22:49:32,044 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-03 22:49:32,044 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 22:49:32,044 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-03 22:49:32,044 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-03 22:49:32,055 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 22:49:32,059 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 22:49:32,060 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 22:49:32,060 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 22:49:32,067 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 22:49:32,070 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 22:49:32,070 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 22:49:32,075 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.91s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.03s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.87s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.87s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.03s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.87s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.87s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.02s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.88s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.89s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.56s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]L41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]L41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]L41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]L41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.88s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]L41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]L41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]L41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.87s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:26<00:32,  6.44s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:33<00:26,  6.55s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.12s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.54s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.54s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 22:50:29,181 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 22:50:29,181 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
[INFO|configuration_utils.py:1008] 2025-10-03 22:50:29,183 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 22:50:29,183 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 22:50:29,188 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 22:50:29,188 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it][INFO|configuration_utils.py:1008] 2025-10-03 22:50:29,190 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
[INFO|configuration_utils.py:1055] 2025-10-03 22:50:29,190 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

oading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it][INFO|modeling_utils.py:5724] 2025-10-03 22:50:29,190 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
[INFO|modeling_utils.py:5732] 2025-10-03 22:50:29,191 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
oading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]

oading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██oading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:38<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:45<00:12,  6.39s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:52<00:06,  6.51s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 22:50:29,192 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
[INFO|modeling_utils.py:5732] 2025-10-03 22:50:29,192 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]

[INFO|modeling_utils.py:5724] 2025-10-03 22:50:29,192 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 22:50:29,192 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 22:50:29,192 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 22:50:29,193 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1008] 2025-10-03 22:50:29,193 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 22:50:29,193 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.55s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
[INFO|configuration_utils.py:1008] 2025-10-03 22:50:29,193 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 22:50:29,193 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-03 22:50:29,194 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|modeling_utils.py:5724] 2025-10-03 22:50:29,193 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 22:50:29,194 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1055] 2025-10-03 22:50:29,194 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-03 22:50:29,194 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 22:50:29,194 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-03 22:50:29,196 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 22:50:29,196 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 22:50:29,208 >> Using auto half precision backend
[WARNING|trainer.py:985] 2025-10-03 22:50:29,209 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 22:50:29,212 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  5.54s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:55<00:00,  6.18s/it]
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 22:50:29,213 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|modeling_utils.py:5724] 2025-10-03 22:50:29,213 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 22:50:29,213 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|trainer.py:757] 2025-10-03 22:50:29,214 >> Using auto half precision backend
[INFO|configuration_utils.py:1008] 2025-10-03 22:50:29,214 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 22:50:29,215 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|trainer.py:757] 2025-10-03 22:50:29,215 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 22:50:29,215 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 22:50:29,215 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 22:50:29,216 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 22:50:29,217 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 22:50:29,218 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 22:50:29,219 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 22:50:29,221 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 22:50:29,222 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 22:50:29,234 >> Using auto half precision backend
[WARNING|trainer.py:985] 2025-10-03 22:50:29,238 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 22:50:29,490 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[INFO|deepspeed.py:380] 2025-10-03 22:50:29,491 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[INFO|deepspeed.py:380] 2025-10-03 22:50:29,492 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 22:50:29,498 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 22:50:29,504 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 4. Using DeepSpeed's value.
[INFO|deepspeed.py:380] 2025-10-03 22:50:29,520 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[INFO|deepspeed.py:380] 2025-10-03 22:50:29,521 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 22:50:29,530 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[rank16]:W1003 22:50:34.482000 3207275 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank16]:W1003 22:50:34.482000 3207275 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank2]:W1003 22:50:34.507000 3055510 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank2]:W1003 22:50:34.507000 3055510 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank14]:W1003 22:50:34.546000 1409184 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank14]:W1003 22:50:34.546000 1409184 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank15]:W1003 22:50:34.605000 1409185 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank15]:W1003 22:50:34.605000 1409185 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank26]:W1003 22:50:34.660000 1345262 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank26]:W1003 22:50:34.660000 1345262 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank27]:W1003 22:50:34.701000 1345263 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank27]:W1003 22:50:34.701000 1345263 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank29]:W1003 22:50:34.740000 228880 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank29]:W1003 22:50:34.740000 228880 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[INFO|trainer.py:2523] 2025-10-03 22:50:42,716 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 22:50:42,716 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-03 22:50:42,716 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 22:50:42,716 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 22:50:42,716 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-03 22:50:42,716 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-03 22:50:42,716 >>   Total optimization steps = 1,459
[INFO|trainer.py:2523] 2025-10-03 22:50:42,716 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 22:50:42,716 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-03 22:50:42,716 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 22:50:42,716 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 22:50:42,716 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-03 22:50:42,716 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-03 22:50:42,716 >>   Total optimization steps = 1,459
[INFO|trainer.py:2523] 2025-10-03 22:50:42,717 >> ***** Running training *****
[INFO|trainer.py:2532] 2025-10-03 22:50:42,717 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2524] 2025-10-03 22:50:42,717 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-03 22:50:42,717 >>   Num Epochs = 1
[INFO|trainer.py:2523] 2025-10-03 22:50:42,717 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 22:50:42,717 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-03 22:50:42,717 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 22:50:42,717 >>   Instantaneous batch size per device = 1
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2532] 2025-10-03 22:50:42,717 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2523] 2025-10-03 22:50:42,717 >> ***** Running training *****
[INFO|trainer.py:2526] 2025-10-03 22:50:42,717 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2523] 2025-10-03 22:50:42,718 >> ***** Running training *****
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2529] 2025-10-03 22:50:42,717 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2523] 2025-10-03 22:50:42,717 >> ***** Running training *****
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2524] 2025-10-03 22:50:42,718 >>   Num examples = 186,688
[INFO|trainer.py:2529] 2025-10-03 22:50:42,717 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2524] 2025-10-03 22:50:42,718 >>   Num examples = 186,688
[INFO|trainer.py:2530] 2025-10-03 22:50:42,717 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2524] 2025-10-03 22:50:42,718 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-03 22:50:42,718 >>   Num Epochs = 1
[INFO|trainer.py:2530] 2025-10-03 22:50:42,717 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2525] 2025-10-03 22:50:42,718 >>   Num Epochs = 1
[INFO|trainer.py:2531] 2025-10-03 22:50:42,717 >>   Total optimization steps = 1,459
[INFO|trainer.py:2525] 2025-10-03 22:50:42,718 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 22:50:42,718 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2531] 2025-10-03 22:50:42,717 >>   Total optimization steps = 1,459
[INFO|trainer.py:2526] 2025-10-03 22:50:42,718 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2532] 2025-10-03 22:50:42,718 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2526] 2025-10-03 22:50:42,718 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 22:50:42,718 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2529] 2025-10-03 22:50:42,718 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2529] 2025-10-03 22:50:42,718 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-03 22:50:42,718 >>   Gradient Accumulation steps = 4
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2530] 2025-10-03 22:50:42,718 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2530] 2025-10-03 22:50:42,718 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-03 22:50:42,718 >>   Total optimization steps = 1,459
[INFO|trainer.py:2532] 2025-10-03 22:50:42,718 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2531] 2025-10-03 22:50:42,718 >>   Total optimization steps = 1,459
[INFO|trainer.py:2531] 2025-10-03 22:50:42,718 >>   Total optimization steps = 1,459
[INFO|trainer.py:2532] 2025-10-03 22:50:42,718 >>   Number of trainable parameters = 20,914,757,184
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2532] 2025-10-03 22:50:42,719 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2532] 2025-10-03 22:50:42,718 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2523] 2025-10-03 22:50:42,943 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 22:50:42,943 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-03 22:50:42,943 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 22:50:42,943 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 22:50:42,943 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-03 22:50:42,944 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-03 22:50:42,944 >>   Total optimization steps = 1,459
[INFO|trainer.py:2532] 2025-10-03 22:50:42,944 >>   Number of trainable parameters = 20,914,757,184
  0%|          | 0/1459 [00:00<?, ?it/s]  0%|          | 1/1459 [00:46<18:47:50, 46.41s/it]  0%|          | 2/1459 [01:21<16:06:32, 39.80s/it]  0%|          | 3/1459 [01:56<15:14:59, 37.71s/it]  0%|          | 4/1459 [02:40<16:13:51, 40.16s/it]  0%|          | 5/1459 [03:13<15:06:20, 37.40s/it]  0%|          | 6/1459 [03:51<15:12:08, 37.67s/it]  0%|          | 7/1459 [04:24<14:38:54, 36.32s/it]  1%|          | 8/1459 [04:59<14:26:04, 35.81s/it]  1%|          | 9/1459 [05:32<14:05:40, 34.99s/it]  1%|          | 10/1459 [06:07<14:03:47, 34.94s/it]                                                      1%|          | 10/1459 [06:07<14:03:47, 34.94s/it]  1%|          | 11/1459 [06:40<13:46:43, 34.26s/it]  1%|          | 12/1459 [07:14<13:43:33, 34.15s/it]  1%|          | 13/1459 [07:47<13:38:03, 33.94s/it]  1%|          | 14/1459 [08:20<13:26:56, 33.51s/it]  1%|          | 15/1459 [08:54<13:32:44, 33.77s/it]  1%|          | 16/1459 [09:29<13:43:07, 34.23s/it]  1%|          | 17/1459 [10:09<14:20:14, 35.79s/it]  1%|          | 18/1459 [10:44<14:16:08, 35.65s/it]  1%|▏         | 19/1459 [11:20<14:16:39, 35.69s/it]  1%|▏         | 20/1459 [11:58<14:30:51, 36.31s/it]                                                      1%|▏         | 20/1459 [11:58<14:30:51, 36.31s/it]  1%|▏         | 21/1459 [12:29<13:53:33, 34.78s/it]  2%|▏         | 22/1459 [13:00<13:25:36, 33.64s/it]  2%|▏         | 23/1459 [13:32<13:13:07, 33.14s/it]  2%|▏         | 24/1459 [14:14<14:14:09, 35.71s/it]  2%|▏         | 25/1459 [14:51<14:28:42, 36.35s/it]  2%|▏         | 26/1459 [15:29<14:37:13, 36.73s/it]  2%|▏         | 27/1459 [16:12<15:19:40, 38.53s/it]  2%|▏         | 28/1459 [16:43<14:27:16, 36.36s/it]  2%|▏         | 29/1459 [17:18<14:14:13, 35.84s/it]  2%|▏         | 30/1459 [17:59<14:54:52, 37.57s/it]                                                      2%|▏         | 30/1459 [17:59<14:54:52, 37.57s/it]  2%|▏         | 31/1459 [18:39<15:09:55, 38.23s/it]  2%|▏         | 32/1459 [19:17<15:04:06, 38.01s/it]  2%|▏         | 33/1459 [19:48<14:17:48, 36.09s/it]  2%|▏         | 34/1459 [20:25<14:20:43, 36.24s/it]  2%|▏         | 35/1459 [20:58<13:55:52, 35.22s/it]  2%|▏         | 36/1459 [21:28<13:19:49, 33.72s/it]  3%|▎         | 37/1459 [22:00<13:04:58, 33.12s/it]  3%|▎         | 38/1459 [22:33<13:04:25, 33.12s/it]  3%|▎         | 39/1459 [23:03<12:44:48, 32.32s/it]  3%|▎         | 40/1459 [23:36<12:49:58, 32.56s/it]                                                      3%|▎         | 40/1459 [23:36<12:49:58, 32.56s/it]  3%|▎         | 41/1459 [24:10<12:55:38, 32.82s/it]  3%|▎         | 42/1459 [24:44<13:07:19, 33.34s/it]  3%|▎         | 43/1459 [25:17<13:03:16, 33.19s/it]  3%|▎         | 44/1459 [25:47<12:41:12, 32.28s/it]  3%|▎         | 45/1459 [26:23<13:02:56, 33.22s/it]  3%|▎         | 46/1459 [26:58<13:20:25, 33.99s/it]  3%|▎         | 47/1459 [27:33<13:20:43, 34.03s/it]  3%|▎         | 48/1459 [28:07<13:20:22, 34.03s/it]  3%|▎         | 49/1459 [28:39<13:06:23, 33.46s/it]  3%|▎         | 50/1459 [29:16<13:29:56, 34.49s/it]                                                      3%|▎         | 50/1459 [29:16<13:29:56, 34.49s/it]  3%|▎         | 51/1459 [29:51<13:38:16, 34.87s/it]  4%|▎         | 52/1459 [30:23<13:12:30, 33.80s/it]  4%|▎         | 53/1459 [30:56<13:10:19, 33.73s/it]  4%|▎         | 54/1459 [31:27<12:45:44, 32.70s/it]  4%|▍         | 55/1459 [32:03<13:10:11, 33.77s/it]  4%|▍         | 56/1459 [32:35<13:01:50, 33.44s/it]  4%|▍         | 57/1459 [33:17<13:54:36, 35.72s/it]  4%|▍         | 58/1459 [33:54<14:03:33, 36.13s/it]  4%|▍         | 59/1459 [34:29<14:00:58, 36.04s/it]  4%|▍         | 60/1459 [35:00<13:19:10, 34.27s/it]                                                      4%|▍         | 60/1459 [35:00<13:19:10, 34.27s/it]  4%|▍         | 61/1459 [35:31<13:00:10, 33.48s/it]  4%|▍         | 62/1459 [36:06<13:08:44, 33.88s/it]  4%|▍         | 63/1459 [36:48<14:08:11, 36.45s/it]  4%|▍         | 64/1459 [37:23<13:55:13, 35.92s/it]  4%|▍         | 65/1459 [37:59<13:54:34, 35.92s/it]  5%|▍         | 66/1459 [38:36<14:01:09, 36.23s/it]  5%|▍         | 67/1459 [39:07<13:22:12, 34.58s/it]  5%|▍         | 68/1459 [39:40<13:10:34, 34.10s/it]  5%|▍         | 69/1459 [40:15<13:17:07, 34.41s/it]  5%|▍         | 70/1459 [40:53<13:40:11, 35.43s/it]                                                      5%|▍         | 70/1459 [40:53<13:40:11, 35.43s/it]  5%|▍         | 71/1459 [41:23<13:02:38, 33.83s/it]  5%|▍         | 72/1459 [41:54<12:41:38, 32.95s/it]  5%|▌         | 73/1459 [42:28<12:50:26, 33.35s/it]  5%|▌         | 74/1459 [43:03<13:00:23, 33.81s/it]  5%|▌         | 75/1459 [43:36<12:56:26, 33.66s/it]  5%|▌         | 76/1459 [44:10<12:55:23, 33.64s/it]  5%|▌         | 77/1459 [44:46<13:11:25, 34.36s/it]  5%|▌         | 78/1459 [45:19<13:04:39, 34.09s/it]  5%|▌         | 79/1459 [45:51<12:48:11, 33.40s/it]  5%|▌         | 80/1459 [46:27<13:08:32, 34.31s/it]                                                      5%|▌         | 80/1459 [46:27<13:08:32, 34.31s/it]  6%|▌         | 81/1459 [47:03<13:13:56, 34.57s/it]  6%|▌         | 82/1459 [47:33<12:47:26, 33.44s/it]  6%|▌         | 83/1459 [48:04<12:28:02, 32.62s/it]  6%|▌         | 84/1459 [48:38<12:38:22, 33.09s/it]  6%|▌         | 85/1459 [49:15<13:00:56, 34.10s/it]  6%|▌         | 86/1459 [49:53<13:29:02, 35.36s/it]  6%|▌         | 87/1459 [50:30<13:38:31, 35.80s/it]  6%|▌         | 88/1459 [51:10<14:07:28, 37.09s/it]  6%|▌         | 89/1459 [51:43<13:35:38, 35.72s/it]  6%|▌         | 90/1459 [52:18<13:34:15, 35.69s/it]                                                      6%|▌         | 90/1459 [52:18<13:34:15, 35.69s/it]  6%|▌         | 91/1459 [52:49<12:58:05, 34.13s/it]  6%|▋         | 92/1459 [53:28<13:29:57, 35.55s/it]  6%|▋         | 93/1459 [54:09<14:06:30, 37.18s/it]  6%|▋         | 94/1459 [54:45<13:59:38, 36.91s/it]  7%|▋         | 95/1459 [55:21<13:56:39, 36.80s/it]  7%|▋         | 96/1459 [55:51<13:04:37, 34.54s/it]  7%|▋         | 97/1459 [56:25<13:00:58, 34.40s/it]  7%|▋         | 98/1459 [56:57<12:48:27, 33.88s/it]  7%|▋         | 99/1459 [57:32<12:53:58, 34.15s/it]  7%|▋         | 100/1459 [58:06<12:50:54, 34.04s/it]                                                       7%|▋         | 100/1459 [58:06<12:50:54, 34.04s/it]  7%|▋         | 101/1459 [58:37<12:31:42, 33.21s/it]  7%|▋         | 102/1459 [59:17<13:18:48, 35.32s/it]  7%|▋         | 103/1459 [59:48<12:49:00, 34.03s/it]  7%|▋         | 104/1459 [1:00:23<12:49:36, 34.08s/it]  7%|▋         | 105/1459 [1:00:55<12:35:47, 33.49s/it]  7%|▋         | 106/1459 [1:01:33<13:07:03, 34.90s/it]  7%|▋         | 107/1459 [1:02:06<12:52:25, 34.28s/it]  7%|▋         | 108/1459 [1:02:40<12:54:17, 34.39s/it]  7%|▋         | 109/1459 [1:03:15<12:56:04, 34.49s/it]  8%|▊         | 110/1459 [1:03:53<13:20:09, 35.59s/it]                                                         8%|▊         | 110/1459 [1:03:53<13:20:09, 35.59s/it]  8%|▊         | 111/1459 [1:04:32<13:42:43, 36.62s/it]  8%|▊         | 112/1459 [1:05:06<13:20:55, 35.68s/it]  8%|▊         | 113/1459 [1:05:37<12:53:06, 34.46s/it]  8%|▊         | 114/1459 [1:06:15<13:15:58, 35.51s/it]  8%|▊         | 115/1459 [1:06:55<13:44:53, 36.83s/it]  8%|▊         | 116/1459 [1:07:29<13:23:54, 35.92s/it]  8%|▊         | 117/1459 [1:08:07<13:39:05, 36.62s/it]  8%|▊         | 118/1459 [1:08:40<13:14:36, 35.55s/it]  8%|▊         | 119/1459 [1:09:14<13:03:15, 35.07s/it]  8%|▊         | 120/1459 [1:09:47<12:43:42, 34.22s/it]                                                         8%|▊         | 120/1459 [1:09:47<12:43:42, 34.22s/it]  8%|▊         | 121/1459 [1:10:22<12:47:54, 34.44s/it]  8%|▊         | 122/1459 [1:10:54<12:32:39, 33.78s/it]  8%|▊         | 123/1459 [1:11:25<12:15:57, 33.05s/it]  8%|▊         | 124/1459 [1:12:05<12:58:17, 34.98s/it]  9%|▊         | 125/1459 [1:12:42<13:15:13, 35.77s/it]  9%|▊         | 126/1459 [1:13:18<13:12:21, 35.67s/it]  9%|▊         | 127/1459 [1:13:51<12:57:02, 35.00s/it]  9%|▉         | 128/1459 [1:14:25<12:51:49, 34.79s/it]  9%|▉         | 129/1459 [1:14:56<12:22:56, 33.52s/it]  9%|▉         | 130/1459 [1:15:30<12:29:05, 33.82s/it]                                                         9%|▉         | 130/1459 [1:15:30<12:29:05, 33.82s/it]  9%|▉         | 131/1459 [1:16:08<12:51:11, 34.84s/it]  9%|▉         | 132/1459 [1:16:44<12:59:36, 35.25s/it]  9%|▉         | 133/1459 [1:17:23<13:25:21, 36.44s/it]  9%|▉         | 134/1459 [1:17:57<13:05:06, 35.55s/it]  9%|▉         | 135/1459 [1:18:34<13:15:07, 36.03s/it]  9%|▉         | 136/1459 [1:19:10<13:13:25, 35.98s/it]  9%|▉         | 137/1459 [1:19:42<12:49:32, 34.93s/it]  9%|▉         | 138/1459 [1:20:16<12:43:02, 34.66s/it] 10%|▉         | 139/1459 [1:20:48<12:23:14, 33.78s/it] 10%|▉         | 140/1459 [1:21:25<12:46:24, 34.86s/it]                                                        10%|▉         | 140/1459 [1:21:25<12:46:24, 34.86s/it] 10%|▉         | 141/1459 [1:21:58<12:29:17, 34.11s/it] 10%|▉         | 142/1459 [1:22:30<12:17:50, 33.61s/it] 10%|▉         | 143/1459 [1:23:17<13:44:13, 37.58s/it] 10%|▉         | 144/1459 [1:23:51<13:21:17, 36.56s/it] 10%|▉         | 145/1459 [1:24:29<13:29:19, 36.96s/it] 10%|█         | 146/1459 [1:25:04<13:16:01, 36.38s/it] 10%|█         | 147/1459 [1:25:37<12:55:53, 35.48s/it] 10%|█         | 148/1459 [1:26:13<12:59:26, 35.67s/it] 10%|█         | 149/1459 [1:26:50<13:03:35, 35.89s/it] 10%|█         | 150/1459 [1:27:31<13:39:12, 37.55s/it]                                                        10%|█         | 150/1459 [1:27:31<13:39:12, 37.55s/it] 10%|█         | 151/1459 [1:28:03<12:58:09, 35.70s/it] 10%|█         | 152/1459 [1:28:38<12:58:05, 35.72s/it] 10%|█         | 153/1459 [1:29:13<12:50:50, 35.41s/it] 11%|█         | 154/1459 [1:29:48<12:46:05, 35.22s/it] 11%|█         | 155/1459 [1:30:22<12:35:09, 34.75s/it] 11%|█         | 156/1459 [1:30:55<12:25:12, 34.31s/it] 11%|█         | 157/1459 [1:31:30<12:29:36, 34.54s/it] 11%|█         | 158/1459 [1:32:01<12:06:56, 33.53s/it] 11%|█         | 159/1459 [1:32:34<12:03:12, 33.38s/it] 11%|█         | 160/1459 [1:33:12<12:29:22, 34.61s/it]                                                        11%|█         | 160/1459 [1:33:12<12:29:22, 34.61s/it] 11%|█         | 161/1459 [1:33:43<12:09:28, 33.72s/it] 11%|█         | 162/1459 [1:34:16<12:02:06, 33.41s/it] 11%|█         | 163/1459 [1:34:49<11:57:07, 33.20s/it] 11%|█         | 164/1459 [1:35:19<11:40:20, 32.45s/it] 11%|█▏        | 165/1459 [1:36:00<12:31:50, 34.86s/it] 11%|█▏        | 166/1459 [1:36:33<12:22:47, 34.47s/it] 11%|█▏        | 167/1459 [1:37:08<12:26:34, 34.67s/it] 12%|█▏        | 168/1459 [1:37:45<12:37:10, 35.19s/it] 12%|█▏        | 169/1459 [1:38:18<12:24:26, 34.63s/it] 12%|█▏        | 170/1459 [1:38:56<12:42:05, 35.47s/it]                                                        12%|█▏        | 170/1459 [1:38:56<12:42:05, 35.47s/it] 12%|█▏        | 171/1459 [1:39:29<12:25:31, 34.73s/it] 12%|█▏        | 172/1459 [1:40:04<12:29:51, 34.96s/it] 12%|█▏        | 173/1459 [1:40:42<12:45:30, 35.72s/it] 12%|█▏        | 174/1459 [1:41:12<12:07:54, 33.99s/it] 12%|█▏        | 175/1459 [1:41:57<13:19:00, 37.34s/it] 12%|█▏        | 176/1459 [1:42:31<13:00:43, 36.51s/it] 12%|█▏        | 177/1459 [1:43:05<12:39:46, 35.56s/it] 12%|█▏        | 178/1459 [1:43:37<12:20:02, 34.66s/it] 12%|█▏        | 179/1459 [1:44:10<12:08:18, 34.14s/it] 12%|█▏        | 180/1459 [1:44:53<13:05:36, 36.85s/it]                                                        12%|█▏        | 180/1459 [1:44:53<13:05:36, 36.85s/it] 12%|█▏        | 181/1459 [1:45:33<13:25:01, 37.79s/it] 12%|█▏        | 182/1459 [1:46:05<12:43:32, 35.88s/it] 13%|█▎        | 183/1459 [1:46:37<12:22:18, 34.90s/it] 13%|█▎        | 184/1459 [1:47:12<12:18:29, 34.75s/it] 13%|█▎        | 185/1459 [1:47:49<12:35:24, 35.58s/it] 13%|█▎        | 186/1459 [1:48:20<12:02:28, 34.05s/it] 13%|█▎        | 187/1459 [1:48:50<11:40:26, 33.04s/it] 13%|█▎        | 188/1459 [1:49:27<12:00:30, 34.01s/it] 13%|█▎        | 189/1459 [1:50:01<12:04:22, 34.22s/it] 13%|█▎        | 190/1459 [1:50:43<12:49:37, 36.39s/it]                                                        13%|█▎        | 190/1459 [1:50:43<12:49:37, 36.39s/it] 13%|█▎        | 191/1459 [1:51:13<12:11:37, 34.62s/it] 13%|█▎        | 192/1459 [1:51:45<11:50:29, 33.65s/it] 13%|█▎        | 193/1459 [1:52:19<11:51:43, 33.73s/it] 13%|█▎        | 194/1459 [1:52:55<12:07:11, 34.49s/it] 13%|█▎        | 195/1459 [1:53:33<12:28:09, 35.51s/it] 13%|█▎        | 196/1459 [1:54:04<11:58:19, 34.12s/it] 14%|█▎        | 197/1459 [1:54:41<12:16:09, 35.00s/it] 14%|█▎        | 198/1459 [1:55:14<12:01:54, 34.35s/it] 14%|█▎        | 199/1459 [1:55:50<12:16:58, 35.09s/it] 14%|█▎        | 200/1459 [1:56:24<12:09:49, 34.78s/it]                                                        14%|█▎        | 200/1459 [1:56:24<12:09:49, 34.78s/it] 14%|█▍        | 201/1459 [1:56:56<11:48:41, 33.80s/it] 14%|█▍        | 202/1459 [1:57:32<12:02:38, 34.49s/it] 14%|█▍        | 203/1459 [1:58:05<11:52:49, 34.05s/it] 14%|█▍        | 204/1459 [1:58:45<12:30:43, 35.89s/it] 14%|█▍        | 205/1459 [1:59:20<12:23:38, 35.58s/it] 14%|█▍        | 206/1459 [1:59:51<11:54:24, 34.21s/it] 14%|█▍        | 207/1459 [2:00:22<11:34:35, 33.29s/it] 14%|█▍        | 208/1459 [2:00:56<11:35:20, 33.35s/it] 14%|█▍        | 209/1459 [2:01:28<11:24:31, 32.86s/it] 14%|█▍        | 210/1459 [2:01:59<11:17:05, 32.53s/it]                                                        14%|█▍        | 210/1459 [2:01:59<11:17:05, 32.53s/it] 14%|█▍        | 211/1459 [2:02:34<11:29:11, 33.13s/it] 15%|█▍        | 212/1459 [2:03:04<11:08:47, 32.18s/it] 15%|█▍        | 213/1459 [2:03:37<11:16:11, 32.56s/it] 15%|█▍        | 214/1459 [2:04:11<11:20:51, 32.81s/it] 15%|█▍        | 215/1459 [2:04:47<11:40:45, 33.80s/it] 15%|█▍        | 216/1459 [2:05:18<11:26:09, 33.12s/it] 15%|█▍        | 217/1459 [2:05:53<11:34:39, 33.56s/it] 15%|█▍        | 218/1459 [2:06:25<11:26:49, 33.21s/it] 15%|█▌        | 219/1459 [2:07:01<11:42:48, 34.01s/it] 15%|█▌        | 220/1459 [2:07:37<11:54:17, 34.59s/it]                                                        15%|█▌        | 220/1459 [2:07:37<11:54:17, 34.59s/it] 15%|█▌        | 221/1459 [2:08:09<11:39:01, 33.88s/it] 15%|█▌        | 222/1459 [2:08:42<11:30:51, 33.51s/it] 15%|█▌        | 223/1459 [2:09:18<11:46:52, 34.31s/it] 15%|█▌        | 224/1459 [2:09:49<11:27:41, 33.41s/it] 15%|█▌        | 225/1459 [2:10:21<11:18:34, 32.99s/it] 15%|█▌        | 226/1459 [2:10:56<11:28:50, 33.52s/it] 16%|█▌        | 227/1459 [2:11:29<11:21:48, 33.20s/it] 16%|█▌        | 228/1459 [2:11:59<11:05:20, 32.43s/it] 16%|█▌        | 229/1459 [2:12:32<11:08:05, 32.59s/it] 16%|█▌        | 230/1459 [2:13:09<11:31:42, 33.77s/it]                                                        16%|█▌        | 230/1459 [2:13:09<11:31:42, 33.77s/it] 16%|█▌        | 231/1459 [2:13:46<11:52:55, 34.83s/it] 16%|█▌        | 232/1459 [2:14:18<11:34:29, 33.96s/it] 16%|█▌        | 233/1459 [2:14:50<11:20:45, 33.32s/it] 16%|█▌        | 234/1459 [2:15:21<11:07:08, 32.68s/it] 16%|█▌        | 235/1459 [2:15:53<10:59:52, 32.35s/it] 16%|█▌        | 236/1459 [2:16:24<10:53:18, 32.05s/it] 16%|█▌        | 237/1459 [2:16:56<10:53:45, 32.10s/it] 16%|█▋        | 238/1459 [2:17:32<11:18:19, 33.33s/it] 16%|█▋        | 239/1459 [2:18:03<10:59:16, 32.42s/it] 16%|█▋        | 240/1459 [2:18:34<10:53:04, 32.14s/it]                                                        16%|█▋        | 240/1459 [2:18:34<10:53:04, 32.14s/it] 17%|█▋        | 241/1459 [2:19:10<11:15:18, 33.27s/it] 17%|█▋        | 242/1459 [2:19:42<11:07:18, 32.90s/it] 17%|█▋        | 243/1459 [2:20:18<11:23:22, 33.72s/it] 17%|█▋        | 244/1459 [2:20:53<11:32:13, 34.18s/it] 17%|█▋        | 245/1459 [2:21:27<11:29:14, 34.06s/it] 17%|█▋        | 246/1459 [2:22:04<11:47:04, 34.97s/it] 17%|█▋        | 247/1459 [2:22:37<11:32:48, 34.30s/it] 17%|█▋        | 248/1459 [2:23:09<11:23:21, 33.86s/it] 17%|█▋        | 249/1459 [2:23:43<11:21:49, 33.81s/it] 17%|█▋        | 250/1459 [2:24:21<11:43:42, 34.92s/it]                                                        17%|█▋        | 250/1459 [2:24:21<11:43:42, 34.92s/it] 17%|█▋        | 251/1459 [2:24:57<11:50:09, 35.27s/it] 17%|█▋        | 252/1459 [2:25:29<11:32:53, 34.44s/it] 17%|█▋        | 253/1459 [2:26:07<11:49:44, 35.31s/it] 17%|█▋        | 254/1459 [2:26:39<11:33:28, 34.53s/it] 17%|█▋        | 255/1459 [2:27:16<11:43:42, 35.07s/it] 18%|█▊        | 256/1459 [2:27:49<11:35:11, 34.67s/it] 18%|█▊        | 257/1459 [2:28:25<11:43:11, 35.10s/it] 18%|█▊        | 258/1459 [2:28:59<11:31:48, 34.56s/it] 18%|█▊        | 259/1459 [2:29:36<11:48:26, 35.42s/it] 18%|█▊        | 260/1459 [2:30:19<12:31:20, 37.60s/it]                                                        18%|█▊        | 260/1459 [2:30:19<12:31:20, 37.60s/it] 18%|█▊        | 261/1459 [2:30:55<12:22:33, 37.19s/it] 18%|█▊        | 262/1459 [2:31:31<12:16:36, 36.92s/it] 18%|█▊        | 263/1459 [2:32:05<11:55:28, 35.89s/it] 18%|█▊        | 264/1459 [2:32:40<11:50:06, 35.65s/it] 18%|█▊        | 265/1459 [2:33:18<12:02:16, 36.30s/it] 18%|█▊        | 266/1459 [2:33:52<11:46:55, 35.55s/it] 18%|█▊        | 267/1459 [2:34:26<11:37:34, 35.11s/it] 18%|█▊        | 268/1459 [2:34:57<11:12:34, 33.88s/it] 18%|█▊        | 269/1459 [2:35:30<11:11:20, 33.85s/it] 19%|█▊        | 270/1459 [2:36:03<11:00:49, 33.35s/it]                                                        19%|█▊        | 270/1459 [2:36:03<11:00:49, 33.35s/it] 19%|█▊        | 271/1459 [2:36:33<10:45:21, 32.59s/it] 19%|█▊        | 272/1459 [2:37:07<10:49:35, 32.84s/it] 19%|█▊        | 273/1459 [2:37:47<11:34:39, 35.14s/it] 19%|█▉        | 274/1459 [2:38:21<11:25:37, 34.72s/it] 19%|█▉        | 275/1459 [2:38:55<11:23:03, 34.61s/it] 19%|█▉        | 276/1459 [2:39:33<11:40:07, 35.51s/it] 19%|█▉        | 277/1459 [2:40:08<11:35:42, 35.32s/it] 19%|█▉        | 278/1459 [2:40:46<11:51:27, 36.14s/it] 19%|█▉        | 279/1459 [2:41:20<11:36:16, 35.40s/it] 19%|█▉        | 280/1459 [2:41:53<11:21:36, 34.69s/it]                                                        19%|█▉        | 280/1459 [2:41:53<11:21:36, 34.69s/it] 19%|█▉        | 281/1459 [2:42:26<11:11:15, 34.19s/it] 19%|█▉        | 282/1459 [2:43:00<11:08:36, 34.08s/it] 19%|█▉        | 283/1459 [2:43:36<11:22:36, 34.83s/it] 19%|█▉        | 284/1459 [2:44:09<11:11:55, 34.31s/it] 20%|█▉        | 285/1459 [2:44:44<11:11:12, 34.30s/it] 20%|█▉        | 286/1459 [2:45:15<10:55:44, 33.54s/it] 20%|█▉        | 287/1459 [2:45:49<10:54:08, 33.49s/it] 20%|█▉        | 288/1459 [2:46:23<10:57:50, 33.71s/it] 20%|█▉        | 289/1459 [2:46:58<11:04:16, 34.07s/it] 20%|█▉        | 290/1459 [2:47:30<10:51:14, 33.43s/it]                                                        20%|█▉        | 290/1459 [2:47:30<10:51:14, 33.43s/it] 20%|█▉        | 291/1459 [2:48:03<10:52:43, 33.53s/it] 20%|██        | 292/1459 [2:48:43<11:27:18, 35.34s/it] 20%|██        | 293/1459 [2:49:15<11:07:09, 34.33s/it] 20%|██        | 294/1459 [2:49:54<11:32:13, 35.65s/it] 20%|██        | 295/1459 [2:50:24<11:02:48, 34.17s/it] 20%|██        | 296/1459 [2:50:57<10:53:01, 33.69s/it] 20%|██        | 297/1459 [2:51:29<10:43:45, 33.24s/it] 20%|██        | 298/1459 [2:52:00<10:30:04, 32.56s/it] 20%|██        | 299/1459 [2:52:32<10:24:59, 32.33s/it] 21%|██        | 300/1459 [2:53:06<10:33:54, 32.82s/it]                                                        21%|██        | 300/1459 [2:53:06<10:33:54, 32.82s/it] 21%|██        | 301/1459 [2:53:40<10:39:07, 33.12s/it] 21%|██        | 302/1459 [2:54:17<11:01:18, 34.29s/it] 21%|██        | 303/1459 [2:54:51<11:01:15, 34.32s/it] 21%|██        | 304/1459 [2:55:26<11:02:53, 34.44s/it] 21%|██        | 305/1459 [2:56:02<11:12:41, 34.97s/it] 21%|██        | 306/1459 [2:56:39<11:21:22, 35.46s/it] 21%|██        | 307/1459 [2:57:13<11:15:03, 35.16s/it] 21%|██        | 308/1459 [2:57:50<11:22:20, 35.57s/it] 21%|██        | 309/1459 [2:58:23<11:06:04, 34.75s/it] 21%|██        | 310/1459 [2:58:53<10:41:37, 33.51s/it]                                                        21%|██        | 310/1459 [2:58:53<10:41:37, 33.51s/it] 21%|██▏       | 311/1459 [2:59:29<10:57:28, 34.36s/it] 21%|██▏       | 312/1459 [3:00:07<11:15:07, 35.32s/it] 21%|██▏       | 313/1459 [3:00:45<11:27:40, 36.00s/it] 22%|██▏       | 314/1459 [3:01:17<11:08:33, 35.03s/it] 22%|██▏       | 315/1459 [3:01:54<11:16:38, 35.49s/it] 22%|██▏       | 316/1459 [3:02:37<11:58:33, 37.72s/it] 22%|██▏       | 317/1459 [3:03:09<11:27:20, 36.11s/it] 22%|██▏       | 318/1459 [3:03:46<11:32:46, 36.43s/it] 22%|██▏       | 319/1459 [3:04:21<11:19:36, 35.77s/it] 22%|██▏       | 320/1459 [3:04:57<11:20:40, 35.86s/it]                                                        22%|██▏       | 320/1459 [3:04:57<11:20:40, 35.86s/it] 22%|██▏       | 321/1459 [3:05:28<10:54:22, 34.50s/it] 22%|██▏       | 322/1459 [3:06:02<10:48:30, 34.22s/it] 22%|██▏       | 323/1459 [3:06:39<11:05:33, 35.15s/it] 22%|██▏       | 324/1459 [3:07:15<11:08:20, 35.33s/it] 22%|██▏       | 325/1459 [3:07:47<10:53:07, 34.56s/it] 22%|██▏       | 326/1459 [3:08:27<11:18:26, 35.93s/it] 22%|██▏       | 327/1459 [3:09:01<11:09:23, 35.48s/it] 22%|██▏       | 328/1459 [3:09:34<10:54:16, 34.71s/it] 23%|██▎       | 329/1459 [3:10:06<10:38:52, 33.92s/it] 23%|██▎       | 330/1459 [3:10:46<11:13:17, 35.78s/it]                                                        23%|██▎       | 330/1459 [3:10:46<11:13:17, 35.78s/it] 23%|██▎       | 331/1459 [3:11:20<11:03:08, 35.27s/it] 23%|██▎       | 332/1459 [3:11:52<10:42:11, 34.19s/it] 23%|██▎       | 333/1459 [3:12:26<10:38:51, 34.04s/it] 23%|██▎       | 334/1459 [3:13:04<11:00:54, 35.25s/it] 23%|██▎       | 335/1459 [3:13:36<10:43:35, 34.36s/it] 23%|██▎       | 336/1459 [3:14:09<10:37:00, 34.03s/it] 23%|██▎       | 337/1459 [3:14:43<10:36:12, 34.02s/it] 23%|██▎       | 338/1459 [3:15:15<10:25:08, 33.46s/it] 23%|██▎       | 339/1459 [3:15:47<10:13:42, 32.88s/it] 23%|██▎       | 340/1459 [3:16:24<10:39:50, 34.31s/it]                                                        23%|██▎       | 340/1459 [3:16:24<10:39:50, 34.31s/it] 23%|██▎       | 341/1459 [3:16:55<10:19:23, 33.24s/it] 23%|██▎       | 342/1459 [3:17:34<10:48:34, 34.84s/it] 24%|██▎       | 343/1459 [3:18:10<10:53:00, 35.11s/it] 24%|██▎       | 344/1459 [3:18:43<10:43:26, 34.62s/it] 24%|██▎       | 345/1459 [3:19:20<10:53:53, 35.22s/it] 24%|██▎       | 346/1459 [3:19:51<10:33:41, 34.16s/it] 24%|██▍       | 347/1459 [3:20:26<10:34:33, 34.24s/it] 24%|██▍       | 348/1459 [3:21:01<10:37:19, 34.42s/it] 24%|██▍       | 349/1459 [3:21:36<10:39:53, 34.59s/it] 24%|██▍       | 350/1459 [3:22:07<10:22:00, 33.65s/it]                                                        24%|██▍       | 350/1459 [3:22:07<10:22:00, 33.65s/it] 24%|██▍       | 351/1459 [3:22:41<10:25:29, 33.87s/it] 24%|██▍       | 352/1459 [3:23:20<10:48:45, 35.16s/it] 24%|██▍       | 353/1459 [3:23:58<11:05:49, 36.12s/it] 24%|██▍       | 354/1459 [3:24:33<11:01:31, 35.92s/it] 24%|██▍       | 355/1459 [3:25:04<10:33:08, 34.41s/it] 24%|██▍       | 356/1459 [3:25:36<10:18:56, 33.67s/it] 24%|██▍       | 357/1459 [3:26:10<10:21:30, 33.84s/it] 25%|██▍       | 358/1459 [3:26:47<10:34:09, 34.56s/it] 25%|██▍       | 359/1459 [3:27:19<10:22:15, 33.94s/it] 25%|██▍       | 360/1459 [3:27:52<10:17:33, 33.72s/it]                                                        25%|██▍       | 360/1459 [3:27:52<10:17:33, 33.72s/it] 25%|██▍       | 361/1459 [3:28:32<10:50:31, 35.55s/it] 25%|██▍       | 362/1459 [3:29:06<10:40:39, 35.04s/it] 25%|██▍       | 363/1459 [3:29:52<11:39:47, 38.31s/it] 25%|██▍       | 364/1459 [3:30:25<11:10:32, 36.74s/it] 25%|██▌       | 365/1459 [3:31:07<11:37:10, 38.24s/it] 25%|██▌       | 366/1459 [3:31:45<11:36:19, 38.22s/it] 25%|██▌       | 367/1459 [3:32:21<11:25:05, 37.64s/it] 25%|██▌       | 368/1459 [3:32:54<10:54:52, 36.02s/it] 25%|██▌       | 369/1459 [3:33:27<10:40:27, 35.25s/it] 25%|██▌       | 370/1459 [3:33:59<10:24:40, 34.42s/it]                                                        25%|██▌       | 370/1459 [3:33:59<10:24:40, 34.42s/it] 25%|██▌       | 371/1459 [3:34:29<9:58:35, 33.01s/it]  25%|██▌       | 372/1459 [3:35:03<10:03:17, 33.30s/it] 26%|██▌       | 373/1459 [3:35:38<10:10:58, 33.76s/it] 26%|██▌       | 374/1459 [3:36:10<10:02:05, 33.30s/it] 26%|██▌       | 375/1459 [3:36:42<9:53:10, 32.83s/it]  26%|██▌       | 376/1459 [3:37:18<10:12:28, 33.93s/it] 26%|██▌       | 377/1459 [3:37:56<10:29:04, 34.88s/it] 26%|██▌       | 378/1459 [3:38:32<10:38:17, 35.43s/it] 26%|██▌       | 379/1459 [3:39:11<10:54:58, 36.39s/it] 26%|██▌       | 380/1459 [3:39:45<10:44:13, 35.82s/it]                                                        26%|██▌       | 380/1459 [3:39:45<10:44:13, 35.82s/it] 26%|██▌       | 381/1459 [3:40:22<10:45:49, 35.95s/it] 26%|██▌       | 382/1459 [3:40:56<10:39:14, 35.61s/it] 26%|██▋       | 383/1459 [3:41:28<10:18:28, 34.49s/it] 26%|██▋       | 384/1459 [3:42:01<10:06:30, 33.85s/it] 26%|██▋       | 385/1459 [3:42:32<9:54:29, 33.21s/it]  26%|██▋       | 386/1459 [3:43:04<9:44:49, 32.70s/it] 27%|██▋       | 387/1459 [3:43:36<9:39:52, 32.46s/it] 27%|██▋       | 388/1459 [3:44:11<9:54:41, 33.32s/it] 27%|██▋       | 389/1459 [3:44:48<10:13:31, 34.40s/it] 27%|██▋       | 390/1459 [3:45:21<10:04:09, 33.91s/it]                                                        27%|██▋       | 390/1459 [3:45:21<10:04:09, 33.91s/it] 27%|██▋       | 391/1459 [3:45:53<9:53:41, 33.35s/it]  27%|██▋       | 392/1459 [3:46:27<9:58:36, 33.66s/it] 27%|██▋       | 393/1459 [3:46:59<9:47:41, 33.08s/it] 27%|██▋       | 394/1459 [3:47:30<9:33:44, 32.32s/it] 27%|██▋       | 395/1459 [3:48:04<9:45:44, 33.03s/it] 27%|██▋       | 396/1459 [3:48:43<10:15:40, 34.75s/it] 27%|██▋       | 397/1459 [3:49:21<10:33:00, 35.76s/it] 27%|██▋       | 398/1459 [3:49:54<10:17:28, 34.92s/it] 27%|██▋       | 399/1459 [3:50:25<9:53:10, 33.58s/it]  27%|██▋       | 400/1459 [3:50:59<9:56:32, 33.80s/it]                                                       27%|██▋       | 400/1459 [3:50:59<9:56:32, 33.80s/it] 27%|██▋       | 401/1459 [3:51:32<9:50:29, 33.49s/it] 28%|██▊       | 402/1459 [3:52:07<9:58:21, 33.97s/it] 28%|██▊       | 403/1459 [3:52:42<10:02:20, 34.22s/it] 28%|██▊       | 404/1459 [3:53:13<9:45:57, 33.32s/it]  28%|██▊       | 405/1459 [3:53:45<9:38:30, 32.93s/it] 28%|██▊       | 406/1459 [3:54:16<9:30:30, 32.51s/it] 28%|██▊       | 407/1459 [3:54:50<9:38:56, 33.02s/it] 28%|██▊       | 408/1459 [3:55:27<9:54:49, 33.96s/it] 28%|██▊       | 409/1459 [3:56:00<9:51:58, 33.83s/it] 28%|██▊       | 410/1459 [3:56:40<10:20:35, 35.50s/it]                                                        28%|██▊       | 410/1459 [3:56:40<10:20:35, 35.50s/it] 28%|██▊       | 411/1459 [3:57:15<10:19:10, 35.45s/it] 28%|██▊       | 412/1459 [3:57:51<10:19:29, 35.50s/it] 28%|██▊       | 413/1459 [3:58:25<10:14:02, 35.22s/it] 28%|██▊       | 414/1459 [3:59:00<10:12:06, 35.14s/it] 28%|██▊       | 415/1459 [3:59:32<9:56:36, 34.29s/it]  29%|██▊       | 416/1459 [4:00:05<9:46:54, 33.76s/it] 29%|██▊       | 417/1459 [4:00:38<9:41:48, 33.50s/it] 29%|██▊       | 418/1459 [4:01:12<9:45:06, 33.72s/it] 29%|██▊       | 419/1459 [4:01:47<9:50:12, 34.05s/it] 29%|██▉       | 420/1459 [4:02:22<9:55:15, 34.37s/it]                                                       29%|██▉       | 420/1459 [4:02:22<9:55:15, 34.37s/it] 29%|██▉       | 421/1459 [4:02:57<10:00:49, 34.73s/it] 29%|██▉       | 422/1459 [4:03:27<9:32:59, 33.15s/it]  29%|██▉       | 423/1459 [4:04:00<9:30:53, 33.06s/it] 29%|██▉       | 424/1459 [4:04:36<9:47:27, 34.06s/it] 29%|██▉       | 425/1459 [4:05:06<9:24:11, 32.74s/it] 29%|██▉       | 426/1459 [4:05:40<9:32:46, 33.27s/it] 29%|██▉       | 427/1459 [4:06:13<9:27:03, 32.97s/it] 29%|██▉       | 428/1459 [4:06:45<9:22:31, 32.74s/it] 29%|██▉       | 429/1459 [4:07:18<9:22:50, 32.79s/it] 29%|██▉       | 430/1459 [4:07:48<9:08:16, 31.97s/it]                                                       29%|██▉       | 430/1459 [4:07:48<9:08:16, 31.97s/it] 30%|██▉       | 431/1459 [4:08:30<10:01:03, 35.08s/it] 30%|██▉       | 432/1459 [4:09:00<9:32:44, 33.46s/it]  30%|██▉       | 433/1459 [4:09:31<9:21:06, 32.81s/it] 30%|██▉       | 434/1459 [4:10:08<9:40:37, 33.99s/it] 30%|██▉       | 435/1459 [4:10:43<9:47:00, 34.40s/it] 30%|██▉       | 436/1459 [4:11:15<9:32:38, 33.59s/it] 30%|██▉       | 437/1459 [4:11:49<9:33:01, 33.64s/it] 30%|███       | 438/1459 [4:12:26<9:52:53, 34.84s/it] 30%|███       | 439/1459 [4:13:01<9:51:43, 34.81s/it] 30%|███       | 440/1459 [4:13:34<9:40:41, 34.19s/it]                                                       30%|███       | 440/1459 [4:13:34<9:40:41, 34.19s/it] 30%|███       | 441/1459 [4:14:05<9:27:04, 33.42s/it] 30%|███       | 442/1459 [4:14:45<9:58:27, 35.31s/it] 30%|███       | 443/1459 [4:15:16<9:36:30, 34.05s/it] 30%|███       | 444/1459 [4:15:49<9:31:14, 33.77s/it] 31%|███       | 445/1459 [4:16:23<9:29:33, 33.70s/it] 31%|███       | 446/1459 [4:17:00<9:45:53, 34.70s/it] 31%|███       | 447/1459 [4:17:31<9:28:28, 33.70s/it] 31%|███       | 448/1459 [4:18:04<9:22:18, 33.37s/it] 31%|███       | 449/1459 [4:18:35<9:08:44, 32.60s/it] 31%|███       | 450/1459 [4:19:07<9:08:40, 32.63s/it]                                                       31%|███       | 450/1459 [4:19:07<9:08:40, 32.63s/it] 31%|███       | 451/1459 [4:19:41<9:14:46, 33.02s/it] 31%|███       | 452/1459 [4:20:18<9:34:46, 34.25s/it] 31%|███       | 453/1459 [4:20:49<9:16:03, 33.16s/it] 31%|███       | 454/1459 [4:21:24<9:25:38, 33.77s/it] 31%|███       | 455/1459 [4:22:00<9:34:37, 34.34s/it] 31%|███▏      | 456/1459 [4:22:30<9:12:53, 33.07s/it] 31%|███▏      | 457/1459 [4:23:09<9:40:04, 34.73s/it] 31%|███▏      | 458/1459 [4:23:42<9:33:20, 34.37s/it] 31%|███▏      | 459/1459 [4:24:12<9:10:53, 33.05s/it] 32%|███▏      | 460/1459 [4:24:45<9:08:12, 32.93s/it]                                                       32%|███▏      | 460/1459 [4:24:45<9:08:12, 32.93s/it] 32%|███▏      | 461/1459 [4:25:15<8:56:03, 32.23s/it] 32%|███▏      | 462/1459 [4:25:46<8:46:14, 31.67s/it] 32%|███▏      | 463/1459 [4:26:17<8:43:25, 31.53s/it] 32%|███▏      | 464/1459 [4:26:59<9:34:08, 34.62s/it] 32%|███▏      | 465/1459 [4:27:32<9:27:34, 34.26s/it] 32%|███▏      | 466/1459 [4:28:07<9:29:19, 34.40s/it] 32%|███▏      | 467/1459 [4:28:40<9:21:37, 33.97s/it] 32%|███▏      | 468/1459 [4:29:15<9:25:22, 34.23s/it] 32%|███▏      | 469/1459 [4:29:46<9:10:55, 33.39s/it] 32%|███▏      | 470/1459 [4:30:19<9:08:52, 33.30s/it]                                                       32%|███▏      | 470/1459 [4:30:19<9:08:52, 33.30s/it] 32%|███▏      | 471/1459 [4:30:56<9:25:51, 34.36s/it] 32%|███▏      | 472/1459 [4:31:32<9:31:02, 34.71s/it] 32%|███▏      | 473/1459 [4:32:05<9:23:09, 34.27s/it] 32%|███▏      | 474/1459 [4:32:43<9:40:34, 35.37s/it] 33%|███▎      | 475/1459 [4:33:15<9:24:52, 34.44s/it] 33%|███▎      | 476/1459 [4:33:46<9:07:33, 33.42s/it] 33%|███▎      | 477/1459 [4:34:19<9:05:25, 33.33s/it] 33%|███▎      | 478/1459 [4:34:57<9:24:21, 34.52s/it] 33%|███▎      | 479/1459 [4:35:31<9:22:50, 34.46s/it] 33%|███▎      | 480/1459 [4:36:06<9:27:54, 34.81s/it]                                                       33%|███▎      | 480/1459 [4:36:06<9:27:54, 34.81s/it] 33%|███▎      | 481/1459 [4:36:46<9:49:19, 36.15s/it] 33%|███▎      | 482/1459 [4:37:23<9:56:27, 36.63s/it] 33%|███▎      | 483/1459 [4:38:01<10:01:40, 36.99s/it] 33%|███▎      | 484/1459 [4:38:38<9:57:50, 36.79s/it]  33%|███▎      | 485/1459 [4:39:10<9:34:25, 35.39s/it] 33%|███▎      | 486/1459 [4:39:43<9:23:42, 34.76s/it] 33%|███▎      | 487/1459 [4:40:19<9:26:40, 34.98s/it] 33%|███▎      | 488/1459 [4:40:51<9:13:05, 34.18s/it] 34%|███▎      | 489/1459 [4:41:22<8:56:53, 33.21s/it] 34%|███▎      | 490/1459 [4:41:59<9:14:23, 34.33s/it]                                                       34%|███▎      | 490/1459 [4:41:59<9:14:23, 34.33s/it] 34%|███▎      | 491/1459 [4:42:29<8:54:45, 33.15s/it] 34%|███▎      | 492/1459 [4:43:00<8:43:59, 32.51s/it] 34%|███▍      | 493/1459 [4:43:35<8:53:37, 33.14s/it] 34%|███▍      | 494/1459 [4:44:13<9:19:11, 34.77s/it] 34%|███▍      | 495/1459 [4:44:47<9:14:38, 34.52s/it] 34%|███▍      | 496/1459 [4:45:24<9:26:35, 35.30s/it] 34%|███▍      | 497/1459 [4:46:02<9:35:53, 35.92s/it] 34%|███▍      | 498/1459 [4:46:35<9:20:56, 35.02s/it] 34%|███▍      | 499/1459 [4:47:11<9:26:19, 35.40s/it] 34%|███▍      | 500/1459 [4:47:42<9:06:49, 34.21s/it]                                                       34%|███▍      | 500/1459 [4:47:42<9:06:49, 34.21s/it][INFO|trainer.py:4289] 2025-10-04 03:38:37,608 >> Saving model checkpoint to /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-500
[INFO|configuration_utils.py:491] 2025-10-04 03:38:37,615 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-500/config.json
[INFO|configuration_utils.py:826] 2025-10-04 03:38:37,616 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-500/generation_config.json
[INFO|modeling_utils.py:4308] 2025-10-04 03:38:56,593 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2394] 2025-10-04 03:38:56,594 >> chat template saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-500/chat_template.jinja
[INFO|tokenization_utils_base.py:2563] 2025-10-04 03:38:56,595 >> tokenizer config file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2572] 2025-10-04 03:38:56,595 >> Special tokens file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-500/special_tokens_map.json
 34%|███▍      | 501/1459 [4:48:50<11:44:04, 44.10s/it] 34%|███▍      | 502/1459 [4:49:24<10:59:21, 41.34s/it] 34%|███▍      | 503/1459 [4:49:57<10:16:15, 38.68s/it] 35%|███▍      | 504/1459 [4:50:35<10:12:11, 38.46s/it] 35%|███▍      | 505/1459 [4:51:13<10:08:16, 38.26s/it] 35%|███▍      | 506/1459 [4:51:47<9:48:03, 37.02s/it]  35%|███▍      | 507/1459 [4:52:19<9:24:55, 35.60s/it] 35%|███▍      | 508/1459 [4:52:53<9:17:11, 35.15s/it] 35%|███▍      | 509/1459 [4:53:28<9:12:45, 34.91s/it] 35%|███▍      | 510/1459 [4:54:00<9:00:40, 34.18s/it]                                                       35%|███▍      | 510/1459 [4:54:00<9:00:40, 34.18s/it] 35%|███▌      | 511/1459 [4:54:38<9:16:31, 35.22s/it] 35%|███▌      | 512/1459 [4:55:11<9:05:56, 34.59s/it] 35%|███▌      | 513/1459 [4:55:48<9:15:56, 35.26s/it] 35%|███▌      | 514/1459 [4:56:22<9:09:26, 34.88s/it] 35%|███▌      | 515/1459 [4:56:59<9:19:55, 35.59s/it] 35%|███▌      | 516/1459 [4:57:31<9:04:36, 34.65s/it] 35%|███▌      | 517/1459 [4:58:04<8:52:44, 33.93s/it] 36%|███▌      | 518/1459 [4:58:45<9:29:31, 36.31s/it] 36%|███▌      | 519/1459 [4:59:18<9:09:56, 35.10s/it] 36%|███▌      | 520/1459 [4:59:50<8:55:49, 34.24s/it]                                                       36%|███▌      | 520/1459 [4:59:50<8:55:49, 34.24s/it] 36%|███▌      | 521/1459 [5:00:22<8:45:39, 33.62s/it] 36%|███▌      | 522/1459 [5:00:53<8:30:42, 32.70s/it] 36%|███▌      | 523/1459 [5:01:30<8:50:14, 33.99s/it] 36%|███▌      | 524/1459 [5:02:01<8:36:23, 33.14s/it] 36%|███▌      | 525/1459 [5:02:30<8:18:45, 32.04s/it] 36%|███▌      | 526/1459 [5:03:05<8:31:28, 32.89s/it] 36%|███▌      | 527/1459 [5:03:36<8:22:38, 32.36s/it] 36%|███▌      | 528/1459 [5:04:08<8:19:37, 32.20s/it] 36%|███▋      | 529/1459 [5:04:42<8:27:44, 32.76s/it] 36%|███▋      | 530/1459 [5:05:19<8:46:54, 34.03s/it]                                                       36%|███▋      | 530/1459 [5:05:19<8:46:54, 34.03s/it] 36%|███▋      | 531/1459 [5:05:53<8:46:18, 34.03s/it] 36%|███▋      | 532/1459 [5:06:30<9:00:08, 34.96s/it] 37%|███▋      | 533/1459 [5:07:06<9:00:28, 35.02s/it] 37%|███▋      | 534/1459 [5:07:39<8:54:36, 34.68s/it] 37%|███▋      | 535/1459 [5:08:10<8:35:38, 33.48s/it] 37%|███▋      | 536/1459 [5:08:42<8:28:57, 33.09s/it] 37%|███▋      | 537/1459 [5:09:18<8:41:24, 33.93s/it] 37%|███▋      | 538/1459 [5:09:54<8:51:25, 34.62s/it] 37%|███▋      | 539/1459 [5:10:27<8:43:17, 34.13s/it] 37%|███▋      | 540/1459 [5:11:06<9:03:04, 35.46s/it]                                                       37%|███▋      | 540/1459 [5:11:06<9:03:04, 35.46s/it] 37%|███▋      | 541/1459 [5:11:40<8:55:13, 34.98s/it] 37%|███▋      | 542/1459 [5:12:13<8:46:02, 34.42s/it] 37%|███▋      | 543/1459 [5:12:56<9:26:06, 37.08s/it] 37%|███▋      | 544/1459 [5:13:30<9:11:30, 36.16s/it] 37%|███▋      | 545/1459 [5:14:09<9:22:36, 36.93s/it] 37%|███▋      | 546/1459 [5:14:41<8:58:27, 35.39s/it] 37%|███▋      | 547/1459 [5:15:13<8:45:15, 34.56s/it] 38%|███▊      | 548/1459 [5:15:48<8:47:07, 34.72s/it] 38%|███▊      | 549/1459 [5:16:27<9:02:20, 35.76s/it] 38%|███▊      | 550/1459 [5:17:02<9:00:57, 35.71s/it]                                                       38%|███▊      | 550/1459 [5:17:02<9:00:57, 35.71s/it] 38%|███▊      | 551/1459 [5:17:37<8:56:32, 35.45s/it] 38%|███▊      | 552/1459 [5:18:12<8:54:46, 35.38s/it] 38%|███▊      | 553/1459 [5:18:47<8:49:46, 35.08s/it] 38%|███▊      | 554/1459 [5:19:19<8:37:38, 34.32s/it] 38%|███▊      | 555/1459 [5:19:50<8:21:14, 33.27s/it] 38%|███▊      | 556/1459 [5:20:30<8:50:21, 35.24s/it] 38%|███▊      | 557/1459 [5:21:06<8:52:48, 35.44s/it] 38%|███▊      | 558/1459 [5:21:43<8:58:02, 35.83s/it] 38%|███▊      | 559/1459 [5:22:17<8:50:10, 35.35s/it] 38%|███▊      | 560/1459 [5:22:47<8:28:48, 33.96s/it]                                                       38%|███▊      | 560/1459 [5:22:47<8:28:48, 33.96s/it] 38%|███▊      | 561/1459 [5:23:24<8:40:46, 34.80s/it] 39%|███▊      | 562/1459 [5:24:01<8:50:12, 35.47s/it] 39%|███▊      | 563/1459 [5:24:36<8:48:17, 35.38s/it] 39%|███▊      | 564/1459 [5:25:09<8:33:39, 34.43s/it] 39%|███▊      | 565/1459 [5:25:39<8:14:04, 33.16s/it] 39%|███▉      | 566/1459 [5:26:10<8:04:28, 32.55s/it] 39%|███▉      | 567/1459 [5:26:45<8:13:35, 33.20s/it] 39%|███▉      | 568/1459 [5:27:21<8:27:14, 34.16s/it] 39%|███▉      | 569/1459 [5:27:54<8:19:39, 33.69s/it] 39%|███▉      | 570/1459 [5:28:31<8:35:09, 34.77s/it]                                                       39%|███▉      | 570/1459 [5:28:31<8:35:09, 34.77s/it] 39%|███▉      | 571/1459 [5:29:03<8:20:56, 33.85s/it] 39%|███▉      | 572/1459 [5:29:35<8:14:04, 33.42s/it] 39%|███▉      | 573/1459 [5:30:07<8:05:52, 32.90s/it] 39%|███▉      | 574/1459 [5:30:41<8:09:56, 33.22s/it] 39%|███▉      | 575/1459 [5:31:12<7:59:15, 32.53s/it] 39%|███▉      | 576/1459 [5:31:41<7:45:46, 31.65s/it] 40%|███▉      | 577/1459 [5:32:16<7:59:22, 32.61s/it] 40%|███▉      | 578/1459 [5:32:51<8:08:25, 33.26s/it] 40%|███▉      | 579/1459 [5:33:22<7:59:23, 32.69s/it] 40%|███▉      | 580/1459 [5:33:58<8:10:44, 33.50s/it]                                                       40%|███▉      | 580/1459 [5:33:58<8:10:44, 33.50s/it] 40%|███▉      | 581/1459 [5:34:29<7:58:55, 32.73s/it] 40%|███▉      | 582/1459 [5:35:02<8:03:06, 33.05s/it] 40%|███▉      | 583/1459 [5:35:37<8:10:09, 33.57s/it] 40%|████      | 584/1459 [5:36:07<7:51:42, 32.35s/it] 40%|████      | 585/1459 [5:36:39<7:52:40, 32.45s/it] 40%|████      | 586/1459 [5:37:19<8:22:49, 34.56s/it] 40%|████      | 587/1459 [5:37:51<8:13:30, 33.96s/it] 40%|████      | 588/1459 [5:38:23<8:02:24, 33.23s/it] 40%|████      | 589/1459 [5:38:58<8:10:35, 33.83s/it] 40%|████      | 590/1459 [5:39:33<8:13:04, 34.04s/it]                                                       40%|████      | 590/1459 [5:39:33<8:13:04, 34.04s/it] 41%|████      | 591/1459 [5:40:12<8:34:10, 35.54s/it] 41%|████      | 592/1459 [5:40:45<8:25:39, 34.99s/it] 41%|████      | 593/1459 [5:41:18<8:12:38, 34.13s/it] 41%|████      | 594/1459 [5:41:50<8:04:07, 33.58s/it] 41%|████      | 595/1459 [5:42:24<8:04:48, 33.67s/it] 41%|████      | 596/1459 [5:42:58<8:08:58, 34.00s/it] 41%|████      | 597/1459 [5:43:32<8:07:58, 33.97s/it] 41%|████      | 598/1459 [5:44:08<8:14:30, 34.46s/it] 41%|████      | 599/1459 [5:44:40<8:03:29, 33.73s/it] 41%|████      | 600/1459 [5:45:22<8:39:23, 36.28s/it]                                                       41%|████      | 600/1459 [5:45:22<8:39:23, 36.28s/it] 41%|████      | 601/1459 [5:46:02<8:52:53, 37.26s/it] 41%|████▏     | 602/1459 [5:46:36<8:41:19, 36.50s/it] 41%|████▏     | 603/1459 [5:47:10<8:27:08, 35.55s/it] 41%|████▏     | 604/1459 [5:47:44<8:22:50, 35.29s/it] 41%|████▏     | 605/1459 [5:48:17<8:11:53, 34.56s/it] 42%|████▏     | 606/1459 [5:48:54<8:21:47, 35.30s/it] 42%|████▏     | 607/1459 [5:49:28<8:15:23, 34.89s/it] 42%|████▏     | 608/1459 [5:50:00<7:59:45, 33.83s/it] 42%|████▏     | 609/1459 [5:50:39<8:21:13, 35.38s/it] 42%|████▏     | 610/1459 [5:51:18<8:38:56, 36.67s/it]                                                       42%|████▏     | 610/1459 [5:51:18<8:38:56, 36.67s/it] 42%|████▏     | 611/1459 [5:51:56<8:44:05, 37.08s/it] 42%|████▏     | 612/1459 [5:52:34<8:43:40, 37.10s/it] 42%|████▏     | 613/1459 [5:53:07<8:27:24, 35.99s/it] 42%|████▏     | 614/1459 [5:53:41<8:16:52, 35.28s/it] 42%|████▏     | 615/1459 [5:54:19<8:31:01, 36.33s/it] 42%|████▏     | 616/1459 [5:54:51<8:09:44, 34.86s/it] 42%|████▏     | 617/1459 [5:55:29<8:21:40, 35.75s/it] 42%|████▏     | 618/1459 [5:56:03<8:17:00, 35.46s/it] 42%|████▏     | 619/1459 [5:56:39<8:19:08, 35.65s/it] 42%|████▏     | 620/1459 [5:57:12<8:04:54, 34.68s/it]                                                       42%|████▏     | 620/1459 [5:57:12<8:04:54, 34.68s/it] 43%|████▎     | 621/1459 [5:57:50<8:17:00, 35.59s/it] 43%|████▎     | 622/1459 [5:58:22<8:01:10, 34.49s/it] 43%|████▎     | 623/1459 [5:58:56<8:02:27, 34.63s/it] 43%|████▎     | 624/1459 [5:59:27<7:44:22, 33.37s/it] 43%|████▎     | 625/1459 [6:00:00<7:41:04, 33.17s/it] 43%|████▎     | 626/1459 [6:00:30<7:27:39, 32.24s/it] 43%|████▎     | 627/1459 [6:01:06<7:44:50, 33.52s/it] 43%|████▎     | 628/1459 [6:01:42<7:54:05, 34.23s/it] 43%|████▎     | 629/1459 [6:02:15<7:49:08, 33.91s/it] 43%|████▎     | 630/1459 [6:02:46<7:34:05, 32.87s/it]                                                       43%|████▎     | 630/1459 [6:02:46<7:34:05, 32.87s/it] 43%|████▎     | 631/1459 [6:03:23<7:53:22, 34.30s/it] 43%|████▎     | 632/1459 [6:03:59<7:59:46, 34.81s/it] 43%|████▎     | 633/1459 [6:04:31<7:45:04, 33.78s/it] 43%|████▎     | 634/1459 [6:05:01<7:30:16, 32.75s/it] 44%|████▎     | 635/1459 [6:05:32<7:21:12, 32.13s/it] 44%|████▎     | 636/1459 [6:06:09<7:43:00, 33.76s/it] 44%|████▎     | 637/1459 [6:06:41<7:32:55, 33.06s/it] 44%|████▎     | 638/1459 [6:07:14<7:34:24, 33.21s/it] 44%|████▍     | 639/1459 [6:07:47<7:33:59, 33.22s/it] 44%|████▍     | 640/1459 [6:08:26<7:54:15, 34.74s/it]                                                       44%|████▍     | 640/1459 [6:08:26<7:54:15, 34.74s/it] 44%|████▍     | 641/1459 [6:08:58<7:41:55, 33.88s/it] 44%|████▍     | 642/1459 [6:09:32<7:41:33, 33.90s/it] 44%|████▍     | 643/1459 [6:10:05<7:39:42, 33.80s/it] 44%|████▍     | 644/1459 [6:10:37<7:29:14, 33.07s/it] 44%|████▍     | 645/1459 [6:11:08<7:21:44, 32.56s/it] 44%|████▍     | 646/1459 [6:11:51<8:02:19, 35.60s/it] 44%|████▍     | 647/1459 [6:12:24<7:53:03, 34.95s/it] 44%|████▍     | 648/1459 [6:12:59<7:54:19, 35.09s/it] 44%|████▍     | 649/1459 [6:13:32<7:42:36, 34.27s/it] 45%|████▍     | 650/1459 [6:14:03<7:30:47, 33.43s/it]                                                       45%|████▍     | 650/1459 [6:14:03<7:30:47, 33.43s/it] 45%|████▍     | 651/1459 [6:14:38<7:33:30, 33.68s/it] 45%|████▍     | 652/1459 [6:15:12<7:36:09, 33.92s/it] 45%|████▍     | 653/1459 [6:15:45<7:29:59, 33.50s/it] 45%|████▍     | 654/1459 [6:16:18<7:28:50, 33.45s/it] 45%|████▍     | 655/1459 [6:16:50<7:23:03, 33.06s/it] 45%|████▍     | 656/1459 [6:17:24<7:25:40, 33.30s/it] 45%|████▌     | 657/1459 [6:18:00<7:37:47, 34.25s/it] 45%|████▌     | 658/1459 [6:18:31<7:23:04, 33.19s/it] 45%|████▌     | 659/1459 [6:19:01<7:11:14, 32.34s/it] 45%|████▌     | 660/1459 [6:19:37<7:23:10, 33.28s/it]                                                       45%|████▌     | 660/1459 [6:19:37<7:23:10, 33.28s/it] 45%|████▌     | 661/1459 [6:20:09<7:19:16, 33.03s/it] 45%|████▌     | 662/1459 [6:20:41<7:14:58, 32.75s/it] 45%|████▌     | 663/1459 [6:21:15<7:19:11, 33.10s/it] 46%|████▌     | 664/1459 [6:21:49<7:20:00, 33.21s/it] 46%|████▌     | 665/1459 [6:22:20<7:10:49, 32.56s/it] 46%|████▌     | 666/1459 [6:22:56<7:24:21, 33.62s/it] 46%|████▌     | 667/1459 [6:23:29<7:22:27, 33.52s/it] 46%|████▌     | 668/1459 [6:24:04<7:28:04, 33.99s/it] 46%|████▌     | 669/1459 [6:24:45<7:53:53, 35.99s/it] 46%|████▌     | 670/1459 [6:25:19<7:46:59, 35.51s/it]                                                       46%|████▌     | 670/1459 [6:25:19<7:46:59, 35.51s/it] 46%|████▌     | 671/1459 [6:25:51<7:32:04, 34.42s/it] 46%|████▌     | 672/1459 [6:26:20<7:09:44, 32.76s/it] 46%|████▌     | 673/1459 [6:26:53<7:09:54, 32.82s/it] 46%|████▌     | 674/1459 [6:27:25<7:05:05, 32.49s/it] 46%|████▋     | 675/1459 [6:27:58<7:06:20, 32.63s/it] 46%|████▋     | 676/1459 [6:28:35<7:23:22, 33.98s/it] 46%|████▋     | 677/1459 [6:29:09<7:23:57, 34.06s/it] 46%|████▋     | 678/1459 [6:29:44<7:27:59, 34.42s/it] 47%|████▋     | 679/1459 [6:30:16<7:14:52, 33.45s/it] 47%|████▋     | 680/1459 [6:30:51<7:22:06, 34.05s/it]                                                       47%|████▋     | 680/1459 [6:30:51<7:22:06, 34.05s/it] 47%|████▋     | 681/1459 [6:31:31<7:43:59, 35.78s/it] 47%|████▋     | 682/1459 [6:32:11<7:58:47, 36.97s/it] 47%|████▋     | 683/1459 [6:32:42<7:38:04, 35.42s/it] 47%|████▋     | 684/1459 [6:33:17<7:34:30, 35.19s/it] 47%|████▋     | 685/1459 [6:33:48<7:16:44, 33.86s/it] 47%|████▋     | 686/1459 [6:34:28<7:41:30, 35.82s/it] 47%|████▋     | 687/1459 [6:34:58<7:18:25, 34.07s/it] 47%|████▋     | 688/1459 [6:35:30<7:09:04, 33.39s/it] 47%|████▋     | 689/1459 [6:36:01<6:57:56, 32.57s/it] 47%|████▋     | 690/1459 [6:36:32<6:54:02, 32.30s/it]                                                       47%|████▋     | 690/1459 [6:36:32<6:54:02, 32.30s/it] 47%|████▋     | 691/1459 [6:37:04<6:49:09, 31.97s/it] 47%|████▋     | 692/1459 [6:37:36<6:52:17, 32.25s/it] 47%|████▋     | 693/1459 [6:38:11<7:01:37, 33.03s/it] 48%|████▊     | 694/1459 [6:38:48<7:14:40, 34.09s/it] 48%|████▊     | 695/1459 [6:39:27<7:33:44, 35.63s/it] 48%|████▊     | 696/1459 [6:39:59<7:19:12, 34.54s/it] 48%|████▊     | 697/1459 [6:40:40<7:44:43, 36.59s/it] 48%|████▊     | 698/1459 [6:41:13<7:27:39, 35.29s/it] 48%|████▊     | 699/1459 [6:41:48<7:25:31, 35.17s/it] 48%|████▊     | 700/1459 [6:42:19<7:11:09, 34.08s/it]                                                       48%|████▊     | 700/1459 [6:42:19<7:11:09, 34.08s/it] 48%|████▊     | 701/1459 [6:42:49<6:54:39, 32.82s/it] 48%|████▊     | 702/1459 [6:43:20<6:47:10, 32.27s/it] 48%|████▊     | 703/1459 [6:43:49<6:35:09, 31.36s/it] 48%|████▊     | 704/1459 [6:44:23<6:43:42, 32.08s/it] 48%|████▊     | 705/1459 [6:44:54<6:39:14, 31.77s/it] 48%|████▊     | 706/1459 [6:45:29<6:50:55, 32.74s/it] 48%|████▊     | 707/1459 [6:46:08<7:14:23, 34.66s/it] 49%|████▊     | 708/1459 [6:46:42<7:09:58, 34.35s/it] 49%|████▊     | 709/1459 [6:47:15<7:03:06, 33.85s/it] 49%|████▊     | 710/1459 [6:47:49<7:03:36, 33.93s/it]                                                       49%|████▊     | 710/1459 [6:47:49<7:03:36, 33.93s/it] 49%|████▊     | 711/1459 [6:48:22<7:02:32, 33.89s/it] 49%|████▉     | 712/1459 [6:48:56<7:01:48, 33.88s/it] 49%|████▉     | 713/1459 [6:49:27<6:47:34, 32.78s/it] 49%|████▉     | 714/1459 [6:50:00<6:51:15, 33.12s/it] 49%|████▉     | 715/1459 [6:50:34<6:52:15, 33.25s/it] 49%|████▉     | 716/1459 [6:51:14<7:17:04, 35.30s/it] 49%|████▉     | 717/1459 [6:51:45<6:58:36, 33.85s/it] 49%|████▉     | 718/1459 [6:52:18<6:55:39, 33.66s/it] 49%|████▉     | 719/1459 [6:52:51<6:51:56, 33.40s/it] 49%|████▉     | 720/1459 [6:53:21<6:41:45, 32.62s/it]                                                       49%|████▉     | 720/1459 [6:53:21<6:41:45, 32.62s/it] 49%|████▉     | 721/1459 [6:53:54<6:40:02, 32.52s/it] 49%|████▉     | 722/1459 [6:54:25<6:35:21, 32.19s/it] 50%|████▉     | 723/1459 [6:54:59<6:41:01, 32.69s/it] 50%|████▉     | 724/1459 [6:55:32<6:41:23, 32.77s/it] 50%|████▉     | 725/1459 [6:56:05<6:41:22, 32.81s/it] 50%|████▉     | 726/1459 [6:56:37<6:39:32, 32.70s/it] 50%|████▉     | 727/1459 [6:57:15<6:57:38, 34.23s/it] 50%|████▉     | 728/1459 [6:57:48<6:52:37, 33.87s/it] 50%|████▉     | 729/1459 [6:58:22<6:52:36, 33.91s/it] 50%|█████     | 730/1459 [6:58:52<6:37:30, 32.72s/it]                                                       50%|█████     | 730/1459 [6:58:52<6:37:30, 32.72s/it] 50%|█████     | 731/1459 [6:59:26<6:42:57, 33.21s/it] 50%|█████     | 732/1459 [7:00:04<6:58:12, 34.51s/it] 50%|█████     | 733/1459 [7:00:37<6:53:02, 34.14s/it] 50%|█████     | 734/1459 [7:01:17<7:13:46, 35.90s/it] 50%|█████     | 735/1459 [7:01:49<7:00:08, 34.82s/it] 50%|█████     | 736/1459 [7:02:23<6:53:36, 34.32s/it] 51%|█████     | 737/1459 [7:02:58<6:55:38, 34.54s/it] 51%|█████     | 738/1459 [7:03:28<6:40:02, 33.29s/it] 51%|█████     | 739/1459 [7:04:00<6:33:07, 32.76s/it] 51%|█████     | 740/1459 [7:04:41<7:04:40, 35.44s/it]                                                       51%|█████     | 740/1459 [7:04:41<7:04:40, 35.44s/it] 51%|█████     | 741/1459 [7:05:14<6:52:43, 34.49s/it] 51%|█████     | 742/1459 [7:05:43<6:34:11, 32.99s/it] 51%|█████     | 743/1459 [7:06:17<6:37:53, 33.34s/it] 51%|█████     | 744/1459 [7:06:51<6:38:00, 33.40s/it] 51%|█████     | 745/1459 [7:07:26<6:43:56, 33.94s/it] 51%|█████     | 746/1459 [7:08:00<6:43:27, 33.95s/it] 51%|█████     | 747/1459 [7:08:36<6:52:00, 34.72s/it] 51%|█████▏    | 748/1459 [7:09:11<6:49:44, 34.58s/it] 51%|█████▏    | 749/1459 [7:09:43<6:41:43, 33.95s/it] 51%|█████▏    | 750/1459 [7:10:15<6:34:00, 33.34s/it]                                                       51%|█████▏    | 750/1459 [7:10:15<6:34:00, 33.34s/it] 51%|█████▏    | 751/1459 [7:10:48<6:32:55, 33.30s/it] 52%|█████▏    | 752/1459 [7:11:22<6:35:11, 33.54s/it] 52%|█████▏    | 753/1459 [7:11:57<6:37:02, 33.74s/it] 52%|█████▏    | 754/1459 [7:12:26<6:22:20, 32.54s/it] 52%|█████▏    | 755/1459 [7:12:58<6:20:07, 32.40s/it] 52%|█████▏    | 756/1459 [7:13:36<6:39:03, 34.06s/it] 52%|█████▏    | 757/1459 [7:14:10<6:36:28, 33.89s/it] 52%|█████▏    | 758/1459 [7:14:43<6:32:33, 33.60s/it] 52%|█████▏    | 759/1459 [7:15:19<6:40:01, 34.29s/it] 52%|█████▏    | 760/1459 [7:15:50<6:28:17, 33.33s/it]                                                       52%|█████▏    | 760/1459 [7:15:50<6:28:17, 33.33s/it] 52%|█████▏    | 761/1459 [7:16:26<6:36:32, 34.09s/it] 52%|█████▏    | 762/1459 [7:17:06<6:59:43, 36.13s/it] 52%|█████▏    | 763/1459 [7:17:38<6:43:41, 34.80s/it] 52%|█████▏    | 764/1459 [7:18:21<7:10:31, 37.17s/it] 52%|█████▏    | 765/1459 [7:19:01<7:19:32, 38.00s/it] 53%|█████▎    | 766/1459 [7:19:33<6:57:13, 36.12s/it] 53%|█████▎    | 767/1459 [7:20:05<6:43:58, 35.03s/it] 53%|█████▎    | 768/1459 [7:20:38<6:36:26, 34.42s/it] 53%|█████▎    | 769/1459 [7:21:12<6:33:17, 34.20s/it] 53%|█████▎    | 770/1459 [7:21:46<6:31:43, 34.11s/it]                                                       53%|█████▎    | 770/1459 [7:21:46<6:31:43, 34.11s/it] 53%|█████▎    | 771/1459 [7:22:19<6:26:59, 33.75s/it] 53%|█████▎    | 772/1459 [7:22:54<6:31:16, 34.17s/it] 53%|█████▎    | 773/1459 [7:23:29<6:35:48, 34.62s/it] 53%|█████▎    | 774/1459 [7:24:04<6:34:47, 34.58s/it] 53%|█████▎    | 775/1459 [7:24:45<6:55:59, 36.49s/it] 53%|█████▎    | 776/1459 [7:25:20<6:50:49, 36.09s/it] 53%|█████▎    | 777/1459 [7:25:57<6:52:13, 36.27s/it] 53%|█████▎    | 778/1459 [7:26:31<6:44:22, 35.63s/it] 53%|█████▎    | 779/1459 [7:27:03<6:32:06, 34.60s/it] 53%|█████▎    | 780/1459 [7:27:39<6:36:05, 35.00s/it]                                                       53%|█████▎    | 780/1459 [7:27:39<6:36:05, 35.00s/it] 54%|█████▎    | 781/1459 [7:28:12<6:27:40, 34.31s/it] 54%|█████▎    | 782/1459 [7:28:46<6:27:01, 34.30s/it] 54%|█████▎    | 783/1459 [7:29:22<6:33:46, 34.95s/it] 54%|█████▎    | 784/1459 [7:29:59<6:39:29, 35.51s/it] 54%|█████▍    | 785/1459 [7:30:31<6:26:47, 34.43s/it] 54%|█████▍    | 786/1459 [7:31:13<6:50:38, 36.61s/it] 54%|█████▍    | 787/1459 [7:31:48<6:45:40, 36.22s/it] 54%|█████▍    | 788/1459 [7:32:20<6:31:34, 35.01s/it] 54%|█████▍    | 789/1459 [7:32:53<6:24:05, 34.40s/it] 54%|█████▍    | 790/1459 [7:33:30<6:30:15, 35.00s/it]                                                       54%|█████▍    | 790/1459 [7:33:30<6:30:15, 35.00s/it] 54%|█████▍    | 791/1459 [7:34:03<6:23:23, 34.44s/it] 54%|█████▍    | 792/1459 [7:34:38<6:24:27, 34.58s/it] 54%|█████▍    | 793/1459 [7:35:12<6:21:42, 34.39s/it] 54%|█████▍    | 794/1459 [7:35:45<6:16:10, 33.94s/it] 54%|█████▍    | 795/1459 [7:36:17<6:09:33, 33.39s/it] 55%|█████▍    | 796/1459 [7:36:53<6:17:16, 34.14s/it] 55%|█████▍    | 797/1459 [7:37:27<6:18:08, 34.27s/it] 55%|█████▍    | 798/1459 [7:38:04<6:27:20, 35.16s/it] 55%|█████▍    | 799/1459 [7:38:37<6:18:15, 34.39s/it] 55%|█████▍    | 800/1459 [7:39:15<6:29:11, 35.43s/it]                                                       55%|█████▍    | 800/1459 [7:39:15<6:29:11, 35.43s/it] 55%|█████▍    | 801/1459 [7:39:46<6:16:12, 34.30s/it] 55%|█████▍    | 802/1459 [7:40:17<6:04:12, 33.26s/it] 55%|█████▌    | 803/1459 [7:40:49<5:59:29, 32.88s/it] 55%|█████▌    | 804/1459 [7:41:24<6:04:37, 33.40s/it] 55%|█████▌    | 805/1459 [7:41:54<5:54:35, 32.53s/it] 55%|█████▌    | 806/1459 [7:42:30<6:03:21, 33.39s/it] 55%|█████▌    | 807/1459 [7:43:01<5:55:43, 32.74s/it] 55%|█████▌    | 808/1459 [7:43:38<6:09:34, 34.06s/it] 55%|█████▌    | 809/1459 [7:44:11<6:03:56, 33.59s/it] 56%|█████▌    | 810/1459 [7:44:53<6:30:34, 36.11s/it]                                                       56%|█████▌    | 810/1459 [7:44:53<6:30:34, 36.11s/it] 56%|█████▌    | 811/1459 [7:45:24<6:14:21, 34.66s/it] 56%|█████▌    | 812/1459 [7:46:05<6:33:43, 36.51s/it] 56%|█████▌    | 813/1459 [7:46:39<6:25:11, 35.78s/it] 56%|█████▌    | 814/1459 [7:47:12<6:14:52, 34.87s/it] 56%|█████▌    | 815/1459 [7:47:45<6:08:47, 34.36s/it] 56%|█████▌    | 816/1459 [7:48:18<6:05:12, 34.08s/it] 56%|█████▌    | 817/1459 [7:48:51<6:00:17, 33.67s/it] 56%|█████▌    | 818/1459 [7:49:27<6:07:13, 34.37s/it] 56%|█████▌    | 819/1459 [7:50:01<6:06:22, 34.35s/it] 56%|█████▌    | 820/1459 [7:50:35<6:03:09, 34.10s/it]                                                       56%|█████▌    | 820/1459 [7:50:35<6:03:09, 34.10s/it] 56%|█████▋    | 821/1459 [7:51:10<6:05:23, 34.36s/it] 56%|█████▋    | 822/1459 [7:51:42<5:57:05, 33.63s/it] 56%|█████▋    | 823/1459 [7:52:12<5:46:41, 32.71s/it] 56%|█████▋    | 824/1459 [7:52:43<5:40:52, 32.21s/it] 57%|█████▋    | 825/1459 [7:53:19<5:51:11, 33.24s/it] 57%|█████▋    | 826/1459 [7:53:54<5:57:38, 33.90s/it] 57%|█████▋    | 827/1459 [7:54:31<6:04:25, 34.60s/it] 57%|█████▋    | 828/1459 [7:55:06<6:06:55, 34.89s/it] 57%|█████▋    | 829/1459 [7:55:40<6:04:50, 34.75s/it] 57%|█████▋    | 830/1459 [7:56:21<6:22:42, 36.51s/it]                                                       57%|█████▋    | 830/1459 [7:56:21<6:22:42, 36.51s/it] 57%|█████▋    | 831/1459 [7:56:57<6:20:34, 36.36s/it] 57%|█████▋    | 832/1459 [7:57:31<6:13:02, 35.70s/it] 57%|█████▋    | 833/1459 [7:58:06<6:09:26, 35.41s/it] 57%|█████▋    | 834/1459 [7:58:41<6:06:38, 35.20s/it] 57%|█████▋    | 835/1459 [7:59:11<5:51:30, 33.80s/it] 57%|█████▋    | 836/1459 [7:59:42<5:40:51, 32.83s/it] 57%|█████▋    | 837/1459 [8:00:14<5:39:41, 32.77s/it] 57%|█████▋    | 838/1459 [8:00:48<5:40:53, 32.94s/it] 58%|█████▊    | 839/1459 [8:01:20<5:37:37, 32.67s/it] 58%|█████▊    | 840/1459 [8:01:53<5:40:05, 32.97s/it]                                                       58%|█████▊    | 840/1459 [8:01:53<5:40:05, 32.97s/it] 58%|█████▊    | 841/1459 [8:02:24<5:33:29, 32.38s/it] 58%|█████▊    | 842/1459 [8:02:58<5:36:37, 32.73s/it] 58%|█████▊    | 843/1459 [8:03:31<5:36:13, 32.75s/it] 58%|█████▊    | 844/1459 [8:04:05<5:39:17, 33.10s/it] 58%|█████▊    | 845/1459 [8:04:37<5:36:22, 32.87s/it] 58%|█████▊    | 846/1459 [8:05:10<5:35:18, 32.82s/it] 58%|█████▊    | 847/1459 [8:05:43<5:34:52, 32.83s/it] 58%|█████▊    | 848/1459 [8:06:19<5:45:14, 33.90s/it] 58%|█████▊    | 849/1459 [8:06:53<5:44:37, 33.90s/it] 58%|█████▊    | 850/1459 [8:07:28<5:48:51, 34.37s/it]                                                       58%|█████▊    | 850/1459 [8:07:28<5:48:51, 34.37s/it] 58%|█████▊    | 851/1459 [8:08:04<5:51:47, 34.72s/it] 58%|█████▊    | 852/1459 [8:08:38<5:48:32, 34.45s/it] 58%|█████▊    | 853/1459 [8:09:11<5:44:30, 34.11s/it] 59%|█████▊    | 854/1459 [8:09:47<5:50:31, 34.76s/it] 59%|█████▊    | 855/1459 [8:10:22<5:50:51, 34.85s/it] 59%|█████▊    | 856/1459 [8:10:59<5:56:07, 35.44s/it] 59%|█████▊    | 857/1459 [8:11:34<5:54:03, 35.29s/it] 59%|█████▉    | 858/1459 [8:12:06<5:44:06, 34.35s/it] 59%|█████▉    | 859/1459 [8:12:42<5:48:56, 34.89s/it] 59%|█████▉    | 860/1459 [8:13:12<5:32:46, 33.33s/it]                                                       59%|█████▉    | 860/1459 [8:13:12<5:32:46, 33.33s/it] 59%|█████▉    | 861/1459 [8:13:45<5:31:09, 33.23s/it] 59%|█████▉    | 862/1459 [8:14:16<5:23:23, 32.50s/it] 59%|█████▉    | 863/1459 [8:14:48<5:22:30, 32.47s/it] 59%|█████▉    | 864/1459 [8:15:24<5:32:39, 33.55s/it] 59%|█████▉    | 865/1459 [8:16:00<5:39:25, 34.29s/it] 59%|█████▉    | 866/1459 [8:16:34<5:37:47, 34.18s/it] 59%|█████▉    | 867/1459 [8:17:12<5:48:55, 35.36s/it] 59%|█████▉    | 868/1459 [8:17:49<5:50:53, 35.62s/it] 60%|█████▉    | 869/1459 [8:18:19<5:33:35, 33.93s/it] 60%|█████▉    | 870/1459 [8:19:01<5:56:30, 36.32s/it]                                                       60%|█████▉    | 870/1459 [8:19:01<5:56:30, 36.32s/it] 60%|█████▉    | 871/1459 [8:19:32<5:40:02, 34.70s/it] 60%|█████▉    | 872/1459 [8:20:05<5:35:22, 34.28s/it] 60%|█████▉    | 873/1459 [8:20:34<5:20:03, 32.77s/it] 60%|█████▉    | 874/1459 [8:21:12<5:35:02, 34.36s/it] 60%|█████▉    | 875/1459 [8:21:46<5:34:06, 34.33s/it] 60%|██████    | 876/1459 [8:22:17<5:21:22, 33.07s/it] 60%|██████    | 877/1459 [8:22:48<5:16:53, 32.67s/it] 60%|██████    | 878/1459 [8:23:23<5:21:26, 33.20s/it] 60%|██████    | 879/1459 [8:23:58<5:26:10, 33.74s/it] 60%|██████    | 880/1459 [8:24:29<5:19:43, 33.13s/it]                                                       60%|██████    | 880/1459 [8:24:29<5:19:43, 33.13s/it] 60%|██████    | 881/1459 [8:25:06<5:29:43, 34.23s/it] 60%|██████    | 882/1459 [8:25:45<5:42:24, 35.61s/it] 61%|██████    | 883/1459 [8:26:15<5:26:48, 34.04s/it] 61%|██████    | 884/1459 [8:26:52<5:33:38, 34.82s/it] 61%|██████    | 885/1459 [8:27:24<5:24:55, 33.96s/it] 61%|██████    | 886/1459 [8:27:53<5:09:42, 32.43s/it] 61%|██████    | 887/1459 [8:28:25<5:09:36, 32.48s/it] 61%|██████    | 888/1459 [8:28:59<5:10:46, 32.66s/it] 61%|██████    | 889/1459 [8:29:32<5:12:53, 32.94s/it] 61%|██████    | 890/1459 [8:30:08<5:20:36, 33.81s/it]                                                       61%|██████    | 890/1459 [8:30:08<5:20:36, 33.81s/it] 61%|██████    | 891/1459 [8:30:44<5:27:16, 34.57s/it] 61%|██████    | 892/1459 [8:31:18<5:23:15, 34.21s/it] 61%|██████    | 893/1459 [8:31:53<5:25:51, 34.54s/it] 61%|██████▏   | 894/1459 [8:32:25<5:18:52, 33.86s/it] 61%|██████▏   | 895/1459 [8:32:57<5:11:33, 33.14s/it] 61%|██████▏   | 896/1459 [8:33:27<5:04:10, 32.42s/it] 61%|██████▏   | 897/1459 [8:34:07<5:22:35, 34.44s/it] 62%|██████▏   | 898/1459 [8:34:35<5:05:08, 32.64s/it] 62%|██████▏   | 899/1459 [8:35:16<5:27:10, 35.05s/it] 62%|██████▏   | 900/1459 [8:35:47<5:14:41, 33.78s/it]                                                       62%|██████▏   | 900/1459 [8:35:47<5:14:41, 33.78s/it] 62%|██████▏   | 901/1459 [8:36:19<5:11:23, 33.48s/it] 62%|██████▏   | 902/1459 [8:36:52<5:07:40, 33.14s/it] 62%|██████▏   | 903/1459 [8:37:23<5:01:51, 32.57s/it] 62%|██████▏   | 904/1459 [8:37:57<5:06:07, 33.09s/it] 62%|██████▏   | 905/1459 [8:38:31<5:08:34, 33.42s/it] 62%|██████▏   | 906/1459 [8:39:03<5:03:17, 32.91s/it] 62%|██████▏   | 907/1459 [8:39:38<5:08:00, 33.48s/it] 62%|██████▏   | 908/1459 [8:40:13<5:11:08, 33.88s/it] 62%|██████▏   | 909/1459 [8:40:47<5:12:48, 34.13s/it] 62%|██████▏   | 910/1459 [8:41:20<5:08:20, 33.70s/it]                                                       62%|██████▏   | 910/1459 [8:41:20<5:08:20, 33.70s/it] 62%|██████▏   | 911/1459 [8:41:55<5:12:16, 34.19s/it] 63%|██████▎   | 912/1459 [8:42:29<5:10:11, 34.03s/it] 63%|██████▎   | 913/1459 [8:43:06<5:18:35, 35.01s/it] 63%|██████▎   | 914/1459 [8:43:42<5:18:46, 35.10s/it] 63%|██████▎   | 915/1459 [8:44:14<5:10:53, 34.29s/it] 63%|██████▎   | 916/1459 [8:44:48<5:10:07, 34.27s/it] 63%|██████▎   | 917/1459 [8:45:20<5:01:16, 33.35s/it] 63%|██████▎   | 918/1459 [8:45:54<5:03:40, 33.68s/it] 63%|██████▎   | 919/1459 [8:46:26<4:58:59, 33.22s/it] 63%|██████▎   | 920/1459 [8:47:01<5:02:22, 33.66s/it]                                                       63%|██████▎   | 920/1459 [8:47:01<5:02:22, 33.66s/it] 63%|██████▎   | 921/1459 [8:47:36<5:06:55, 34.23s/it] 63%|██████▎   | 922/1459 [8:48:08<4:58:46, 33.38s/it] 63%|██████▎   | 923/1459 [8:48:41<4:56:35, 33.20s/it] 63%|██████▎   | 924/1459 [8:49:13<4:53:43, 32.94s/it] 63%|██████▎   | 925/1459 [8:49:47<4:56:07, 33.27s/it] 63%|██████▎   | 926/1459 [8:50:19<4:52:05, 32.88s/it] 64%|██████▎   | 927/1459 [8:50:49<4:43:48, 32.01s/it] 64%|██████▎   | 928/1459 [8:51:21<4:42:39, 31.94s/it] 64%|██████▎   | 929/1459 [8:51:56<4:51:44, 33.03s/it] 64%|██████▎   | 930/1459 [8:52:28<4:48:10, 32.69s/it]                                                       64%|██████▎   | 930/1459 [8:52:28<4:48:10, 32.69s/it] 64%|██████▍   | 931/1459 [8:53:02<4:49:51, 32.94s/it] 64%|██████▍   | 932/1459 [8:53:32<4:42:53, 32.21s/it] 64%|██████▍   | 933/1459 [8:54:04<4:40:32, 32.00s/it] 64%|██████▍   | 934/1459 [8:54:35<4:37:52, 31.76s/it] 64%|██████▍   | 935/1459 [8:55:10<4:46:28, 32.80s/it] 64%|██████▍   | 936/1459 [8:55:45<4:52:34, 33.56s/it] 64%|██████▍   | 937/1459 [8:56:20<4:55:44, 33.99s/it] 64%|██████▍   | 938/1459 [8:56:55<4:55:54, 34.08s/it] 64%|██████▍   | 939/1459 [8:57:28<4:52:33, 33.76s/it] 64%|██████▍   | 940/1459 [8:57:59<4:46:32, 33.13s/it]                                                       64%|██████▍   | 940/1459 [8:57:59<4:46:32, 33.13s/it] 64%|██████▍   | 941/1459 [8:58:33<4:47:40, 33.32s/it] 65%|██████▍   | 942/1459 [8:59:07<4:48:41, 33.50s/it] 65%|██████▍   | 943/1459 [8:59:39<4:43:07, 32.92s/it] 65%|██████▍   | 944/1459 [9:00:13<4:45:27, 33.26s/it] 65%|██████▍   | 945/1459 [9:00:45<4:42:24, 32.97s/it] 65%|██████▍   | 946/1459 [9:01:22<4:51:00, 34.04s/it] 65%|██████▍   | 947/1459 [9:01:58<4:55:56, 34.68s/it] 65%|██████▍   | 948/1459 [9:02:32<4:53:19, 34.44s/it] 65%|██████▌   | 949/1459 [9:03:10<5:02:37, 35.60s/it] 65%|██████▌   | 950/1459 [9:03:44<4:57:58, 35.13s/it]                                                       65%|██████▌   | 950/1459 [9:03:44<4:57:58, 35.13s/it] 65%|██████▌   | 951/1459 [9:04:20<5:00:54, 35.54s/it] 65%|██████▌   | 952/1459 [9:04:57<5:02:18, 35.78s/it] 65%|██████▌   | 953/1459 [9:05:29<4:52:31, 34.69s/it] 65%|██████▌   | 954/1459 [9:06:00<4:41:51, 33.49s/it] 65%|██████▌   | 955/1459 [9:06:31<4:35:17, 32.77s/it] 66%|██████▌   | 956/1459 [9:07:03<4:33:34, 32.63s/it] 66%|██████▌   | 957/1459 [9:07:46<4:58:11, 35.64s/it] 66%|██████▌   | 958/1459 [9:08:18<4:49:31, 34.67s/it] 66%|██████▌   | 959/1459 [9:08:51<4:45:03, 34.21s/it] 66%|██████▌   | 960/1459 [9:09:24<4:39:59, 33.67s/it]                                                       66%|██████▌   | 960/1459 [9:09:24<4:39:59, 33.67s/it] 66%|██████▌   | 961/1459 [9:10:04<4:55:22, 35.59s/it] 66%|██████▌   | 962/1459 [9:10:48<5:16:56, 38.26s/it] 66%|██████▌   | 963/1459 [9:11:19<4:57:29, 35.99s/it] 66%|██████▌   | 964/1459 [9:11:53<4:52:59, 35.51s/it] 66%|██████▌   | 965/1459 [9:12:25<4:43:25, 34.42s/it] 66%|██████▌   | 966/1459 [9:12:55<4:32:36, 33.18s/it] 66%|██████▋   | 967/1459 [9:13:39<4:58:15, 36.37s/it] 66%|██████▋   | 968/1459 [9:14:18<5:04:06, 37.16s/it] 66%|██████▋   | 969/1459 [9:14:54<4:59:52, 36.72s/it] 66%|██████▋   | 970/1459 [9:15:28<4:51:55, 35.82s/it]                                                       66%|██████▋   | 970/1459 [9:15:28<4:51:55, 35.82s/it] 67%|██████▋   | 971/1459 [9:16:02<4:48:12, 35.44s/it] 67%|██████▋   | 972/1459 [9:16:37<4:45:45, 35.21s/it] 67%|██████▋   | 973/1459 [9:17:11<4:41:28, 34.75s/it] 67%|██████▋   | 974/1459 [9:17:45<4:40:19, 34.68s/it] 67%|██████▋   | 975/1459 [9:18:20<4:41:21, 34.88s/it] 67%|██████▋   | 976/1459 [9:18:59<4:49:59, 36.02s/it] 67%|██████▋   | 977/1459 [9:19:37<4:53:39, 36.55s/it] 67%|██████▋   | 978/1459 [9:20:11<4:45:58, 35.67s/it] 67%|██████▋   | 979/1459 [9:20:45<4:42:23, 35.30s/it] 67%|██████▋   | 980/1459 [9:21:20<4:40:55, 35.19s/it]                                                       67%|██████▋   | 980/1459 [9:21:20<4:40:55, 35.19s/it] 67%|██████▋   | 981/1459 [9:21:54<4:38:07, 34.91s/it] 67%|██████▋   | 982/1459 [9:22:29<4:38:04, 34.98s/it] 67%|██████▋   | 983/1459 [9:23:01<4:30:35, 34.11s/it] 67%|██████▋   | 984/1459 [9:23:33<4:25:09, 33.49s/it] 68%|██████▊   | 985/1459 [9:24:07<4:25:15, 33.58s/it] 68%|██████▊   | 986/1459 [9:24:46<4:36:22, 35.06s/it] 68%|██████▊   | 987/1459 [9:25:17<4:26:15, 33.85s/it] 68%|██████▊   | 988/1459 [9:25:51<4:26:56, 34.00s/it] 68%|██████▊   | 989/1459 [9:26:26<4:28:20, 34.26s/it] 68%|██████▊   | 990/1459 [9:26:57<4:19:59, 33.26s/it]                                                       68%|██████▊   | 990/1459 [9:26:57<4:19:59, 33.26s/it] 68%|██████▊   | 991/1459 [9:27:34<4:27:32, 34.30s/it] 68%|██████▊   | 992/1459 [9:28:05<4:19:21, 33.32s/it] 68%|██████▊   | 993/1459 [9:28:37<4:16:59, 33.09s/it] 68%|██████▊   | 994/1459 [9:29:08<4:11:24, 32.44s/it] 68%|██████▊   | 995/1459 [9:29:45<4:21:18, 33.79s/it] 68%|██████▊   | 996/1459 [9:30:16<4:13:25, 32.84s/it] 68%|██████▊   | 997/1459 [9:30:46<4:06:19, 31.99s/it] 68%|██████▊   | 998/1459 [9:31:18<4:05:29, 31.95s/it] 68%|██████▊   | 999/1459 [9:31:49<4:04:31, 31.89s/it] 69%|██████▊   | 1000/1459 [9:32:20<4:02:12, 31.66s/it]                                                        69%|██████▊   | 1000/1459 [9:32:20<4:02:12, 31.66s/it][INFO|trainer.py:4289] 2025-10-04 08:23:15,386 >> Saving model checkpoint to /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1000
[INFO|configuration_utils.py:491] 2025-10-04 08:23:15,397 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1000/config.json
[INFO|configuration_utils.py:826] 2025-10-04 08:23:15,397 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1000/generation_config.json
[INFO|modeling_utils.py:4308] 2025-10-04 08:23:34,290 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1000/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2394] 2025-10-04 08:23:34,291 >> chat template saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1000/chat_template.jinja
[INFO|tokenization_utils_base.py:2563] 2025-10-04 08:23:34,292 >> tokenizer config file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2572] 2025-10-04 08:23:34,292 >> Special tokens file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1000/special_tokens_map.json
 69%|██████▊   | 1001/1459 [9:33:27<5:21:04, 42.06s/it] 69%|██████▊   | 1002/1459 [9:34:04<5:10:05, 40.71s/it] 69%|██████▊   | 1003/1459 [9:34:37<4:51:05, 38.30s/it] 69%|██████▉   | 1004/1459 [9:35:13<4:45:57, 37.71s/it] 69%|██████▉   | 1005/1459 [9:35:49<4:40:05, 37.02s/it] 69%|██████▉   | 1006/1459 [9:36:21<4:28:28, 35.56s/it] 69%|██████▉   | 1007/1459 [9:36:50<4:14:21, 33.77s/it] 69%|██████▉   | 1008/1459 [9:37:22<4:09:42, 33.22s/it] 69%|██████▉   | 1009/1459 [9:37:53<4:03:21, 32.45s/it] 69%|██████▉   | 1010/1459 [9:38:22<3:55:40, 31.49s/it]                                                        69%|██████▉   | 1010/1459 [9:38:22<3:55:40, 31.49s/it] 69%|██████▉   | 1011/1459 [9:38:56<4:00:10, 32.17s/it] 69%|██████▉   | 1012/1459 [9:39:32<4:08:08, 33.31s/it] 69%|██████▉   | 1013/1459 [9:40:07<4:10:31, 33.70s/it] 69%|██████▉   | 1014/1459 [9:40:41<4:10:40, 33.80s/it] 70%|██████▉   | 1015/1459 [9:41:19<4:20:23, 35.19s/it] 70%|██████▉   | 1016/1459 [9:42:03<4:39:37, 37.87s/it] 70%|██████▉   | 1017/1459 [9:42:34<4:23:39, 35.79s/it] 70%|██████▉   | 1018/1459 [9:43:08<4:19:48, 35.35s/it] 70%|██████▉   | 1019/1459 [9:43:42<4:15:56, 34.90s/it] 70%|██████▉   | 1020/1459 [9:44:16<4:13:26, 34.64s/it]                                                        70%|██████▉   | 1020/1459 [9:44:16<4:13:26, 34.64s/it] 70%|██████▉   | 1021/1459 [9:44:51<4:13:41, 34.75s/it] 70%|███████   | 1022/1459 [9:45:26<4:13:47, 34.85s/it] 70%|███████   | 1023/1459 [9:45:58<4:04:58, 33.71s/it] 70%|███████   | 1024/1459 [9:46:31<4:03:45, 33.62s/it] 70%|███████   | 1025/1459 [9:47:07<4:08:22, 34.34s/it] 70%|███████   | 1026/1459 [9:47:39<4:03:25, 33.73s/it] 70%|███████   | 1027/1459 [9:48:15<4:07:02, 34.31s/it] 70%|███████   | 1028/1459 [9:48:49<4:06:11, 34.27s/it] 71%|███████   | 1029/1459 [9:49:23<4:04:53, 34.17s/it] 71%|███████   | 1030/1459 [9:49:55<3:59:22, 33.48s/it]                                                        71%|███████   | 1030/1459 [9:49:55<3:59:22, 33.48s/it] 71%|███████   | 1031/1459 [9:50:24<3:49:46, 32.21s/it] 71%|███████   | 1032/1459 [9:50:53<3:42:53, 31.32s/it] 71%|███████   | 1033/1459 [9:51:23<3:38:17, 30.74s/it] 71%|███████   | 1034/1459 [9:52:01<3:53:35, 32.98s/it] 71%|███████   | 1035/1459 [9:52:34<3:52:09, 32.85s/it] 71%|███████   | 1036/1459 [9:53:04<3:47:09, 32.22s/it] 71%|███████   | 1037/1459 [9:53:36<3:44:33, 31.93s/it] 71%|███████   | 1038/1459 [9:54:10<3:49:35, 32.72s/it] 71%|███████   | 1039/1459 [9:54:42<3:46:43, 32.39s/it] 71%|███████▏  | 1040/1459 [9:55:15<3:47:18, 32.55s/it]                                                        71%|███████▏  | 1040/1459 [9:55:15<3:47:18, 32.55s/it] 71%|███████▏  | 1041/1459 [9:55:46<3:44:26, 32.22s/it] 71%|███████▏  | 1042/1459 [9:56:20<3:47:17, 32.70s/it] 71%|███████▏  | 1043/1459 [9:56:55<3:51:10, 33.34s/it] 72%|███████▏  | 1044/1459 [9:57:28<3:49:26, 33.17s/it] 72%|███████▏  | 1045/1459 [9:57:59<3:45:34, 32.69s/it] 72%|███████▏  | 1046/1459 [9:58:29<3:38:43, 31.78s/it] 72%|███████▏  | 1047/1459 [9:59:02<3:41:13, 32.22s/it] 72%|███████▏  | 1048/1459 [9:59:39<3:51:02, 33.73s/it] 72%|███████▏  | 1049/1459 [10:00:10<3:44:39, 32.88s/it] 72%|███████▏  | 1050/1459 [10:00:42<3:41:43, 32.53s/it]                                                         72%|███████▏  | 1050/1459 [10:00:42<3:41:43, 32.53s/it] 72%|███████▏  | 1051/1459 [10:01:18<3:48:59, 33.68s/it] 72%|███████▏  | 1052/1459 [10:01:54<3:52:57, 34.34s/it] 72%|███████▏  | 1053/1459 [10:02:30<3:55:24, 34.79s/it] 72%|███████▏  | 1054/1459 [10:03:03<3:51:30, 34.30s/it] 72%|███████▏  | 1055/1459 [10:03:40<3:56:18, 35.09s/it] 72%|███████▏  | 1056/1459 [10:04:12<3:48:49, 34.07s/it] 72%|███████▏  | 1057/1459 [10:04:45<3:47:29, 33.95s/it] 73%|███████▎  | 1058/1459 [10:05:21<3:49:37, 34.36s/it] 73%|███████▎  | 1059/1459 [10:05:53<3:44:33, 33.68s/it] 73%|███████▎  | 1060/1459 [10:06:28<3:46:31, 34.06s/it]                                                         73%|███████▎  | 1060/1459 [10:06:28<3:46:31, 34.06s/it] 73%|███████▎  | 1061/1459 [10:07:02<3:46:10, 34.10s/it] 73%|███████▎  | 1062/1459 [10:07:43<3:59:34, 36.21s/it] 73%|███████▎  | 1063/1459 [10:08:14<3:48:20, 34.60s/it] 73%|███████▎  | 1064/1459 [10:08:45<3:41:38, 33.67s/it] 73%|███████▎  | 1065/1459 [10:09:18<3:39:54, 33.49s/it] 73%|███████▎  | 1066/1459 [10:09:56<3:46:47, 34.63s/it] 73%|███████▎  | 1067/1459 [10:10:29<3:43:19, 34.18s/it] 73%|███████▎  | 1068/1459 [10:10:59<3:34:15, 32.88s/it] 73%|███████▎  | 1069/1459 [10:11:34<3:37:32, 33.47s/it] 73%|███████▎  | 1070/1459 [10:12:11<3:44:58, 34.70s/it]                                                         73%|███████▎  | 1070/1459 [10:12:11<3:44:58, 34.70s/it] 73%|███████▎  | 1071/1459 [10:12:51<3:54:23, 36.25s/it] 73%|███████▎  | 1072/1459 [10:13:30<3:59:27, 37.12s/it] 74%|███████▎  | 1073/1459 [10:14:06<3:55:34, 36.62s/it] 74%|███████▎  | 1074/1459 [10:14:38<3:47:16, 35.42s/it] 74%|███████▎  | 1075/1459 [10:15:15<3:49:27, 35.85s/it] 74%|███████▎  | 1076/1459 [10:15:48<3:42:57, 34.93s/it] 74%|███████▍  | 1077/1459 [10:16:19<3:34:58, 33.77s/it] 74%|███████▍  | 1078/1459 [10:16:49<3:27:43, 32.71s/it] 74%|███████▍  | 1079/1459 [10:17:20<3:23:58, 32.21s/it] 74%|███████▍  | 1080/1459 [10:17:51<3:21:35, 31.91s/it]                                                         74%|███████▍  | 1080/1459 [10:17:51<3:21:35, 31.91s/it] 74%|███████▍  | 1081/1459 [10:18:23<3:21:06, 31.92s/it] 74%|███████▍  | 1082/1459 [10:18:58<3:26:31, 32.87s/it] 74%|███████▍  | 1083/1459 [10:19:38<3:38:12, 34.82s/it] 74%|███████▍  | 1084/1459 [10:20:12<3:37:05, 34.73s/it] 74%|███████▍  | 1085/1459 [10:20:45<3:32:40, 34.12s/it] 74%|███████▍  | 1086/1459 [10:21:14<3:23:06, 32.67s/it] 75%|███████▍  | 1087/1459 [10:21:45<3:18:24, 32.00s/it] 75%|███████▍  | 1088/1459 [10:22:18<3:19:37, 32.28s/it] 75%|███████▍  | 1089/1459 [10:22:51<3:21:04, 32.61s/it] 75%|███████▍  | 1090/1459 [10:23:28<3:28:04, 33.83s/it]                                                         75%|███████▍  | 1090/1459 [10:23:28<3:28:04, 33.83s/it] 75%|███████▍  | 1091/1459 [10:24:00<3:24:06, 33.28s/it] 75%|███████▍  | 1092/1459 [10:24:33<3:23:41, 33.30s/it] 75%|███████▍  | 1093/1459 [10:25:12<3:33:00, 34.92s/it] 75%|███████▍  | 1094/1459 [10:25:48<3:33:49, 35.15s/it] 75%|███████▌  | 1095/1459 [10:26:27<3:40:49, 36.40s/it] 75%|███████▌  | 1096/1459 [10:27:03<3:39:07, 36.22s/it] 75%|███████▌  | 1097/1459 [10:27:36<3:33:31, 35.39s/it] 75%|███████▌  | 1098/1459 [10:28:12<3:33:38, 35.51s/it] 75%|███████▌  | 1099/1459 [10:28:49<3:35:09, 35.86s/it] 75%|███████▌  | 1100/1459 [10:29:26<3:37:19, 36.32s/it]                                                         75%|███████▌  | 1100/1459 [10:29:26<3:37:19, 36.32s/it] 75%|███████▌  | 1101/1459 [10:29:59<3:30:27, 35.27s/it] 76%|███████▌  | 1102/1459 [10:30:30<3:23:27, 34.19s/it] 76%|███████▌  | 1103/1459 [10:31:06<3:25:14, 34.59s/it] 76%|███████▌  | 1104/1459 [10:31:40<3:23:33, 34.40s/it] 76%|███████▌  | 1105/1459 [10:32:18<3:29:00, 35.43s/it] 76%|███████▌  | 1106/1459 [10:32:49<3:21:14, 34.20s/it] 76%|███████▌  | 1107/1459 [10:33:22<3:17:56, 33.74s/it] 76%|███████▌  | 1108/1459 [10:33:56<3:17:50, 33.82s/it] 76%|███████▌  | 1109/1459 [10:34:34<3:24:58, 35.14s/it] 76%|███████▌  | 1110/1459 [10:35:07<3:20:45, 34.52s/it]                                                         76%|███████▌  | 1110/1459 [10:35:07<3:20:45, 34.52s/it] 76%|███████▌  | 1111/1459 [10:35:44<3:24:35, 35.27s/it] 76%|███████▌  | 1112/1459 [10:36:15<3:16:29, 33.97s/it] 76%|███████▋  | 1113/1459 [10:36:50<3:17:54, 34.32s/it] 76%|███████▋  | 1114/1459 [10:37:23<3:14:16, 33.79s/it] 76%|███████▋  | 1115/1459 [10:37:54<3:09:40, 33.08s/it] 76%|███████▋  | 1116/1459 [10:38:28<3:10:11, 33.27s/it] 77%|███████▋  | 1117/1459 [10:39:01<3:09:33, 33.26s/it] 77%|███████▋  | 1118/1459 [10:39:35<3:09:41, 33.38s/it] 77%|███████▋  | 1119/1459 [10:40:06<3:04:58, 32.64s/it] 77%|███████▋  | 1120/1459 [10:40:37<3:02:24, 32.28s/it]                                                         77%|███████▋  | 1120/1459 [10:40:37<3:02:24, 32.28s/it] 77%|███████▋  | 1121/1459 [10:41:08<3:00:19, 32.01s/it] 77%|███████▋  | 1122/1459 [10:41:47<3:11:09, 34.03s/it] 77%|███████▋  | 1123/1459 [10:42:28<3:21:46, 36.03s/it] 77%|███████▋  | 1124/1459 [10:43:07<3:26:03, 36.91s/it] 77%|███████▋  | 1125/1459 [10:43:38<3:16:30, 35.30s/it] 77%|███████▋  | 1126/1459 [10:44:12<3:12:47, 34.74s/it] 77%|███████▋  | 1127/1459 [10:44:46<3:11:10, 34.55s/it] 77%|███████▋  | 1128/1459 [10:45:18<3:05:49, 33.68s/it] 77%|███████▋  | 1129/1459 [10:45:51<3:04:29, 33.55s/it] 77%|███████▋  | 1130/1459 [10:46:27<3:07:40, 34.23s/it]                                                         77%|███████▋  | 1130/1459 [10:46:27<3:07:40, 34.23s/it] 78%|███████▊  | 1131/1459 [10:46:58<3:01:39, 33.23s/it] 78%|███████▊  | 1132/1459 [10:47:29<2:58:29, 32.75s/it] 78%|███████▊  | 1133/1459 [10:48:05<3:02:38, 33.61s/it] 78%|███████▊  | 1134/1459 [10:48:39<3:02:50, 33.76s/it] 78%|███████▊  | 1135/1459 [10:49:12<3:01:28, 33.61s/it] 78%|███████▊  | 1136/1459 [10:49:46<3:01:44, 33.76s/it] 78%|███████▊  | 1137/1459 [10:50:19<2:59:52, 33.52s/it] 78%|███████▊  | 1138/1459 [10:50:56<3:03:44, 34.35s/it] 78%|███████▊  | 1139/1459 [10:51:29<3:02:28, 34.22s/it] 78%|███████▊  | 1140/1459 [10:52:03<3:00:13, 33.90s/it]                                                         78%|███████▊  | 1140/1459 [10:52:03<3:00:13, 33.90s/it] 78%|███████▊  | 1141/1459 [10:52:37<3:00:51, 34.13s/it] 78%|███████▊  | 1142/1459 [10:53:09<2:56:25, 33.39s/it] 78%|███████▊  | 1143/1459 [10:53:41<2:54:30, 33.13s/it] 78%|███████▊  | 1144/1459 [10:54:16<2:56:34, 33.63s/it] 78%|███████▊  | 1145/1459 [10:54:53<3:00:38, 34.52s/it] 79%|███████▊  | 1146/1459 [10:55:27<2:59:58, 34.50s/it] 79%|███████▊  | 1147/1459 [10:55:59<2:55:32, 33.76s/it] 79%|███████▊  | 1148/1459 [10:56:30<2:50:02, 32.80s/it] 79%|███████▉  | 1149/1459 [10:57:08<2:57:32, 34.36s/it] 79%|███████▉  | 1150/1459 [10:57:47<3:03:43, 35.67s/it]                                                         79%|███████▉  | 1150/1459 [10:57:47<3:03:43, 35.67s/it] 79%|███████▉  | 1151/1459 [10:58:20<2:59:11, 34.91s/it] 79%|███████▉  | 1152/1459 [10:58:53<2:55:59, 34.40s/it] 79%|███████▉  | 1153/1459 [10:59:25<2:52:12, 33.77s/it] 79%|███████▉  | 1154/1459 [11:00:00<2:53:27, 34.12s/it] 79%|███████▉  | 1155/1459 [11:00:40<3:01:11, 35.76s/it] 79%|███████▉  | 1156/1459 [11:01:18<3:04:58, 36.63s/it] 79%|███████▉  | 1157/1459 [11:01:56<3:05:11, 36.79s/it] 79%|███████▉  | 1158/1459 [11:02:27<2:56:31, 35.19s/it] 79%|███████▉  | 1159/1459 [11:03:01<2:53:43, 34.75s/it] 80%|███████▉  | 1160/1459 [11:03:35<2:51:48, 34.48s/it]                                                         80%|███████▉  | 1160/1459 [11:03:35<2:51:48, 34.48s/it] 80%|███████▉  | 1161/1459 [11:04:09<2:50:58, 34.42s/it] 80%|███████▉  | 1162/1459 [11:04:43<2:50:10, 34.38s/it] 80%|███████▉  | 1163/1459 [11:05:20<2:53:46, 35.23s/it] 80%|███████▉  | 1164/1459 [11:06:01<3:00:52, 36.79s/it] 80%|███████▉  | 1165/1459 [11:06:32<2:52:41, 35.24s/it] 80%|███████▉  | 1166/1459 [11:07:05<2:48:41, 34.54s/it] 80%|███████▉  | 1167/1459 [11:07:38<2:44:56, 33.89s/it] 80%|████████  | 1168/1459 [11:08:11<2:43:35, 33.73s/it] 80%|████████  | 1169/1459 [11:08:47<2:46:12, 34.39s/it] 80%|████████  | 1170/1459 [11:09:17<2:39:15, 33.06s/it]                                                         80%|████████  | 1170/1459 [11:09:17<2:39:15, 33.06s/it] 80%|████████  | 1171/1459 [11:09:49<2:37:43, 32.86s/it] 80%|████████  | 1172/1459 [11:10:21<2:35:33, 32.52s/it] 80%|████████  | 1173/1459 [11:10:55<2:36:35, 32.85s/it] 80%|████████  | 1174/1459 [11:11:31<2:40:24, 33.77s/it] 81%|████████  | 1175/1459 [11:12:05<2:40:25, 33.89s/it] 81%|████████  | 1176/1459 [11:12:42<2:43:55, 34.75s/it] 81%|████████  | 1177/1459 [11:13:15<2:40:53, 34.23s/it] 81%|████████  | 1178/1459 [11:13:47<2:37:43, 33.68s/it] 81%|████████  | 1179/1459 [11:14:26<2:44:15, 35.20s/it] 81%|████████  | 1180/1459 [11:14:55<2:35:55, 33.53s/it]                                                         81%|████████  | 1180/1459 [11:14:55<2:35:55, 33.53s/it] 81%|████████  | 1181/1459 [11:15:30<2:36:40, 33.81s/it] 81%|████████  | 1182/1459 [11:16:04<2:36:28, 33.89s/it] 81%|████████  | 1183/1459 [11:16:43<2:42:59, 35.43s/it] 81%|████████  | 1184/1459 [11:17:18<2:42:06, 35.37s/it] 81%|████████  | 1185/1459 [11:17:55<2:43:10, 35.73s/it] 81%|████████▏ | 1186/1459 [11:18:23<2:32:40, 33.55s/it] 81%|████████▏ | 1187/1459 [11:18:56<2:30:36, 33.22s/it] 81%|████████▏ | 1188/1459 [11:19:29<2:29:47, 33.16s/it] 81%|████████▏ | 1189/1459 [11:20:06<2:34:14, 34.28s/it] 82%|████████▏ | 1190/1459 [11:20:38<2:30:47, 33.63s/it]                                                         82%|████████▏ | 1190/1459 [11:20:38<2:30:47, 33.63s/it] 82%|████████▏ | 1191/1459 [11:21:17<2:38:04, 35.39s/it] 82%|████████▏ | 1192/1459 [11:21:49<2:33:14, 34.44s/it] 82%|████████▏ | 1193/1459 [11:22:22<2:29:37, 33.75s/it] 82%|████████▏ | 1194/1459 [11:22:58<2:31:59, 34.41s/it] 82%|████████▏ | 1195/1459 [11:23:31<2:30:35, 34.22s/it] 82%|████████▏ | 1196/1459 [11:24:07<2:32:16, 34.74s/it] 82%|████████▏ | 1197/1459 [11:24:39<2:28:18, 33.96s/it] 82%|████████▏ | 1198/1459 [11:25:09<2:22:39, 32.79s/it] 82%|████████▏ | 1199/1459 [11:25:43<2:23:27, 33.10s/it] 82%|████████▏ | 1200/1459 [11:26:18<2:24:59, 33.59s/it]                                                         82%|████████▏ | 1200/1459 [11:26:18<2:24:59, 33.59s/it] 82%|████████▏ | 1201/1459 [11:26:50<2:22:39, 33.17s/it] 82%|████████▏ | 1202/1459 [11:27:23<2:21:58, 33.15s/it] 82%|████████▏ | 1203/1459 [11:27:56<2:20:27, 32.92s/it] 83%|████████▎ | 1204/1459 [11:28:28<2:18:44, 32.65s/it] 83%|████████▎ | 1205/1459 [11:29:09<2:28:39, 35.12s/it] 83%|████████▎ | 1206/1459 [11:29:41<2:24:15, 34.21s/it] 83%|████████▎ | 1207/1459 [11:30:17<2:26:23, 34.86s/it] 83%|████████▎ | 1208/1459 [11:30:53<2:27:21, 35.23s/it] 83%|████████▎ | 1209/1459 [11:31:26<2:24:22, 34.65s/it] 83%|████████▎ | 1210/1459 [11:31:56<2:17:00, 33.01s/it]                                                         83%|████████▎ | 1210/1459 [11:31:56<2:17:00, 33.01s/it] 83%|████████▎ | 1211/1459 [11:32:27<2:14:33, 32.56s/it] 83%|████████▎ | 1212/1459 [11:33:03<2:18:14, 33.58s/it] 83%|████████▎ | 1213/1459 [11:33:37<2:17:49, 33.61s/it] 83%|████████▎ | 1214/1459 [11:34:10<2:17:02, 33.56s/it] 83%|████████▎ | 1215/1459 [11:34:48<2:22:12, 34.97s/it] 83%|████████▎ | 1216/1459 [11:35:21<2:18:30, 34.20s/it] 83%|████████▎ | 1217/1459 [11:35:53<2:14:56, 33.46s/it] 83%|████████▎ | 1218/1459 [11:36:25<2:13:40, 33.28s/it] 84%|████████▎ | 1219/1459 [11:36:56<2:10:12, 32.55s/it] 84%|████████▎ | 1220/1459 [11:37:27<2:07:32, 32.02s/it]                                                         84%|████████▎ | 1220/1459 [11:37:27<2:07:32, 32.02s/it] 84%|████████▎ | 1221/1459 [11:38:04<2:12:56, 33.52s/it] 84%|████████▍ | 1222/1459 [11:38:39<2:14:33, 34.06s/it] 84%|████████▍ | 1223/1459 [11:39:18<2:19:35, 35.49s/it] 84%|████████▍ | 1224/1459 [11:39:52<2:16:33, 34.87s/it] 84%|████████▍ | 1225/1459 [11:40:35<2:25:33, 37.32s/it] 84%|████████▍ | 1226/1459 [11:41:11<2:23:29, 36.95s/it] 84%|████████▍ | 1227/1459 [11:41:44<2:18:21, 35.78s/it] 84%|████████▍ | 1228/1459 [11:42:22<2:20:23, 36.47s/it] 84%|████████▍ | 1229/1459 [11:42:58<2:19:10, 36.31s/it] 84%|████████▍ | 1230/1459 [11:43:30<2:13:58, 35.10s/it]                                                         84%|████████▍ | 1230/1459 [11:43:30<2:13:58, 35.10s/it] 84%|████████▍ | 1231/1459 [11:44:04<2:11:58, 34.73s/it] 84%|████████▍ | 1232/1459 [11:44:38<2:10:10, 34.41s/it] 85%|████████▍ | 1233/1459 [11:45:13<2:10:49, 34.73s/it] 85%|████████▍ | 1234/1459 [11:45:48<2:10:54, 34.91s/it] 85%|████████▍ | 1235/1459 [11:46:22<2:08:28, 34.41s/it] 85%|████████▍ | 1236/1459 [11:46:57<2:09:07, 34.74s/it] 85%|████████▍ | 1237/1459 [11:47:32<2:08:03, 34.61s/it] 85%|████████▍ | 1238/1459 [11:48:03<2:04:12, 33.72s/it] 85%|████████▍ | 1239/1459 [11:48:33<1:59:21, 32.55s/it] 85%|████████▍ | 1240/1459 [11:49:09<2:02:31, 33.57s/it]                                                         85%|████████▍ | 1240/1459 [11:49:09<2:02:31, 33.57s/it] 85%|████████▌ | 1241/1459 [11:49:43<2:01:59, 33.57s/it] 85%|████████▌ | 1242/1459 [11:50:15<2:00:33, 33.33s/it] 85%|████████▌ | 1243/1459 [11:50:55<2:07:13, 35.34s/it] 85%|████████▌ | 1244/1459 [11:51:27<2:02:49, 34.28s/it] 85%|████████▌ | 1245/1459 [11:52:11<2:12:09, 37.06s/it] 85%|████████▌ | 1246/1459 [11:52:46<2:10:00, 36.62s/it] 85%|████████▌ | 1247/1459 [11:53:19<2:04:51, 35.34s/it] 86%|████████▌ | 1248/1459 [11:53:51<2:01:26, 34.54s/it] 86%|████████▌ | 1249/1459 [11:54:26<2:01:27, 34.70s/it] 86%|████████▌ | 1250/1459 [11:55:00<1:59:30, 34.31s/it]                                                         86%|████████▌ | 1250/1459 [11:55:00<1:59:30, 34.31s/it] 86%|████████▌ | 1251/1459 [11:55:30<1:54:53, 33.14s/it] 86%|████████▌ | 1252/1459 [11:56:05<1:55:33, 33.49s/it] 86%|████████▌ | 1253/1459 [11:56:36<1:53:02, 32.93s/it] 86%|████████▌ | 1254/1459 [11:57:07<1:50:17, 32.28s/it] 86%|████████▌ | 1255/1459 [11:57:41<1:51:19, 32.74s/it] 86%|████████▌ | 1256/1459 [11:58:17<1:54:16, 33.77s/it] 86%|████████▌ | 1257/1459 [11:58:49<1:51:42, 33.18s/it] 86%|████████▌ | 1258/1459 [11:59:22<1:51:33, 33.30s/it] 86%|████████▋ | 1259/1459 [11:59:55<1:50:01, 33.01s/it] 86%|████████▋ | 1260/1459 [12:00:25<1:47:16, 32.34s/it]                                                         86%|████████▋ | 1260/1459 [12:00:25<1:47:16, 32.34s/it] 86%|████████▋ | 1261/1459 [12:01:00<1:48:52, 32.99s/it] 86%|████████▋ | 1262/1459 [12:01:29<1:44:20, 31.78s/it] 87%|████████▋ | 1263/1459 [12:02:01<1:43:45, 31.76s/it] 87%|████████▋ | 1264/1459 [12:02:34<1:44:52, 32.27s/it] 87%|████████▋ | 1265/1459 [12:03:16<1:53:24, 35.08s/it] 87%|████████▋ | 1266/1459 [12:03:52<1:53:56, 35.42s/it] 87%|████████▋ | 1267/1459 [12:04:24<1:49:48, 34.31s/it] 87%|████████▋ | 1268/1459 [12:04:56<1:47:49, 33.87s/it] 87%|████████▋ | 1269/1459 [12:05:32<1:48:26, 34.24s/it] 87%|████████▋ | 1270/1459 [12:06:10<1:51:31, 35.41s/it]                                                         87%|████████▋ | 1270/1459 [12:06:10<1:51:31, 35.41s/it] 87%|████████▋ | 1271/1459 [12:06:46<1:51:51, 35.70s/it] 87%|████████▋ | 1272/1459 [12:07:19<1:48:35, 34.84s/it] 87%|████████▋ | 1273/1459 [12:07:51<1:45:34, 34.06s/it] 87%|████████▋ | 1274/1459 [12:08:26<1:45:45, 34.30s/it] 87%|████████▋ | 1275/1459 [12:08:59<1:43:40, 33.81s/it] 87%|████████▋ | 1276/1459 [12:09:28<1:39:17, 32.56s/it] 88%|████████▊ | 1277/1459 [12:10:02<1:39:34, 32.83s/it] 88%|████████▊ | 1278/1459 [12:10:34<1:38:21, 32.60s/it] 88%|████████▊ | 1279/1459 [12:11:07<1:38:10, 32.73s/it] 88%|████████▊ | 1280/1459 [12:11:41<1:39:03, 33.21s/it]                                                         88%|████████▊ | 1280/1459 [12:11:41<1:39:03, 33.21s/it] 88%|████████▊ | 1281/1459 [12:12:11<1:35:40, 32.25s/it] 88%|████████▊ | 1282/1459 [12:12:43<1:34:45, 32.12s/it] 88%|████████▊ | 1283/1459 [12:13:19<1:37:16, 33.16s/it] 88%|████████▊ | 1284/1459 [12:14:02<1:45:38, 36.22s/it] 88%|████████▊ | 1285/1459 [12:14:37<1:44:18, 35.97s/it] 88%|████████▊ | 1286/1459 [12:15:12<1:42:14, 35.46s/it] 88%|████████▊ | 1287/1459 [12:15:45<1:39:54, 34.85s/it] 88%|████████▊ | 1288/1459 [12:16:20<1:39:25, 34.89s/it] 88%|████████▊ | 1289/1459 [12:16:50<1:34:28, 33.34s/it] 88%|████████▊ | 1290/1459 [12:17:21<1:32:14, 32.75s/it]                                                         88%|████████▊ | 1290/1459 [12:17:21<1:32:14, 32.75s/it] 88%|████████▊ | 1291/1459 [12:17:52<1:30:04, 32.17s/it] 89%|████████▊ | 1292/1459 [12:18:23<1:28:43, 31.88s/it] 89%|████████▊ | 1293/1459 [12:18:58<1:30:25, 32.68s/it] 89%|████████▊ | 1294/1459 [12:19:29<1:28:36, 32.22s/it] 89%|████████▉ | 1295/1459 [12:20:01<1:28:24, 32.34s/it] 89%|████████▉ | 1296/1459 [12:20:35<1:28:36, 32.61s/it] 89%|████████▉ | 1297/1459 [12:21:16<1:34:47, 35.11s/it] 89%|████████▉ | 1298/1459 [12:21:48<1:32:06, 34.33s/it] 89%|████████▉ | 1299/1459 [12:22:27<1:34:57, 35.61s/it] 89%|████████▉ | 1300/1459 [12:22:55<1:28:53, 33.54s/it]                                                         89%|████████▉ | 1300/1459 [12:22:55<1:28:53, 33.54s/it] 89%|████████▉ | 1301/1459 [12:23:31<1:29:37, 34.04s/it] 89%|████████▉ | 1302/1459 [12:24:05<1:29:24, 34.17s/it] 89%|████████▉ | 1303/1459 [12:24:42<1:30:42, 34.89s/it] 89%|████████▉ | 1304/1459 [12:25:16<1:29:33, 34.67s/it] 89%|████████▉ | 1305/1459 [12:25:51<1:29:35, 34.90s/it] 90%|████████▉ | 1306/1459 [12:26:26<1:28:41, 34.78s/it] 90%|████████▉ | 1307/1459 [12:26:57<1:25:45, 33.85s/it] 90%|████████▉ | 1308/1459 [12:27:31<1:24:37, 33.62s/it] 90%|████████▉ | 1309/1459 [12:28:07<1:26:11, 34.48s/it] 90%|████████▉ | 1310/1459 [12:28:48<1:30:20, 36.38s/it]                                                         90%|████████▉ | 1310/1459 [12:28:48<1:30:20, 36.38s/it] 90%|████████▉ | 1311/1459 [12:29:19<1:25:59, 34.86s/it] 90%|████████▉ | 1312/1459 [12:29:55<1:25:57, 35.09s/it] 90%|████████▉ | 1313/1459 [12:30:25<1:21:45, 33.60s/it] 90%|█████████ | 1314/1459 [12:30:54<1:18:15, 32.38s/it] 90%|█████████ | 1315/1459 [12:31:29<1:18:55, 32.89s/it] 90%|█████████ | 1316/1459 [12:32:04<1:19:55, 33.53s/it] 90%|█████████ | 1317/1459 [12:32:37<1:19:23, 33.54s/it] 90%|█████████ | 1318/1459 [12:33:12<1:19:33, 33.85s/it] 90%|█████████ | 1319/1459 [12:33:45<1:18:23, 33.59s/it] 90%|█████████ | 1320/1459 [12:34:20<1:19:20, 34.25s/it]                                                         90%|█████████ | 1320/1459 [12:34:20<1:19:20, 34.25s/it] 91%|█████████ | 1321/1459 [12:34:52<1:16:53, 33.43s/it] 91%|█████████ | 1322/1459 [12:35:29<1:19:00, 34.60s/it] 91%|█████████ | 1323/1459 [12:36:04<1:18:16, 34.53s/it] 91%|█████████ | 1324/1459 [12:36:35<1:15:10, 33.41s/it] 91%|█████████ | 1325/1459 [12:37:10<1:15:44, 33.91s/it] 91%|█████████ | 1326/1459 [12:37:45<1:16:01, 34.30s/it] 91%|█████████ | 1327/1459 [12:38:20<1:16:04, 34.58s/it] 91%|█████████ | 1328/1459 [12:38:55<1:15:58, 34.80s/it] 91%|█████████ | 1329/1459 [12:39:29<1:14:54, 34.58s/it] 91%|█████████ | 1330/1459 [12:40:01<1:12:43, 33.82s/it]                                                         91%|█████████ | 1330/1459 [12:40:01<1:12:43, 33.82s/it] 91%|█████████ | 1331/1459 [12:40:32<1:10:22, 32.99s/it] 91%|█████████▏| 1332/1459 [12:41:05<1:09:24, 32.79s/it] 91%|█████████▏| 1333/1459 [12:41:36<1:08:01, 32.40s/it] 91%|█████████▏| 1334/1459 [12:42:08<1:07:14, 32.28s/it] 92%|█████████▏| 1335/1459 [12:42:42<1:07:21, 32.59s/it] 92%|█████████▏| 1336/1459 [12:43:18<1:08:54, 33.62s/it] 92%|█████████▏| 1337/1459 [12:43:58<1:12:41, 35.75s/it] 92%|█████████▏| 1338/1459 [12:44:30<1:09:21, 34.39s/it] 92%|█████████▏| 1339/1459 [12:45:12<1:13:50, 36.92s/it] 92%|█████████▏| 1340/1459 [12:45:48<1:12:32, 36.58s/it]                                                         92%|█████████▏| 1340/1459 [12:45:48<1:12:32, 36.58s/it] 92%|█████████▏| 1341/1459 [12:46:19<1:08:14, 34.70s/it] 92%|█████████▏| 1342/1459 [12:46:57<1:09:42, 35.75s/it] 92%|█████████▏| 1343/1459 [12:47:30<1:07:38, 34.99s/it] 92%|█████████▏| 1344/1459 [12:48:07<1:08:25, 35.70s/it] 92%|█████████▏| 1345/1459 [12:48:44<1:08:23, 35.99s/it] 92%|█████████▏| 1346/1459 [12:49:19<1:07:03, 35.60s/it] 92%|█████████▏| 1347/1459 [12:49:52<1:05:09, 34.91s/it] 92%|█████████▏| 1348/1459 [12:50:24<1:02:55, 34.02s/it] 92%|█████████▏| 1349/1459 [12:50:55<1:00:36, 33.06s/it] 93%|█████████▎| 1350/1459 [12:51:30<1:01:08, 33.66s/it]                                                         93%|█████████▎| 1350/1459 [12:51:30<1:01:08, 33.66s/it] 93%|█████████▎| 1351/1459 [12:52:03<1:00:10, 33.43s/it] 93%|█████████▎| 1352/1459 [12:52:37<59:59, 33.64s/it]   93%|█████████▎| 1353/1459 [12:53:12<1:00:22, 34.18s/it] 93%|█████████▎| 1354/1459 [12:53:42<57:19, 32.75s/it]   93%|█████████▎| 1355/1459 [12:54:13<56:03, 32.34s/it] 93%|█████████▎| 1356/1459 [12:54:44<54:57, 32.02s/it] 93%|█████████▎| 1357/1459 [12:55:26<59:23, 34.94s/it] 93%|█████████▎| 1358/1459 [12:56:01<58:56, 35.01s/it] 93%|█████████▎| 1359/1459 [12:56:32<56:19, 33.80s/it] 93%|█████████▎| 1360/1459 [12:57:03<54:10, 32.84s/it]                                                       93%|█████████▎| 1360/1459 [12:57:03<54:10, 32.84s/it] 93%|█████████▎| 1361/1459 [12:57:37<54:30, 33.37s/it] 93%|█████████▎| 1362/1459 [12:58:10<53:27, 33.07s/it] 93%|█████████▎| 1363/1459 [12:58:41<51:58, 32.49s/it] 93%|█████████▎| 1364/1459 [12:59:13<51:25, 32.48s/it] 94%|█████████▎| 1365/1459 [12:59:46<51:10, 32.67s/it] 94%|█████████▎| 1366/1459 [13:00:17<49:45, 32.10s/it] 94%|█████████▎| 1367/1459 [13:00:50<49:18, 32.16s/it] 94%|█████████▍| 1368/1459 [13:01:26<50:43, 33.44s/it] 94%|█████████▍| 1369/1459 [13:01:59<49:50, 33.23s/it] 94%|█████████▍| 1370/1459 [13:02:35<50:31, 34.07s/it]                                                       94%|█████████▍| 1370/1459 [13:02:35<50:31, 34.07s/it] 94%|█████████▍| 1371/1459 [13:03:07<49:08, 33.50s/it] 94%|█████████▍| 1372/1459 [13:03:39<47:58, 33.08s/it] 94%|█████████▍| 1373/1459 [13:04:10<46:28, 32.42s/it] 94%|█████████▍| 1374/1459 [13:04:50<49:15, 34.77s/it] 94%|█████████▍| 1375/1459 [13:05:23<47:40, 34.05s/it] 94%|█████████▍| 1376/1459 [13:05:59<48:08, 34.80s/it] 94%|█████████▍| 1377/1459 [13:06:35<48:06, 35.21s/it] 94%|█████████▍| 1378/1459 [13:07:07<46:14, 34.26s/it] 95%|█████████▍| 1379/1459 [13:07:39<44:35, 33.45s/it] 95%|█████████▍| 1380/1459 [13:08:11<43:34, 33.09s/it]                                                       95%|█████████▍| 1380/1459 [13:08:11<43:34, 33.09s/it] 95%|█████████▍| 1381/1459 [13:08:47<43:58, 33.82s/it] 95%|█████████▍| 1382/1459 [13:09:22<43:51, 34.17s/it] 95%|█████████▍| 1383/1459 [13:09:53<42:10, 33.30s/it] 95%|█████████▍| 1384/1459 [13:10:28<42:13, 33.78s/it] 95%|█████████▍| 1385/1459 [13:11:14<46:13, 37.47s/it] 95%|█████████▍| 1386/1459 [13:11:49<44:48, 36.83s/it] 95%|█████████▌| 1387/1459 [13:12:25<43:42, 36.42s/it] 95%|█████████▌| 1388/1459 [13:12:58<42:06, 35.59s/it] 95%|█████████▌| 1389/1459 [13:13:33<41:04, 35.20s/it] 95%|█████████▌| 1390/1459 [13:14:05<39:23, 34.25s/it]                                                       95%|█████████▌| 1390/1459 [13:14:05<39:23, 34.25s/it] 95%|█████████▌| 1391/1459 [13:14:38<38:37, 34.09s/it] 95%|█████████▌| 1392/1459 [13:15:15<39:00, 34.93s/it] 95%|█████████▌| 1393/1459 [13:15:55<40:07, 36.48s/it] 96%|█████████▌| 1394/1459 [13:16:30<39:02, 36.03s/it] 96%|█████████▌| 1395/1459 [13:17:03<37:26, 35.10s/it] 96%|█████████▌| 1396/1459 [13:17:38<36:42, 34.96s/it] 96%|█████████▌| 1397/1459 [13:18:10<35:08, 34.01s/it] 96%|█████████▌| 1398/1459 [13:18:43<34:24, 33.85s/it] 96%|█████████▌| 1399/1459 [13:19:14<32:59, 32.99s/it] 96%|█████████▌| 1400/1459 [13:19:53<34:10, 34.75s/it]                                                       96%|█████████▌| 1400/1459 [13:19:53<34:10, 34.75s/it] 96%|█████████▌| 1401/1459 [13:20:24<32:37, 33.74s/it] 96%|█████████▌| 1402/1459 [13:20:58<32:03, 33.74s/it] 96%|█████████▌| 1403/1459 [13:21:28<30:29, 32.68s/it] 96%|█████████▌| 1404/1459 [13:22:02<30:09, 32.89s/it] 96%|█████████▋| 1405/1459 [13:22:41<31:25, 34.92s/it] 96%|█████████▋| 1406/1459 [13:23:18<31:13, 35.35s/it] 96%|█████████▋| 1407/1459 [13:23:51<30:05, 34.72s/it] 97%|█████████▋| 1408/1459 [13:24:26<29:43, 34.96s/it] 97%|█████████▋| 1409/1459 [13:24:58<28:14, 33.89s/it] 97%|█████████▋| 1410/1459 [13:25:33<28:04, 34.37s/it]                                                       97%|█████████▋| 1410/1459 [13:25:33<28:04, 34.37s/it] 97%|█████████▋| 1411/1459 [13:26:06<27:11, 33.99s/it] 97%|█████████▋| 1412/1459 [13:26:43<27:09, 34.67s/it] 97%|█████████▋| 1413/1459 [13:27:16<26:11, 34.16s/it] 97%|█████████▋| 1414/1459 [13:27:48<25:06, 33.48s/it] 97%|█████████▋| 1415/1459 [13:28:23<24:55, 33.98s/it] 97%|█████████▋| 1416/1459 [13:28:51<23:10, 32.35s/it] 97%|█████████▋| 1417/1459 [13:29:29<23:42, 33.87s/it] 97%|█████████▋| 1418/1459 [13:30:05<23:32, 34.46s/it] 97%|█████████▋| 1419/1459 [13:30:38<22:49, 34.24s/it] 97%|█████████▋| 1420/1459 [13:31:16<22:51, 35.16s/it]                                                       97%|█████████▋| 1420/1459 [13:31:16<22:51, 35.16s/it] 97%|█████████▋| 1421/1459 [13:31:49<21:58, 34.69s/it] 97%|█████████▋| 1422/1459 [13:32:27<21:53, 35.51s/it] 98%|█████████▊| 1423/1459 [13:33:01<21:06, 35.18s/it] 98%|█████████▊| 1424/1459 [13:33:37<20:38, 35.37s/it] 98%|█████████▊| 1425/1459 [13:34:15<20:32, 36.24s/it] 98%|█████████▊| 1426/1459 [13:34:46<19:06, 34.73s/it] 98%|█████████▊| 1427/1459 [13:35:19<18:15, 34.23s/it] 98%|█████████▊| 1428/1459 [13:35:52<17:25, 33.71s/it] 98%|█████████▊| 1429/1459 [13:36:26<16:55, 33.85s/it] 98%|█████████▊| 1430/1459 [13:37:04<16:56, 35.05s/it]                                                       98%|█████████▊| 1430/1459 [13:37:04<16:56, 35.05s/it] 98%|█████████▊| 1431/1459 [13:37:38<16:16, 34.87s/it] 98%|█████████▊| 1432/1459 [13:38:15<15:57, 35.45s/it] 98%|█████████▊| 1433/1459 [13:38:51<15:25, 35.58s/it] 98%|█████████▊| 1434/1459 [13:39:24<14:32, 34.90s/it] 98%|█████████▊| 1435/1459 [13:39:59<13:56, 34.87s/it] 98%|█████████▊| 1436/1459 [13:40:31<13:02, 34.03s/it] 98%|█████████▊| 1437/1459 [13:41:04<12:20, 33.65s/it] 99%|█████████▊| 1438/1459 [13:41:36<11:37, 33.22s/it] 99%|█████████▊| 1439/1459 [13:42:09<11:04, 33.25s/it] 99%|█████████▊| 1440/1459 [13:42:44<10:38, 33.60s/it]                                                       99%|█████████▊| 1440/1459 [13:42:44<10:38, 33.60s/it] 99%|█████████▉| 1441/1459 [13:43:20<10:20, 34.48s/it] 99%|█████████▉| 1442/1459 [13:43:54<09:39, 34.07s/it] 99%|█████████▉| 1443/1459 [13:44:26<08:55, 33.48s/it] 99%|█████████▉| 1444/1459 [13:44:57<08:14, 32.96s/it] 99%|█████████▉| 1445/1459 [13:45:32<07:48, 33.44s/it] 99%|█████████▉| 1446/1459 [13:46:04<07:09, 33.05s/it] 99%|█████████▉| 1447/1459 [13:46:37<06:34, 32.88s/it] 99%|█████████▉| 1448/1459 [[INFO|trainer.py:2808] 2025-10-04 12:43:49,890 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2808] 2025-10-04 12:43:49,890 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2808] 2025-10-04 12:43:49,891 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2808] 2025-10-04 12:43:49,891 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2808] 2025-10-04 12:43:49,891 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2808] 2025-10-04 12:43:49,891 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2808] 2025-10-04 12:43:49,891 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


13:47:09<06:01, 32.87s/it] 99%|█████████▉| 1449/1459 [13:47:42<05:27, 32.79s/it] 99%|█████████▉| 1450/1459 [13:48:17<05:01, 33.48s/it]                                                       99%|█████████▉| 1450/1459 [13:48:17<05:01, 33.48s/it] 99%|█████████▉| 1451/1459 [13:48:49<04:25, 33.14s/it]100%|█████████▉| 1452/1459 [13:49:23<03:51, 33.12s/it]100%|█████████▉| 1453/1459 [13:49:56<03:19, 33.31s/it]100%|█████████▉| 1454/1459 [13:50:28<02:44, 32.95s/it]100%|█████████▉| 1455/1459 [13:51:02<02:12, 33.18s/it]100%|█████████▉| 1456/1459 [13:51:35<01:39, 33.03s/it]100%|█████████▉| 1457/1459 [13:52:08<01:06, 33.24s/it]100%|█████████▉| 1458/1459 [13:52:38<00:32, 32.04s/it]100%|██████████| 1459/1459 [13:52:55<00:00, 27.59s/it][INFO|trainer.py:4289] 2025-10-04 12:43:49,892 >> Saving model checkpoint to /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1459
[INFO|configuration_utils.py:491] 2025-10-04 12:43:49,904 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1459/config.json
[INFO|configuration_utils.py:826] 2025-10-04 12:43:49,904 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1459/generation_config.json
[INFO|modeling_utils.py:4308] 2025-10-04 12:44:09,304 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1459/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2394] 2025-10-04 12:44:09,305 >> chat template saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1459/chat_template.jinja
[INFO|tokenization_utils_base.py:2563] 2025-10-04 12:44:09,306 >> tokenizer config file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1459/tokenizer_config.json
[INFO|tokenization_utils_base.py:2572] 2025-10-04 12:44:09,307 >> Special tokens file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-1459/special_tokens_map.json
[INFO|trainer.py:2808] 2025-10-04 12:44:09,503 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                      100%|██████████| 1459/1459 [13:53:26<00:00, 27.59s/it]100%|██████████| 1459/1459 [13:53:26<00:00, 34.27s/it]
[INFO|trainer.py:4289] 2025-10-04 12:44:21,361 >> Saving model checkpoint to /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune
[INFO|configuration_utils.py:491] 2025-10-04 12:44:21,366 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/config.json
[INFO|configuration_utils.py:826] 2025-10-04 12:44:21,366 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/generation_config.json
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x1518d4fe6520>
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 186, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 105, in put
    os.replace(self.file_path + ".tmp", self.file_path)
FileNotFoundError: [Errno 2] No such file or directory: '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle.tmp' -> '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle'
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x15006373e520>
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x145c067f2520>
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 186, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 105, in put
    os.replace(self.file_path + ".tmp", self.file_path)
FileNotFoundError: [Errno 2] No such file or directory: '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle.tmp' -> '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle'
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x154e2d706520>
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 186, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
    cache_manager.put(autotune_table)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 186, in _update_autotune_table
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 105, in put
    os.replace(self.file_path + ".tmp", self.file_path)
    cache_manager.put(autotune_table)
FileNotFoundError: [Errno 2] No such file or directory: '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle.tmp' -> '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle'
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 105, in put
    os.replace(self.file_path + ".tmp", self.file_path)
FileNotFoundError: [Errno 2] No such file or directory: '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle.tmp' -> '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle'
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x150a1ccfa520>
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 186, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 105, in put
    os.replace(self.file_path + ".tmp", self.file_path)
FileNotFoundError: [Errno 2] No such file or directory: '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle.tmp' -> '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle'
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x154eb0492520>
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x14dc18fd2520>
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 186, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 186, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 105, in put
    os.replace(self.file_path + ".tmp", self.file_path)
    cache_manager.put(autotune_table)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 105, in put
FileNotFoundError: [Errno 2] No such file or directory: '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle.tmp' -> '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle'
    os.replace(self.file_path + ".tmp", self.file_path)
FileNotFoundError: [Errno 2] No such file or directory: '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle.tmp' -> '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle'
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x14aa7210e520>
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 186, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 105, in put
    os.replace(self.file_path + ".tmp", self.file_path)
FileNotFoundError: [Errno 2] No such file or directory: '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle.tmp' -> '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle'
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x150f9a6ba520>
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 186, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 105, in put
    os.replace(self.file_path + ".tmp", self.file_path)
FileNotFoundError: [Errno 2] No such file or directory: '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle.tmp' -> '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle'
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x147867fb2520>
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 186, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 105, in put
    os.replace(self.file_path + ".tmp", self.file_path)
FileNotFoundError: [Errno 2] No such file or directory: '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle.tmp' -> '/scratch/gpfs/yl7690/.triton/autotune/Fp16Matmul_2d_kernel.pickle'
[INFO|modeling_utils.py:4308] 2025-10-04 12:44:38,805 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2394] 2025-10-04 12:44:38,807 >> chat template saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/chat_template.jinja
[INFO|tokenization_utils_base.py:2563] 2025-10-04 12:44:38,807 >> tokenizer config file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/tokenizer_config.json
[INFO|tokenization_utils_base.py:2572] 2025-10-04 12:44:38,808 >> Special tokens file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/special_tokens_map.json
[INFO|modelcard.py:456] 2025-10-04 12:44:39,233 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
