+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j15g1,della-k11g3,della-k12g2,della-k13g[1-3],della-k14g[2-3]'
++ head -n 1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j15g1,della-k11g3,della-k12g2,della-k13g[1-3],della-k14g[2-3]'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=0 --master_addr=della-j15g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type lora --lora_rank 8 --lora_target all --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr1.0e-4_cosine_20_lora_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j15g1,della-k11g3,della-k12g2,della-k13g[1-3],della-k14g[2-3]'
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j15g1,della-k11g3,della-k12g2,della-k13g[1-3],della-k14g[2-3]'
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=5 --master_addr=della-j15g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type lora --lora_rank 8 --lora_target all --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr1.0e-4_cosine_20_lora_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j15g1,della-k11g3,della-k12g2,della-k13g[1-3],della-k14g[2-3]'
++ scontrol show hostnames 'della-j15g1,della-k11g3,della-k12g2,della-k13g[1-3],della-k14g[2-3]'
++ head -n 1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j15g1,della-k11g3,della-k12g2,della-k13g[1-3],della-k14g[2-3]'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=4 --master_addr=della-j15g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type lora --lora_rank 8 --lora_target all --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr1.0e-4_cosine_20_lora_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=3 --master_addr=della-j15g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type lora --lora_rank 8 --lora_target all --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr1.0e-4_cosine_20_lora_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=7 --master_addr=della-j15g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type lora --lora_rank 8 --lora_target all --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr1.0e-4_cosine_20_lora_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=6 --master_addr=della-j15g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type lora --lora_rank 8 --lora_target all --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr1.0e-4_cosine_20_lora_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=1 --master_addr=della-j15g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type lora --lora_rank 8 --lora_target all --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr1.0e-4_cosine_20_lora_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
++ head -n 1
++ scontrol show hostnames 'della-j15g1,della-k11g3,della-k12g2,della-k13g[1-3],della-k14g[2-3]'
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=2 --master_addr=della-j15g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type lora --lora_rank 8 --lora_target all --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr1.0e-4_cosine_20_lora_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
W1004 12:02:01.894000 3429774 site-packages/torch/distributed/run.py:774] 
W1004 12:02:01.894000 3429774 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:01.894000 3429774 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:02:01.894000 3429774 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.829000 2456729 site-packages/torch/distributed/run.py:774] 
W1004 12:02:03.829000 2456729 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.829000 2456729 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:02:03.829000 2456729 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.829000 3410539 site-packages/torch/distributed/run.py:774] 
W1004 12:02:03.829000 3410539 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.829000 3410539 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:02:03.829000 3410539 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.830000 2535191 site-packages/torch/distributed/run.py:774] 
W1004 12:02:03.830000 2535191 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.830000 2535191 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:02:03.830000 2535191 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.832000 169633 site-packages/torch/distributed/run.py:774] 
W1004 12:02:03.832000 169633 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.832000 169633 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:02:03.832000 169633 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.833000 3060491 site-packages/torch/distributed/run.py:774] 
W1004 12:02:03.833000 3060491 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.833000 3060491 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:02:03.833000 3060491 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.844000 3903401 site-packages/torch/distributed/run.py:774] 
W1004 12:02:03.844000 3903401 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.844000 3903401 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:02:03.844000 3903401 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.942000 2823983 site-packages/torch/distributed/run.py:774] 
W1004 12:02:03.942000 2823983 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:02:03.942000 2823983 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:02:03.942000 2823983 site-packages/torch/distributed/run.py:774] *****************************************
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,388 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,388 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,388 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,388 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,388 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,388 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,392 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,392 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,392 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,392 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,392 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,392 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,394 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,394 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,394 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,394 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,394 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,394 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,401 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,401 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,401 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,401 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,401 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,401 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,401 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,401 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,401 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,401 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,401 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,401 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,402 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,402 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,402 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,402 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,402 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,402 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,603 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,603 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,603 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,603 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,603 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,603 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,755 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,755 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,755 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,755 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,755 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:48,755 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,066 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:02:49,066 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,075 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:02:49,075 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 12:02:49,076 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,076 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,076 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,076 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,076 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,076 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,076 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,080 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:02:49,081 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 12:02:49,082 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,082 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,082 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,082 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,082 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,082 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,082 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,082 >> loading file chat_template.jinja
[INFO|configuration_utils.py:763] 2025-10-04 12:02:49,083 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 12:02:49,083 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,084 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,084 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,084 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,084 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,084 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,084 >> loading file chat_template.jinja
[INFO|configuration_utils.py:839] 2025-10-04 12:02:49,090 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,090 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,090 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,090 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,090 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,090 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,090 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,090 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:02:49,091 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 12:02:49,093 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,093 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,093 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,093 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,093 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,093 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,093 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,094 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:02:49,095 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 12:02:49,097 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,098 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,098 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,098 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,098 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,098 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,098 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,271 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:02:49,271 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 12:02:49,276 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,277 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,277 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,277 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,277 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,277 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,277 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,412 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:02:49,412 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 12:02:49,414 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,415 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,415 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,415 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,415 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,415 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:02:49,415 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,753 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,779 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,791 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,807 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,815 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,824 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:49,991 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:02:50,120 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187099 examples [00:01, 303.09 examples/s]            Converting format of dataset (num_proc=64): 208681 examples [00:01, 20832.76 examples/s]Converting format of dataset (num_proc=64): 230388 examples [00:01, 44305.25 examples/s]Converting format of dataset (num_proc=64): 251283 examples [00:01, 68346.33 examples/s]Converting format of dataset (num_proc=64): 274764 examples [00:01, 97030.47 examples/s]Converting format of dataset (num_proc=64): 295007 examples [00:01, 116299.59 examples/s]Converting format of dataset (num_proc=64): 314856 examples [00:01, 134065.13 examples/s]Converting format of dataset (num_proc=64): 335143 examples [00:02, 150001.92 examples/s]Converting format of dataset (num_proc=64): 355025 examples [00:02, 132703.44 examples/s]Converting format of dataset (num_proc=64): 372162 examples [00:02, 86466.89 examples/s] Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187149 examples [00:01, 327.69 examples/s]            Converting format of dataset (num_proc=64): 195923 examples [00:01, 8324.24 examples/s]Converting format of dataset (num_proc=64): 227419 examples [00:01, 43691.37 examples/s]Converting format of dataset (num_proc=64): 249522 examples [00:01, 68699.17 examples/s]Converting format of dataset (num_proc=64): 270116 examples [00:01, 91748.37 examples/s]Converting format of dataset (num_proc=64): 294909 examples [00:01, 121521.74 examples/s]Converting format of dataset (num_proc=64): 316155 examples [00:02, 137811.87 examples/s]Converting format of dataset (num_proc=64): 339680 examples [00:02, 160240.03 examples/s]Converting format of dataset (num_proc=64): 361337 examples [00:02, 138824.41 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 47334.67 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187188 examples [00:01, 368.37 examples/s]            Converting format of dataset (num_proc=64): 200845 examples [00:01, 13333.72 examples/s]Converting format of dataset (num_proc=64): 224008 examples [00:01, 39464.60 examples/s]Converting format of dataset (num_proc=64): 253571 examples [00:01, 76711.84 examples/s]Converting format of dataset (num_proc=64): 274268 examples [00:01, 96409.09 examples/s]Converting format of dataset (num_proc=64): 301652 examples [00:01, 130664.27 examples/s]Converting format of dataset (num_proc=64): 324195 examples [00:02, 139865.17 examples/s]Converting format of dataset (num_proc=64): 345800 examples [00:02, 154590.53 examples/s]Converting format of dataset (num_proc=64): 366402 examples [00:02, 122441.38 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 46713.20 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187223 examples [00:01, 408.05 examples/s]            Converting format of dataset (num_proc=64): 219051 examples [00:01, 31618.86 examples/s]Converting format of dataset (num_proc=64): 237927 examples [00:01, 50210.23 examples/s]Converting format of dataset (num_proc=64): 265343 examples [00:01, 82690.87 examples/s]Converting format of dataset (num_proc=64): 290058 examples [00:01, 111094.12 examples/s]Converting format of dataset (num_proc=64): 312658 examples [00:01, 130782.08 examples/s]Converting format of dataset (num_proc=64): 334593 examples [00:01, 147012.95 examples/s]Converting format of dataset (num_proc=64): 356402 examples [00:02, 147441.81 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:04, 45905.72 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187077 examples [00:01, 311.08 examples/s]            Converting format of dataset (num_proc=64): 211181 examples [00:01, 24914.45 examples/s]Converting format of dataset (num_proc=64): 233922 examples [00:01, 50683.12 examples/s]Converting format of dataset (num_proc=64): 255689 examples [00:01, 76230.80 examples/s]Converting format of dataset (num_proc=64): 278857 examples [00:01, 104036.28 examples/s]Converting format of dataset (num_proc=64): 299840 examples [00:01, 121791.51 examples/s]Converting format of dataset (num_proc=64): 319679 examples [00:01, 137018.40 examples/s]Converting format of dataset (num_proc=64): 339213 examples [00:01, 145205.24 examples/s]Converting format of dataset (num_proc=64): 357835 examples [00:02, 138731.12 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:04, 45477.24 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187174 examples [00:01, 361.63 examples/s]            Converting format of dataset (num_proc=64): 195008 examples [00:01, 7852.07 examples/s]Converting format of dataset (num_proc=64): 217417 examples [00:01, 33943.17 examples/s]Converting format of dataset (num_proc=64): 249803 examples [00:01, 76440.53 examples/s]Converting format of dataset (num_proc=64): 270466 examples [00:01, 96906.43 examples/s]Converting format of dataset (num_proc=64): 296724 examples [00:01, 128987.06 examples/s]Converting format of dataset (num_proc=64): 319336 examples [00:01, 148954.09 examples/s]Converting format of dataset (num_proc=64): 341234 examples [00:02, 159986.38 examples/s]Converting format of dataset (num_proc=64): 362193 examples [00:02, 137004.00 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:04, 46156.11 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187187 examples [00:01, 334.66 examples/s]            Converting format of dataset (num_proc=64): 214083 examples [00:01, 23806.73 examples/s]Converting format of dataset (num_proc=64): 237993 examples [00:01, 47535.10 examples/s]Converting format of dataset (num_proc=64): 260589 examples [00:01, 71809.13 examples/s]Converting format of dataset (num_proc=64): 283808 examples [00:01, 97711.57 examples/s]Converting format of dataset (num_proc=64): 305140 examples [00:01, 119392.38 examples/s]Converting format of dataset (num_proc=64): 326735 examples [00:02, 135799.84 examples/s]Converting format of dataset (num_proc=64): 347289 examples [00:02, 147733.31 examples/s]Converting format of dataset (num_proc=64): 367346 examples [00:02, 108124.14 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:04, 45317.57 examples/s] 
Converting format of dataset (num_proc=64): 373376 examples [00:04, 45072.49 examples/s]
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187205 examples [00:01, 370.70 examples/s]            Converting format of dataset (num_proc=64): 193778 examples [00:01, 6443.48 examples/s]Converting format of dataset (num_proc=64): 218915 examples [00:01, 35068.21 examples/s]Converting format of dataset (num_proc=64): 243646 examples [00:01, 65302.25 examples/s]Converting format of dataset (num_proc=64): 266739 examples [00:01, 92907.56 examples/s]Converting format of dataset (num_proc=64): 293263 examples [00:01, 126357.39 examples/s]Converting format of dataset (num_proc=64): 315112 examples [00:02, 139912.53 examples/s]Converting format of dataset (num_proc=64): 336071 examples [00:02, 154136.28 examples/s]Converting format of dataset (num_proc=64): 356489 examples [00:02, 149505.12 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:04, 44229.48 examples/s] 
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:13, 75.32 examples/s]             Running tokenizer on dataset (num_proc=64): 189688 examples [00:13, 271.81 examples/s]Running tokenizer on dataset (num_proc=64): 190688 examples [00:14, 378.18 examples/s]Running tokenizer on dataset (num_proc=64): 192688 examples [00:14, 718.92 examples/s]Running tokenizer on dataset (num_proc=64): 193688 examples [00:15, 870.37 examples/s]Running tokenizer on dataset (num_proc=64): 196688 examples [00:15, 1623.61 examples/s]Running tokenizer on dataset (num_proc=64): 199688 examples [00:16, 2070.67 examples/s]Running tokenizer on dataset (num_proc=64): 200688 examples [00:17, 2054.34 examples/s]Running tokenizer on dataset (num_proc=64): 202688 examples [00:17, 2481.37 examples/s]Running tokenizer on dataset (num_proc=64): 203688 examples [00:18, 2393.56 examples/s]Running tokenizerRunning tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:13, 73.75 examples/s]             Running tokenizer on dataset (num_proc=64): 188688 examples [00:14, 169.70 examples/s]Running tokenizer on dataset (num_proc=64): 190688 examples [00:14, 423.95 examples/s]Running tokenizer on dataset (num_proc=64): 193688 examples [00:15, 903.36 examples/s]Running tokenizer on dataset (num_proc=64): 194688 examples [00:16, 918.64 examples/s]Running tokenizer on dataset (num_proc=64): 195688 examples [00:16, 1055.65 examples/s]Running tokenizer on dataset (num_proc=64): 197688 examples [00:17, 1294.63 examples/s]Running tokenizer on dataset (num_proc=64): 200688 examples [00:18, 2026.16 examples/s]Running tokenizer on dataset (num_proc=64): 201688 examples [00:18, 2008.44 examples/s]Running tokenizer on dataset (num_proc=64): 204688 examples [00:19, 2822.42 examples/s]Running tokenizerRunning tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:13, 71.64 examples/s]             Running tokenizer on dataset (num_proc=64): 189688 examples [00:14, 269.43 examples/s]Running tokenizer on dataset (num_proc=64): 190688 examples [00:15, 365.97 examples/s]Running tokenizer on dataset (num_proc=64): 192688 examples [00:15, 636.70 examples/s]Running tokenizer on dataset (num_proc=64): 194688 examples [00:16, 994.77 examples/s]Running tokenizer on dataset (num_proc=64): 196688 examples [00:16, 1373.42 examples/s]Running tokenizer on dataset (num_proc=64): 197688 examples [00:17, 1384.11 examples/s]Running tokenizer on dataset (num_proc=64): 199688 examples [00:18, 1776.44 examples/s]Running tokenizer on dataset (num_proc=64): 200688 examples [00:18, 1785.94 examples/s]Running tokenizer on dataset (num_proc=64): 202688 examples [00:19, 2075.74 examples/s]Running tokenizerRunning tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:14, 67.30 examples/s]             Running tokenizer on dataset (num_proc=64): 189688 examples [00:15, 246.92 examples/s]Running tokenizer on dataset (num_proc=64): 191688 examples [00:15, 473.33 examples/s]Running tokenizer on dataset (num_proc=64): 192688 examples [00:16, 598.90 examples/s]Running tokenizer on dataset (num_proc=64): 196688 examples [00:16, 1363.41 examples/s]Running tokenizer on dataset (num_proc=64): 198688 examples [00:17, 1699.73 examples/s]Running tokenizer on dataset (num_proc=64): 200688 examples [00:17, 2051.90 examples/s]Running tokenizer on dataset (num_proc=64): 203688 examples [00:18, 2623.48 examples/s]Running tokenizer on dataset (num_proc=64): 206688 examples [00:19, 3278.71 examples/s]Running tokenizer on dataset (num_proc=64): 207688 examples [00:20, 2433.47 examples/s]Running tokenizeRunning tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:13, 72.68 examples/s]             Running tokenizer on dataset (num_proc=64): 188688 examples [00:14, 167.07 examples/s]Running tokenizer on dataset (num_proc=64): 189688 examples [00:15, 265.43 examples/s]Running tokenizer on dataset (num_proc=64): 192688 examples [00:15, 715.60 examples/s]Running tokenizer on dataset (num_proc=64): 195688 examples [00:16, 1234.05 examples/s]Running tokenizer on dataset (num_proc=64): 199688 examples [00:17, 2050.20 examples/s]Running tokenizer on dataset (num_proc=64): 202688 examples [00:17, 2556.10 examples/s]Running tokenizer on dataset (num_proc=64): 203688 examples [00:18, 2471.08 examples/s]Running tokenizer on dataset (num_proc=64): 204688 examples [00:19, 1911.20 examples/s]Running tokenizer on dataset (num_proc=64): 206688 examples [00:19, 2247.21 examples/s]Running tokenizeRunning tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:14, 70.14 examples/s]             Running tokenizer on dataset (num_proc=64): 189688 examples [00:15, 245.37 examples/s]Running tokenizer on dataset (num_proc=64): 192688 examples [00:15, 594.69 examples/s]Running tokenizer on dataset (num_proc=64): 194688 examples [00:16, 851.88 examples/s]Running tokenizer on dataset (num_proc=64): 196688 examples [00:16, 1163.80 examples/s]Running tokenizer on dataset (num_proc=64): 198688 examples [00:17, 1514.70 examples/s]Running tokenizer on dataset (num_proc=64): 200688 examples [00:17, 1882.19 examples/s]Running tokenizer on dataset (num_proc=64): 202688 examples [00:19, 1863.73 examples/s]Running tokenizer on dataset (num_proc=64): 206688 examples [00:19, 2905.07 examples/s]Running tokenizer on dataset (num_proc=64): 208688 examples [00:20, 2571.62 examples/s]Running tokenizeRunning tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:14, 68.99 examples/s]             Running tokenizer on dataset (num_proc=64): 188688 examples [00:14, 159.73 examples/s]Running tokenizer on dataset (num_proc=64): 191688 examples [00:16, 494.35 examples/s]Running tokenizer on dataset (num_proc=64): 193688 examples [00:17, 707.15 examples/s]Running tokenizer on dataset (num_proc=64): 198688 examples [00:18, 1420.99 examples/s]Running tokenizer on dataset (num_proc=64): 199688 examples [00:18, 1496.65 examples/s]Running tokenizer on dataset (num_proc=64): 203688 examples [00:19, 2366.88 examples/s]Running tokenizer on dataset (num_proc=64): 206688 examples [00:19, 2925.95 examples/s]Running tokenizer on dataset (num_proc=64): 210688 examples [00:19, 4194.98 examples/s]Running tokenizer on dataset (num_proc=64): 213688 examples [00:20, 3852.86 examples/s]Running tokenizeRunning tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:14, 67.44 examples/s]             Running tokenizer on dataset (num_proc=64): 189688 examples [00:15, 237.32 examples/s]Running tokenizer on dataset (num_proc=64): 193688 examples [00:16, 694.49 examples/s]Running tokenizer on dataset (num_proc=64): 198688 examples [00:16, 1417.08 examples/s]Running tokenizer on dataset (num_proc=64): 202688 examples [00:17, 2078.78 examples/s]Running tokenizer on dataset (num_proc=64): 203688 examples [00:17, 2074.64 examples/s]Running tokenizer on dataset (num_proc=64): 205688 examples [00:18, 2044.17 examples/s]Running tokenizer on dataset (num_proc=64): 206688 examples [00:19, 2024.97 examples/s]Running tokenizer on dataset (num_proc=64): 208688 examples [00:20, 2009.83 examples/s]Running tokenizer on dataset (num_proc=64): 211688 examples [00:20, 2723.54 examples/s]Running tokeniz on dataset (num_proc=64): 204688 examples [00:19, 2654.67 examples/s]Running tokenizer on dataset (num_proc=64): 205688 examples [00:20, 2417.21 examples/s]Running tokenizer on dataset (num_proc=64): 207688 examples [00:20, 2565.74 examples/s]Running tokenizer on dataset (num_proc=64): 208688 examples [00:21, 2152.81 examples/s]Running tokenizer on dataset (num_proc=64): 210688 examples [00:21, 3198.59 examples/s]Running tokenizer on dataset (num_proc=64): 211688 examples [00:22, 2531.68 examples/s]Running tokenizer on dataset (num_proc=64): 212688 examples [00:23, 2243.17 examples/s]Running tokenizer on dataset (num_proc=64): 213688 examples [00:24, 1462.10 examples/s]Running tokenizer on dataset (num_proc=64): 214688 examples [00:24, 1654.21 examples/s]Running tokenizer on dataset (num_proc=64): 215688 examples [00:25, 2016.40 examples/s]Running tokenizer on dataset (num_proc=64): 217688 examples [00:25, 2956.66 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:25, 3096. on dataset (num_proc=64): 205688 examples [00:18, 2813.97 examples/s]Running tokenizer on dataset (num_proc=64): 206688 examples [00:19, 2644.54 examples/s]Running tokenizer on dataset (num_proc=64): 207688 examples [00:19, 2356.85 examples/s]Running tokenizer on dataset (num_proc=64): 208688 examples [00:20, 2349.04 examples/s]Running tokenizer on dataset (num_proc=64): 209688 examples [00:20, 2197.07 examples/s]Running tokenizer on dataset (num_proc=64): 213688 examples [00:20, 4775.44 examples/s]Running tokenizer on dataset (num_proc=64): 215688 examples [00:21, 5102.40 examples/s]Running tokenizer on dataset (num_proc=64): 216688 examples [00:21, 3673.71 examples/s]Running tokenizer on dataset (num_proc=64): 217688 examples [00:22, 2161.02 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:23, 2233.11 examples/s]Running tokenizer on dataset (num_proc=64): 219688 examples [00:23, 2370.26 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:25, 1146. on dataset (num_proc=64): 205688 examples [00:19, 2593.86 examples/s]Running tokenizer on dataset (num_proc=64): 206688 examples [00:20, 2472.18 examples/s]Running tokenizer on dataset (num_proc=64): 209688 examples [00:20, 3366.96 examples/s]Running tokenizer on dataset (num_proc=64): 211688 examples [00:21, 3504.42 examples/s]Running tokenizer on dataset (num_proc=64): 215688 examples [00:22, 3160.29 examples/s]Running tokenizer on dataset (num_proc=64): 216688 examples [00:23, 2717.19 examples/s]Running tokenizer on dataset (num_proc=64): 217688 examples [00:23, 2777.09 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:25, 1506.99 examples/s]Running tokenizer on dataset (num_proc=64): 219688 examples [00:26, 1427.25 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:26, 1770.13 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:26, 1959.89 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:27, 2348.r on dataset (num_proc=64): 210688 examples [00:20, 3131.87 examples/s]Running tokenizer on dataset (num_proc=64): 211688 examples [00:21, 2900.81 examples/s]Running tokenizer on dataset (num_proc=64): 212688 examples [00:21, 2649.86 examples/s]Running tokenizer on dataset (num_proc=64): 215688 examples [00:22, 3519.17 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:22, 4112.95 examples/s]Running tokenizer on dataset (num_proc=64): 219688 examples [00:23, 3530.35 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:24, 1952.00 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:25, 1671.81 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:26, 1779.32 examples/s]Running tokenizer on dataset (num_proc=64): 223688 examples [00:27, 1156.58 examples/s]Running tokenizer on dataset (num_proc=64): 224688 examples [00:28, 1417.23 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:29, 1274r on dataset (num_proc=64): 208688 examples [00:21, 2063.89 examples/s]Running tokenizer on dataset (num_proc=64): 212688 examples [00:21, 3129.20 examples/s]Running tokenizer on dataset (num_proc=64): 213688 examples [00:22, 2825.52 examples/s]Running tokenizer on dataset (num_proc=64): 217688 examples [00:22, 3905.53 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:23, 3306.70 examples/s]Running tokenizer on dataset (num_proc=64): 219688 examples [00:23, 2803.94 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:24, 2654.49 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:24, 2394.56 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:26, 1252.82 examples/s]Running tokenizer on dataset (num_proc=64): 223688 examples [00:27, 1232.33 examples/s]Running tokenizer on dataset (num_proc=64): 224688 examples [00:28, 1403.45 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:29, 1313r on dataset (num_proc=64): 209688 examples [00:21, 2450.65 examples/s]Running tokenizer on dataset (num_proc=64): 211688 examples [00:21, 2737.65 examples/s]Running tokenizer on dataset (num_proc=64): 213688 examples [00:21, 3287.39 examples/s]Running tokenizer on dataset (num_proc=64): 215688 examples [00:22, 3989.96 examples/s]Running tokenizer on dataset (num_proc=64): 217688 examples [00:22, 4681.14 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:22, 5771.72 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:23, 4954.13 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:27, 1108.03 examples/s]Running tokenizer on dataset (num_proc=64): 223688 examples [00:27, 1263.84 examples/s]Running tokenizer on dataset (num_proc=64): 224688 examples [00:28, 1339.67 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:29, 1279.24 examples/s]Running tokenizer on dataset (num_proc=64): 226688 examples [00:29, 1457er on dataset (num_proc=64): 215688 examples [00:21, 3850.24 examples/s]Running tokenizer on dataset (num_proc=64): 216688 examples [00:22, 2776.58 examples/s]Running tokenizer on dataset (num_proc=64): 217688 examples [00:22, 3058.61 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:22, 2937.96 examples/s]Running tokenizer on dataset (num_proc=64): 219688 examples [00:23, 3301.38 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:24, 1720.06 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:24, 1869.41 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:25, 2057.37 examples/s]Running tokenizer on dataset (num_proc=64): 223688 examples [00:28, 912.86 examples/s] Running tokenizer on dataset (num_proc=64): 224688 examples [00:28, 1181.97 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:29, 928.10 examples/s] Running tokenizer on dataset (num_proc=64): 226688 examples [00:30, 10948 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:26, 1171.97 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:26, 1490.18 examples/s]Running tokenizer on dataset (num_proc=64): 223688 examples [00:27, 1626.51 examples/s]Running tokenizer on dataset (num_proc=64): 224688 examples [00:27, 1890.74 examples/s]Running tokenizer on dataset (num_proc=64): 226688 examples [00:27, 3106.74 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:29, 1652.59 examples/s]Running tokenizer on dataset (num_proc=64): 228688 examples [00:29, 1881.48 examples/s]Running tokenizer on dataset (num_proc=64): 229688 examples [00:30, 1646.07 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:30, 2643.35 examples/s]Running tokenizer on dataset (num_proc=64): 232688 examples [00:31, 2159.23 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:31, 2630.27 examples/s]Running tokenizer on dataset (num_proc=64r on dataset (num_proc=64): 214688 examples [00:21, 3452.95 examples/s]Running tokenizer on dataset (num_proc=64): 216688 examples [00:21, 3441.21 examples/s]Running tokenizer on dataset (num_proc=64): 217688 examples [00:22, 3193.70 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:23, 2681.30 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:23, 2689.78 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:25, 1398.60 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:29, 740.26 examples/s] Running tokenizer on dataset (num_proc=64): 224688 examples [00:29, 1127.02 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:30, 1064.38 examples/s]Running tokenizer on dataset (num_proc=64): 226688 examples [00:31, 1280.00 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:31, 1576.25 examples/s]Running tokenizer on dataset (num_proc=64): 228688 examples [00:31, 158750 examples/s]Running tokenizer on dataset (num_proc=64): 219688 examples [00:26, 1937.92 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:27, 2274.77 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:27, 2164.03 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:27, 2374.61 examples/s]Running tokenizer on dataset (num_proc=64): 223688 examples [00:28, 2391.18 examples/s]Running tokenizer on dataset (num_proc=64): 224688 examples [00:29, 1683.17 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:29, 1787.12 examples/s]Running tokenizer on dataset (num_proc=64): 226688 examples [00:30, 2048.74 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:30, 1953.73 examples/s]Running tokenizer on dataset (num_proc=64): 228688 examples [00:31, 1590.27 examples/s]Running tokenizer on dataset (num_proc=64): 229688 examples [00:32, 1539.96 examples/s]Running tokenizer on dataset (num_proc=64.30 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:30, 1211.35 examples/s]Running tokenizer on dataset (num_proc=64): 228688 examples [00:31, 1369.91 examples/s]Running tokenizer on dataset (num_proc=64): 229688 examples [00:31, 1632.13 examples/s]Running tokenizer on dataset (num_proc=64): 230688 examples [00:32, 1519.90 examples/s]Running tokenizer on dataset (num_proc=64): 232688 examples [00:32, 2563.25 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:33, 2233.81 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:33, 2382.79 examples/s]Running tokenizer on dataset (num_proc=64): 235688 examples [00:33, 2726.96 examples/s]Running tokenizer on dataset (num_proc=64): 236688 examples [00:33, 2716.86 examples/s]Running tokenizer on dataset (num_proc=64): 238688 examples [00:34, 2603.79 examples/s]Running tokenizer on dataset (num_proc=64): 239688 examples [00:35, 2362.81 examples/s]Running tokenizer on dataset (num_proc=6.45 examples/s]Running tokenizer on dataset (num_proc=64): 226688 examples [00:29, 1640.71 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:30, 1500.05 examples/s]Running tokenizer on dataset (num_proc=64): 228688 examples [00:32, 904.14 examples/s] Running tokenizer on dataset (num_proc=64): 229688 examples [00:32, 1058.37 examples/s]Running tokenizer on dataset (num_proc=64): 230688 examples [00:33, 1231.72 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:33, 1429.61 examples/s]Running tokenizer on dataset (num_proc=64): 232688 examples [00:34, 1775.79 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:34, 2064.79 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:34, 2159.04 examples/s]Running tokenizer on dataset (num_proc=64): 235688 examples [00:35, 2186.10 examples/s]Running tokenizer on dataset (num_proc=64): 236688 examples [00:35, 2110.80 examples/s]Running tokenizer on dataset (num_proc=613 examples/s]Running tokenizer on dataset (num_proc=64): 224688 examples [00:28, 1709.91 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:29, 1720.79 examples/s]Running tokenizer on dataset (num_proc=64): 226688 examples [00:29, 1998.07 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:30, 1417.09 examples/s]Running tokenizer on dataset (num_proc=64): 228688 examples [00:31, 1508.97 examples/s]Running tokenizer on dataset (num_proc=64): 229688 examples [00:31, 1835.07 examples/s]Running tokenizer on dataset (num_proc=64): 230688 examples [00:31, 2064.66 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:33, 1320.26 examples/s]Running tokenizer on dataset (num_proc=64): 232688 examples [00:33, 1418.60 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:35, 1148.83 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:36, 1139.31 examples/s]Running tokenizer on dataset (num_proc=641.59 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:30, 1280.11 examples/s]Running tokenizer on dataset (num_proc=64): 229688 examples [00:31, 2164.49 examples/s]Running tokenizer on dataset (num_proc=64): 230688 examples [00:33, 1204.10 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:33, 1316.30 examples/s]Running tokenizer on dataset (num_proc=64): 232688 examples [00:33, 1587.43 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:33, 2037.25 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:34, 2546.83 examples/s]Running tokenizer on dataset (num_proc=64): 235688 examples [00:35, 1769.95 examples/s]Running tokenizer on dataset (num_proc=64): 236688 examples [00:35, 1982.49 examples/s]Running tokenizer on dataset (num_proc=64): 237688 examples [00:35, 2132.93 examples/s]Running tokenizer on dataset (num_proc=64): 238688 examples [00:37, 1462.10 examples/s]Running tokenizer on dataset (num_proc=.50 examples/s]Running tokenizer on dataset (num_proc=64): 226688 examples [00:30, 1080.70 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:32, 784.54 examples/s] Running tokenizer on dataset (num_proc=64): 229688 examples [00:33, 1150.46 examples/s]Running tokenizer on dataset (num_proc=64): 230688 examples [00:34, 1173.87 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:34, 1396.42 examples/s]Running tokenizer on dataset (num_proc=64): 232688 examples [00:34, 1601.55 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:35, 1519.19 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:35, 1887.83 examples/s]Running tokenizer on dataset (num_proc=64): 235688 examples [00:35, 2444.73 examples/s]Running tokenizer on dataset (num_proc=64): 236688 examples [00:36, 1971.01 examples/s]Running tokenizer on dataset (num_proc=64): 237688 examples [00:37, 2017.60 examples/s]Running tokenizer on dataset (num_proc=6): 234688 examples [00:31, 2409.65 examples/s]Running tokenizer on dataset (num_proc=64): 237688 examples [00:32, 4088.41 examples/s]Running tokenizer on dataset (num_proc=64): 238688 examples [00:32, 3081.79 examples/s]Running tokenizer on dataset (num_proc=64): 239688 examples [00:33, 3364.07 examples/s]Running tokenizer on dataset (num_proc=64): 240688 examples [00:34, 2098.74 examples/s]Running tokenizer on dataset (num_proc=64): 242688 examples [00:34, 3261.33 examples/s]Running tokenizer on dataset (num_proc=64): 243688 examples [00:34, 3809.32 examples/s]Running tokenizer on dataset (num_proc=64): 245688 examples [00:34, 4807.83 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:34, 5170.56 examples/s]Running tokenizer on dataset (num_proc=64): 247688 examples [00:35, 4295.21 examples/s]Running tokenizer on dataset (num_proc=64): 248688 examples [00:35, 2705.26 examples/s]Running tokenizer on dataset (num_proc=64): 249688 examples [00:37, 1366.92 examples/s]Running t): 230688 examples [00:32, 1925.29 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:33, 1639.65 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:33, 2797.52 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:33, 2535.01 examples/s]Running tokenizer on dataset (num_proc=64): 235688 examples [00:34, 2192.92 examples/s]Running tokenizer on dataset (num_proc=64): 237688 examples [00:34, 2826.72 examples/s]Running tokenizer on dataset (num_proc=64): 239688 examples [00:35, 3301.29 examples/s]Running tokenizer on dataset (num_proc=64): 240688 examples [00:36, 2394.16 examples/s]Running tokenizer on dataset (num_proc=64): 241688 examples [00:36, 2792.87 examples/s]Running tokenizer on dataset (num_proc=64): 242688 examples [00:38, 1479.63 examples/s]Running tokenizer on dataset (num_proc=64): 243688 examples [00:38, 1835.16 examples/s]Running tokenizer on dataset (num_proc=64): 244688 examples [00:38, 2037.24 examples/s]Running t.65 examples/s]Running tokenizer on dataset (num_proc=64): 229688 examples [00:32, 2038.50 examples/s]Running tokenizer on dataset (num_proc=64): 230688 examples [00:33, 1519.76 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:33, 1692.00 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:34, 2301.55 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:34, 2703.49 examples/s]Running tokenizer on dataset (num_proc=64): 235688 examples [00:35, 1729.03 examples/s]Running tokenizer on dataset (num_proc=64): 237688 examples [00:35, 2662.90 examples/s]Running tokenizer on dataset (num_proc=64): 238688 examples [00:36, 2247.02 examples/s]Running tokenizer on dataset (num_proc=64): 239688 examples [00:37, 1756.17 examples/s]Running tokenizer on dataset (num_proc=64): 240688 examples [00:38, 1318.38 examples/s]Running tokenizer on dataset (num_proc=64): 241688 examples [00:39, 1467.40 examples/s]Running tokenizer on dataset (num_proc=6): 235688 examples [00:36, 1375.63 examples/s]Running tokenizer on dataset (num_proc=64): 237688 examples [00:36, 2389.73 examples/s]Running tokenizer on dataset (num_proc=64): 238688 examples [00:37, 2092.25 examples/s]Running tokenizer on dataset (num_proc=64): 239688 examples [00:37, 2083.40 examples/s]Running tokenizer on dataset (num_proc=64): 240688 examples [00:38, 2078.67 examples/s]Running tokenizer on dataset (num_proc=64): 241688 examples [00:38, 1867.76 examples/s]Running tokenizer on dataset (num_proc=64): 243688 examples [00:39, 2489.11 examples/s]Running tokenizer on dataset (num_proc=64): 245688 examples [00:39, 3635.40 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:39, 3992.91 examples/s]Running tokenizer on dataset (num_proc=64): 247688 examples [00:39, 4113.15 examples/s]Running tokenizer on dataset (num_proc=64): 250688 examples [00:40, 6528.87 examples/s]Running tokenizer on dataset (num_proc=64): 251688 examples [00:40, 4834.60 examples/s]Running t4): 237688 examples [00:36, 2160.03 examples/s]Running tokenizer on dataset (num_proc=64): 239688 examples [00:37, 1605.78 examples/s]Running tokenizer on dataset (num_proc=64): 240688 examples [00:38, 1785.70 examples/s]Running tokenizer on dataset (num_proc=64): 242688 examples [00:38, 2570.00 examples/s]Running tokenizer on dataset (num_proc=64): 243688 examples [00:38, 2433.55 examples/s]Running tokenizer on dataset (num_proc=64): 245688 examples [00:39, 3619.64 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:39, 3675.79 examples/s]Running tokenizer on dataset (num_proc=64): 247688 examples [00:39, 4054.98 examples/s]Running tokenizer on dataset (num_proc=64): 248688 examples [00:40, 2830.49 examples/s]Running tokenizer on dataset (num_proc=64): 249688 examples [00:40, 3440.87 examples/s]Running tokenizer on dataset (num_proc=64): 250605 examples [00:40, 3964.00 examples/s]Running tokenizer on dataset (num_proc=64): 251605 examples [00:40, 4149.08 examples/s]Running 64): 239688 examples [00:37, 1758.89 examples/s]Running tokenizer on dataset (num_proc=64): 240688 examples [00:37, 1843.43 examples/s]Running tokenizer on dataset (num_proc=64): 241688 examples [00:38, 2153.32 examples/s]Running tokenizer on dataset (num_proc=64): 244688 examples [00:38, 2716.48 examples/s]Running tokenizer on dataset (num_proc=64): 245688 examples [00:39, 2418.36 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:40, 1802.23 examples/s]Running tokenizer on dataset (num_proc=64): 247688 examples [00:40, 2071.87 examples/s]Running tokenizer on dataset (num_proc=64): 249688 examples [00:41, 3042.77 examples/s]Running tokenizer on dataset (num_proc=64): 250688 examples [00:41, 3138.13 examples/s]Running tokenizer on dataset (num_proc=64): 251688 examples [00:41, 3431.72 examples/s]Running tokenizer on dataset (num_proc=64): 253688 examples [00:41, 5067.48 examples/s]Running tokenizer on dataset (num_proc=64): 255688 examples [00:41, 5694.16 examples/s]Runningokenizer on dataset (num_proc=64): 251688 examples [00:37, 2108.84 examples/s]Running tokenizer on dataset (num_proc=64): 252688 examples [00:38, 2371.25 examples/s]Running tokenizer on dataset (num_proc=64): 254688 examples [00:38, 3404.38 examples/s]Running tokenizer on dataset (num_proc=64): 255688 examples [00:38, 3401.64 examples/s]Running tokenizer on dataset (num_proc=64): 256688 examples [00:38, 3381.62 examples/s]Running tokenizer on dataset (num_proc=64): 257688 examples [00:39, 4064.82 examples/s]Running tokenizer on dataset (num_proc=64): 258688 examples [00:39, 2689.13 examples/s]Running tokenizer on dataset (num_proc=64): 259688 examples [00:40, 2109.34 examples/s]Running tokenizer on dataset (num_proc=64): 260688 examples [00:41, 1735.87 examples/s]Running tokenizer on dataset (num_proc=64): 261688 examples [00:41, 1837.02 examples/s]Running tokenizer on dataset (num_proc=64): 262688 examples [00:41, 2267.04 examples/s]Running tokenizer on dataset (num_proc=64): 265605 examples [00:44): 242688 examples [00:39, 1470.96 examples/s]Running tokenizer on dataset (num_proc=64): 244688 examples [00:39, 2460.55 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:39, 3700.96 examples/s]Running tokenizer on dataset (num_proc=64): 248688 examples [00:40, 4361.47 examples/s]Running tokenizer on dataset (num_proc=64): 249688 examples [00:40, 4885.18 examples/s]Running tokenizer on dataset (num_proc=64): 250688 examples [00:40, 4507.26 examples/s]Running tokenizer on dataset (num_proc=64): 251688 examples [00:40, 4305.92 examples/s]Running tokenizer on dataset (num_proc=64): 252688 examples [00:41, 3107.46 examples/s]Running tokenizer on dataset (num_proc=64): 254688 examples [00:41, 4651.68 examples/s]Running tokenizer on dataset (num_proc=64): 255688 examples [00:41, 4584.55 examples/s]Running tokenizer on dataset (num_proc=64): 256688 examples [00:41, 5200.00 examples/s]Running tokenizer on dataset (num_proc=64): 257688 examples [00:42, 4747.59 examples/s]Running 4): 240688 examples [00:36, 1877.66 examples/s]Running tokenizer on dataset (num_proc=64): 241688 examples [00:37, 1209.78 examples/s]Running tokenizer on dataset (num_proc=64): 243688 examples [00:38, 1698.27 examples/s]Running tokenizer on dataset (num_proc=64): 244688 examples [00:38, 1795.33 examples/s]Running tokenizer on dataset (num_proc=64): 245688 examples [00:39, 1594.76 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:39, 1962.49 examples/s]Running tokenizer on dataset (num_proc=64): 248688 examples [00:40, 2874.98 examples/s]Running tokenizer on dataset (num_proc=64): 249688 examples [00:40, 3192.87 examples/s]Running tokenizer on dataset (num_proc=64): 250688 examples [00:40, 3261.43 examples/s]Running tokenizer on dataset (num_proc=64): 252688 examples [00:40, 4985.08 examples/s]Running tokenizer on dataset (num_proc=64): 255688 examples [00:40, 7395.82 examples/s]Running tokenizer on dataset (num_proc=64): 257688 examples [00:41, 4298.68 examples/s]Running 4): 239688 examples [00:37, 2857.84 examples/s]Running tokenizer on dataset (num_proc=64): 241688 examples [00:38, 3092.15 examples/s]Running tokenizer on dataset (num_proc=64): 243688 examples [00:39, 1821.74 examples/s]Running tokenizer on dataset (num_proc=64): 244688 examples [00:40, 2015.65 examples/s]Running tokenizer on dataset (num_proc=64): 245688 examples [00:40, 2232.72 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:40, 2387.90 examples/s]Running tokenizer on dataset (num_proc=64): 247688 examples [00:41, 2701.15 examples/s]Running tokenizer on dataset (num_proc=64): 250688 examples [00:41, 4622.05 examples/s]Running tokenizer on dataset (num_proc=64): 252688 examples [00:41, 5871.05 examples/s]Running tokenizer on dataset (num_proc=64): 253688 examples [00:41, 4242.85 examples/s]Running tokenizer on dataset (num_proc=64): 255688 examples [00:42, 4997.76 examples/s]Running tokenizer on dataset (num_proc=64): 256688 examples [00:42, 3409.63 examples/s]Running okenizer on dataset (num_proc=64): 245688 examples [00:38, 2195.11 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:39, 2051.38 examples/s]Running tokenizer on dataset (num_proc=64): 247688 examples [00:39, 2488.01 examples/s]Running tokenizer on dataset (num_proc=64): 248688 examples [00:39, 2958.87 examples/s]Running tokenizer on dataset (num_proc=64): 249688 examples [00:40, 3382.28 examples/s]Running tokenizer on dataset (num_proc=64): 250688 examples [00:40, 3763.00 examples/s]Running tokenizer on dataset (num_proc=64): 251688 examples [00:41, 2245.70 examples/s]Running tokenizer on dataset (num_proc=64): 252688 examples [00:41, 2435.24 examples/s]Running tokenizer on dataset (num_proc=64): 253688 examples [00:41, 2265.90 examples/s]Running tokenizer on dataset (num_proc=64): 255688 examples [00:42, 3594.87 examples/s]Running tokenizer on dataset (num_proc=64): 256688 examples [00:42, 4143.48 examples/s]Running tokenizer on dataset (num_proc=64): 257688 examples [00:4okenizer on dataset (num_proc=64): 252688 examples [00:40, 4859.78 examples/s]Running tokenizer on dataset (num_proc=64): 255688 examples [00:41, 6137.78 examples/s]Running tokenizer on dataset (num_proc=64): 256688 examples [00:41, 6612.47 examples/s]Running tokenizer on dataset (num_proc=64): 257688 examples [00:41, 4695.58 examples/s]Running tokenizer on dataset (num_proc=64): 258688 examples [00:41, 5339.60 examples/s]Running tokenizer on dataset (num_proc=64): 259688 examples [00:41, 5879.41 examples/s]Running tokenizer on dataset (num_proc=64): 260688 examples [00:42, 3757.10 examples/s]Running tokenizer on dataset (num_proc=64): 261688 examples [00:42, 2899.96 examples/s]Running tokenizer on dataset (num_proc=64): 262688 examples [00:43, 2698.48 examples/s]Running tokenizer on dataset (num_proc=64): 263688 examples [00:43, 2914.38 examples/s]Running tokenizer on dataset (num_proc=64): 264605 examples [00:43, 3406.94 examples/s]Running tokenizer on dataset (num_proc=64): 266605 examples [00:4tokenizer on dataset (num_proc=64): 252605 examples [00:40, 4880.65 examples/s]Running tokenizer on dataset (num_proc=64): 254605 examples [00:41, 3870.52 examples/s]Running tokenizer on dataset (num_proc=64): 255605 examples [00:42, 2205.78 examples/s]Running tokenizer on dataset (num_proc=64): 256605 examples [00:42, 2247.65 examples/s]Running tokenizer on dataset (num_proc=64): 258605 examples [00:43, 3504.00 examples/s]Running tokenizer on dataset (num_proc=64): 259605 examples [00:43, 3920.30 examples/s]Running tokenizer on dataset (num_proc=64): 260605 examples [00:43, 3754.72 examples/s]Running tokenizer on dataset (num_proc=64): 261605 examples [00:43, 3765.16 examples/s]Running tokenizer on dataset (num_proc=64): 262605 examples [00:44, 3452.13 examples/s]Running tokenizer on dataset (num_proc=64): 264605 examples [00:44, 4676.88 examples/s]Running tokenizer on dataset (num_proc=64): 267605 examples [00:44, 6388.88 examples/s]Running tokenizer on dataset (num_proc=64): 269522 examples [00:tokenizer on dataset (num_proc=64): 257688 examples [00:43, 3711.51 examples/s]Running tokenizer on dataset (num_proc=64): 258688 examples [00:43, 3668.65 examples/s]Running tokenizer on dataset (num_proc=64): 261688 examples [00:43, 6341.79 examples/s]Running tokenizer on dataset (num_proc=64): 262688 examples [00:43, 4424.05 examples/s]Running tokenizer on dataset (num_proc=64): 263688 examples [00:44, 3340.73 examples/s]Running tokenizer on dataset (num_proc=64): 264688 examples [00:44, 3358.18 examples/s]Running tokenizer on dataset (num_proc=64): 265688 examples [00:44, 3859.16 examples/s]Running tokenizer on dataset (num_proc=64): 266688 examples [00:45, 4549.88 examples/s]Running tokenizer on dataset (num_proc=64): 267688 examples [00:45, 4518.35 examples/s]Running tokenizer on dataset (num_proc=64): 268688 examples [00:46, 2243.11 examples/s]Running tokenizer on dataset (num_proc=64): 269688 examples [00:47, 1948.36 examples/s]Running tokenizer on dataset (num_proc=64): 271605 examples [00:3, 2102.46 examples/s]Running tokenizer on dataset (num_proc=64): 258688 examples [00:43, 2369.16 examples/s]Running tokenizer on dataset (num_proc=64): 259688 examples [00:44, 2236.35 examples/s]Running tokenizer on dataset (num_proc=64): 260688 examples [00:44, 2820.55 examples/s]Running tokenizer on dataset (num_proc=64): 261688 examples [00:44, 3289.87 examples/s]Running tokenizer on dataset (num_proc=64): 262688 examples [00:44, 3447.82 examples/s]Running tokenizer on dataset (num_proc=64): 263688 examples [00:45, 2705.05 examples/s]Running tokenizer on dataset (num_proc=64): 265688 examples [00:45, 3328.19 examples/s]Running tokenizer on dataset (num_proc=64): 266688 examples [00:45, 3671.53 examples/s]Running tokenizer on dataset (num_proc=64): 268688 examples [00:46, 3219.79 examples/s]Running tokenizer on dataset (num_proc=64): 269688 examples [00:46, 3319.79 examples/s]Running tokenizer on dataset (num_proc=64): 270688 examples [00:47, 3467.59 examples/s]Running tokenizer on dataset (numtokenizer on dataset (num_proc=64): 259688 examples [00:42, 6748.54 examples/s]Running tokenizer on dataset (num_proc=64): 260688 examples [00:42, 3949.71 examples/s]Running tokenizer on dataset (num_proc=64): 261688 examples [00:43, 4369.91 examples/s]Running tokenizer on dataset (num_proc=64): 262688 examples [00:44, 1882.42 examples/s]Running tokenizer on dataset (num_proc=64): 265605 examples [00:44, 3240.98 examples/s]Running tokenizer on dataset (num_proc=64): 266605 examples [00:45, 2649.41 examples/s]Running tokenizer on dataset (num_proc=64): 267605 examples [00:45, 2788.33 examples/s]Running tokenizer on dataset (num_proc=64): 268605 examples [00:46, 2491.29 examples/s]Running tokenizer on dataset (num_proc=64): 269605 examples [00:46, 2615.63 examples/s]Running tokenizer on dataset (num_proc=64): 270605 examples [00:46, 2660.90 examples/s]Running tokenizer on dataset (num_proc=64): 271605 examples [00:47, 3124.11 examples/s]Running tokenizer on dataset (num_proc=64): 272605 examples [00:tokenizer on dataset (num_proc=64): 258688 examples [00:42, 3189.39 examples/s]Running tokenizer on dataset (num_proc=64): 260688 examples [00:42, 3512.39 examples/s]Running tokenizer on dataset (num_proc=64): 262688 examples [00:43, 4068.15 examples/s]Running tokenizer on dataset (num_proc=64): 263688 examples [00:43, 4162.22 examples/s]Running tokenizer on dataset (num_proc=64): 264688 examples [00:44, 2686.43 examples/s]Running tokenizer on dataset (num_proc=64): 265688 examples [00:44, 2612.69 examples/s]Running tokenizer on dataset (num_proc=64): 267605 examples [00:45, 2031.60 examples/s]Running tokenizer on dataset (num_proc=64): 269522 examples [00:46, 2902.26 examples/s]Running tokenizer on dataset (num_proc=64): 271522 examples [00:46, 4127.36 examples/s]Running tokenizer on dataset (num_proc=64): 273522 examples [00:47, 3339.57 examples/s]Running tokenizer on dataset (num_proc=64): 274522 examples [00:48, 2239.38 examples/s]Running tokenizer on dataset (num_proc=64): 276522 examples [00: tokenizer on dataset (num_proc=64): 257688 examples [00:42, 7039.64 examples/s]Running tokenizer on dataset (num_proc=64): 259688 examples [00:42, 7352.47 examples/s]Running tokenizer on dataset (num_proc=64): 261688 examples [00:42, 8731.31 examples/s]Running tokenizer on dataset (num_proc=64): 263688 examples [00:43, 3382.62 examples/s]Running tokenizer on dataset (num_proc=64): 264688 examples [00:44, 2981.48 examples/s]Running tokenizer on dataset (num_proc=64): 266688 examples [00:45, 2452.25 examples/s]Running tokenizer on dataset (num_proc=64): 268688 examples [00:45, 3249.00 examples/s]Running tokenizer on dataset (num_proc=64): 270688 examples [00:45, 4110.29 examples/s]Running tokenizer on dataset (num_proc=64): 271688 examples [00:47, 2497.30 examples/s]Running tokenizer on dataset (num_proc=64): 272688 examples [00:47, 1935.05 examples/s]Running tokenizer on dataset (num_proc=64): 273605 examples [00:48, 1861.76 examples/s]Running tokenizer on dataset (num_proc=64): 274605 examples [002, 4030.10 examples/s]Running tokenizer on dataset (num_proc=64): 267605 examples [00:42, 5020.62 examples/s]Running tokenizer on dataset (num_proc=64): 268605 examples [00:42, 4201.57 examples/s]Running tokenizer on dataset (num_proc=64): 269605 examples [00:43, 3876.33 examples/s]Running tokenizer on dataset (num_proc=64): 270605 examples [00:43, 4453.91 examples/s]Running tokenizer on dataset (num_proc=64): 271605 examples [00:44, 1795.60 examples/s]Running tokenizer on dataset (num_proc=64): 273522 examples [00:45, 2086.50 examples/s]Running tokenizer on dataset (num_proc=64): 274439 examples [00:48, 954.93 examples/s] Running tokenizer on dataset (num_proc=64): 275356 examples [00:48, 1147.94 examples/s]Running tokenizer on dataset (num_proc=64): 276273 examples [00:48, 1346.51 examples/s]Running tokenizer on dataset (num_proc=64): 277273 examples [00:49, 1750.18 examples/s]Running tokenizer on dataset (num_proc=64): 278273 examples [00:49, 2111.16 examples/s]Running tokenizer on dataset (num4, 4171.52 examples/s]Running tokenizer on dataset (num_proc=64): 267605 examples [00:44, 3256.03 examples/s]Running tokenizer on dataset (num_proc=64): 268605 examples [00:46, 1644.10 examples/s]Running tokenizer on dataset (num_proc=64): 270522 examples [00:46, 2337.86 examples/s]Running tokenizer on dataset (num_proc=64): 272522 examples [00:46, 3316.06 examples/s]Running tokenizer on dataset (num_proc=64): 273522 examples [00:47, 2005.58 examples/s]Running tokenizer on dataset (num_proc=64): 274522 examples [00:48, 1695.18 examples/s]Running tokenizer on dataset (num_proc=64): 275439 examples [00:49, 1646.07 examples/s]Running tokenizer on dataset (num_proc=64): 276439 examples [00:49, 2055.37 examples/s]Running tokenizer on dataset (num_proc=64): 277439 examples [00:50, 1980.45 examples/s]Running tokenizer on dataset (num_proc=64): 278439 examples [00:50, 2345.33 examples/s]Running tokenizer on dataset (num_proc=64): 279356 examples [00:50, 2279.32 examples/s]Running tokenizer on dataset (num_proc=64): 280190 examples [00:49, 3018.99 examples/s]Running tokenizer on dataset (num_proc=64): 281190 examples [00:49, 3026.03 examples/s]Running tokenizer on dataset (num_proc=64): 282107 examples [00:50, 3614.90 examples/s]Running tokenizer on dataset (num_proc=64): 284107 examples [00:50, 5056.42 examples/s]Running tokenizer on dataset (num_proc=64): 286024 examples [00:50, 5139.26 examples/s]Running tokenizer on dataset (num_proc=64): 286941 examples [00:51, 2946.78 examples/s]Running tokenizer on dataset (num_proc=64): 287858 examples [00:51, 3228.05 examples/s]Running tokenizer on dataset (num_proc=64): 288775 examples [00:51, 3296.46 examples/s]Running tokenizer on dataset (num_proc=64): 290692 examples [00:52, 4027.35 examples/s]Running tokenizer on dataset (num_proc=64): 291692 examples [00:52, 4226.51 examples/s]Running tokenizer on dataset (num_proc=64): 292609 examples [00:52, 4133.97 examples/s]Running tokenizer on dataset (num_proc=64): 293609 examples [00:53, 2041.00 examples/s]R47, 3055.84 examples/s]Running tokenizer on dataset (num_proc=64): 273605 examples [00:47, 3649.92 examples/s]Running tokenizer on dataset (num_proc=64): 274522 examples [00:48, 3020.35 examples/s]Running tokenizer on dataset (num_proc=64): 275522 examples [00:49, 1952.97 examples/s]Running tokenizer on dataset (num_proc=64): 277522 examples [00:51, 1309.99 examples/s]Running tokenizer on dataset (num_proc=64): 278439 examples [00:51, 1274.73 examples/s]Running tokenizer on dataset (num_proc=64): 279356 examples [00:52, 1462.70 examples/s]Running tokenizer on dataset (num_proc=64): 280273 examples [00:52, 1667.89 examples/s]Running tokenizer on dataset (num_proc=64): 281190 examples [00:52, 2145.98 examples/s]Running tokenizer on dataset (num_proc=64): 282107 examples [00:52, 2679.39 examples/s]Running tokenizer on dataset (num_proc=64): 283107 examples [00:53, 1890.06 examples/s]Running tokenizer on dataset (num_proc=64): 284024 examples [00:54, 1872.56 examples/s]Running tokenizer on dataset (nu47, 1891.33 examples/s]Running tokenizer on dataset (num_proc=64): 270439 examples [00:48, 1725.95 examples/s]Running tokenizer on dataset (num_proc=64): 272439 examples [00:48, 2158.05 examples/s]Running tokenizer on dataset (num_proc=64): 273439 examples [00:48, 2257.93 examples/s]Running tokenizer on dataset (num_proc=64): 275356 examples [00:49, 2610.01 examples/s]Running tokenizer on dataset (num_proc=64): 276356 examples [00:50, 2298.09 examples/s]Running tokenizer on dataset (num_proc=64): 278356 examples [00:50, 2856.11 examples/s]Running tokenizer on dataset (num_proc=64): 279356 examples [00:51, 1696.79 examples/s]Running tokenizer on dataset (num_proc=64): 281356 examples [00:52, 2548.90 examples/s]Running tokenizer on dataset (num_proc=64): 282356 examples [00:52, 2394.02 examples/s]Running tokenizer on dataset (num_proc=64): 283273 examples [00:52, 2499.16 examples/s]Running tokenizer on dataset (num_proc=64): 284273 examples [00:53, 1830.58 examples/s]Running tokenizer on dataset (nu_proc=64): 272688 examples [00:47, 5055.07 examples/s]Running tokenizer on dataset (num_proc=64): 274605 examples [00:47, 4904.80 examples/s]Running tokenizer on dataset (num_proc=64): 275605 examples [00:47, 5331.90 examples/s]Running tokenizer on dataset (num_proc=64): 276605 examples [00:48, 5315.16 examples/s]Running tokenizer on dataset (num_proc=64): 277522 examples [00:48, 3861.83 examples/s]Running tokenizer on dataset (num_proc=64): 278522 examples [00:48, 3840.17 examples/s]Running tokenizer on dataset (num_proc=64): 279522 examples [00:50, 1656.09 examples/s]Running tokenizer on dataset (num_proc=64): 280522 examples [00:50, 2027.36 examples/s]Running tokenizer on dataset (num_proc=64): 281439 examples [00:51, 1547.89 examples/s]Running tokenizer on dataset (num_proc=64): 284273 examples [00:52, 1995.07 examples/s]Running tokenizer on dataset (num_proc=64): 285190 examples [00:54, 1319.71 examples/s]Running tokenizer on dataset (num_proc=64): 287024 examples [00:54, 1776.44 examples/s]R47, 2801.43 examples/s]Running tokenizer on dataset (num_proc=64): 272605 examples [00:47, 3314.21 examples/s]Running tokenizer on dataset (num_proc=64): 273605 examples [00:47, 3480.89 examples/s]Running tokenizer on dataset (num_proc=64): 274605 examples [00:48, 2225.96 examples/s]Running tokenizer on dataset (num_proc=64): 275605 examples [00:48, 2783.18 examples/s]Running tokenizer on dataset (num_proc=64): 276605 examples [00:49, 2740.56 examples/s]Running tokenizer on dataset (num_proc=64): 277522 examples [00:51, 1162.60 examples/s]Running tokenizer on dataset (num_proc=64): 279439 examples [00:51, 1611.79 examples/s]Running tokenizer on dataset (num_proc=64): 280356 examples [00:52, 1313.45 examples/s]Running tokenizer on dataset (num_proc=64): 281273 examples [00:53, 1487.85 examples/s]Running tokenizer on dataset (num_proc=64): 282190 examples [00:54, 1085.83 examples/s]Running tokenizer on dataset (num_proc=64): 284107 examples [00:54, 1842.02 examples/s]Running tokenizer on dataset (nu:49, 1705.01 examples/s]Running tokenizer on dataset (num_proc=64): 276522 examples [00:50, 1921.65 examples/s]Running tokenizer on dataset (num_proc=64): 277522 examples [00:51, 1297.93 examples/s]Running tokenizer on dataset (num_proc=64): 278439 examples [00:51, 1589.49 examples/s]Running tokenizer on dataset (num_proc=64): 279356 examples [00:52, 1677.39 examples/s]Running tokenizer on dataset (num_proc=64): 280273 examples [00:52, 1810.91 examples/s]Running tokenizer on dataset (num_proc=64): 281190 examples [00:53, 1749.32 examples/s]Running tokenizer on dataset (num_proc=64): 282190 examples [00:53, 2018.31 examples/s]Running tokenizer on dataset (num_proc=64): 283190 examples [00:53, 2128.88 examples/s]Running tokenizer on dataset (num_proc=64): 284190 examples [00:54, 2507.08 examples/s]Running tokenizer on dataset (num_proc=64): 285107 examples [00:54, 2035.92 examples/s]Running tokenizer on dataset (num_proc=64): 286024 examples [00:55, 2243.91 examples/s]Running tokenizer on dataset (n48, 3118.62 examples/s]Running tokenizer on dataset (num_proc=64): 277439 examples [00:48, 2635.49 examples/s]Running tokenizer on dataset (num_proc=64): 278439 examples [00:49, 2363.50 examples/s]Running tokenizer on dataset (num_proc=64): 279439 examples [00:50, 1947.37 examples/s]Running tokenizer on dataset (num_proc=64): 281356 examples [00:51, 2011.57 examples/s]Running tokenizer on dataset (num_proc=64): 282356 examples [00:52, 1424.55 examples/s]Running tokenizer on dataset (num_proc=64): 283273 examples [00:53, 1269.60 examples/s]Running tokenizer on dataset (num_proc=64): 284273 examples [00:54, 1206.39 examples/s]Running tokenizer on dataset (num_proc=64): 286190 examples [00:54, 1863.38 examples/s]Running tokenizer on dataset (num_proc=64): 287107 examples [00:55, 1798.24 examples/s]Running tokenizer on dataset (num_proc=64): 288024 examples [00:55, 1731.71 examples/s]Running tokenizer on dataset (num_proc=64): 288941 examples [00:56, 1818.77 examples/s]Running tokenizer on dataset (nuunning tokenizer on dataset (num_proc=64): 294526 examples [00:54, 2153.94 examples/s]Running tokenizer on dataset (num_proc=64): 295443 examples [00:54, 2594.27 examples/s]Running tokenizer on dataset (num_proc=64): 296443 examples [00:54, 3199.39 examples/s]Running tokenizer on dataset (num_proc=64): 298277 examples [00:54, 4919.13 examples/s]Running tokenizer on dataset (num_proc=64): 301194 examples [00:54, 7385.15 examples/s]Running tokenizer on dataset (num_proc=64): 303028 examples [00:54, 8245.45 examples/s]Running tokenizer on dataset (num_proc=64): 305028 examples [00:55, 8041.83 examples/s]Running tokenizer on dataset (num_proc=64): 306028 examples [00:55, 6847.90 examples/s]Running tokenizer on dataset (num_proc=64): 307028 examples [00:56, 2605.25 examples/s]Running tokenizer on dataset (num_proc=64): 307945 examples [00:56, 2874.83 examples/s]Running tokenizer on dataset (num_proc=64): 308862 examples [00:57, 3240.93 examples/s]Running tokenizer on dataset (num_proc=64): 310696 exampl_proc=64): 280273 examples [00:51, 1906.61 examples/s]Running tokenizer on dataset (num_proc=64): 281273 examples [00:52, 1756.32 examples/s]Running tokenizer on dataset (num_proc=64): 284024 examples [00:53, 1749.50 examples/s]Running tokenizer on dataset (num_proc=64): 285941 examples [00:53, 2498.90 examples/s]Running tokenizer on dataset (num_proc=64): 286941 examples [00:54, 2718.05 examples/s]Running tokenizer on dataset (num_proc=64): 288858 examples [00:54, 3139.00 examples/s]Running tokenizer on dataset (num_proc=64): 289858 examples [00:54, 3504.23 examples/s]Running tokenizer on dataset (num_proc=64): 291775 examples [00:56, 2068.78 examples/s]Running tokenizer on dataset (num_proc=64): 292775 examples [00:56, 2137.90 examples/s]Running tokenizer on dataset (num_proc=64): 293775 examples [00:56, 2588.42 examples/s]Running tokenizer on dataset (num_proc=64): 294692 examples [00:56, 3017.55 examples/s]Running tokenizer on dataset (num_proc=64): 295692 examples [00:57, 2417.33 examples/s]Rm_proc=64): 284941 examples [00:54, 2387.59 examples/s]Running tokenizer on dataset (num_proc=64): 286858 examples [00:54, 3485.55 examples/s]Running tokenizer on dataset (num_proc=64): 287775 examples [00:54, 3589.95 examples/s]Running tokenizer on dataset (num_proc=64): 288692 examples [00:54, 3890.77 examples/s]Running tokenizer on dataset (num_proc=64): 289692 examples [00:55, 2894.38 examples/s]Running tokenizer on dataset (num_proc=64): 290609 examples [00:56, 1928.66 examples/s]Running tokenizer on dataset (num_proc=64): 292526 examples [00:56, 3025.23 examples/s]Running tokenizer on dataset (num_proc=64): 293443 examples [00:56, 3011.47 examples/s]Running tokenizer on dataset (num_proc=64): 294443 examples [00:57, 3358.13 examples/s]Running tokenizer on dataset (num_proc=64): 295443 examples [00:57, 3011.96 examples/s]Running tokenizer on dataset (num_proc=64): 296443 examples [00:58, 2327.14 examples/s]Running tokenizer on dataset (num_proc=64): 297360 examples [00:58, 2291.35 examples/s]m_proc=64): 286107 examples [00:54, 2064.81 examples/s]Running tokenizer on dataset (num_proc=64): 287024 examples [00:54, 2223.69 examples/s]Running tokenizer on dataset (num_proc=64): 288858 examples [00:55, 2606.55 examples/s]Running tokenizer on dataset (num_proc=64): 289858 examples [00:55, 3069.18 examples/s]Running tokenizer on dataset (num_proc=64): 290858 examples [00:55, 2741.20 examples/s]Running tokenizer on dataset (num_proc=64): 291775 examples [00:56, 2681.86 examples/s]Running tokenizer on dataset (num_proc=64): 293609 examples [00:56, 3649.93 examples/s]Running tokenizer on dataset (num_proc=64): 294526 examples [00:56, 3868.93 examples/s]Running tokenizer on dataset (num_proc=64): 296443 examples [00:57, 2515.58 examples/s]Running tokenizer on dataset (num_proc=64): 297443 examples [00:59, 1753.92 examples/s]Running tokenizer on dataset (num_proc=64): 298360 examples [00:59, 2008.17 examples/s]Running tokenizer on dataset (num_proc=64): 299277 examples [00:59, 2427.03 examples/s]m_proc=64): 285107 examples [00:55, 1835.03 examples/s]Running tokenizer on dataset (num_proc=64): 286024 examples [00:55, 2266.30 examples/s]Running tokenizer on dataset (num_proc=64): 286941 examples [00:56, 1560.40 examples/s]Running tokenizer on dataset (num_proc=64): 287858 examples [00:57, 1421.61 examples/s]Running tokenizer on dataset (num_proc=64): 288775 examples [00:57, 1844.31 examples/s]Running tokenizer on dataset (num_proc=64): 289692 examples [00:57, 2324.16 examples/s]Running tokenizer on dataset (num_proc=64): 290609 examples [00:57, 2670.25 examples/s]Running tokenizer on dataset (num_proc=64): 292526 examples [00:58, 3616.96 examples/s]Running tokenizer on dataset (num_proc=64): 293443 examples [00:58, 3086.50 examples/s]Running tokenizer on dataset (num_proc=64): 294443 examples [00:58, 3263.59 examples/s]Running tokenizer on dataset (num_proc=64): 295443 examples [00:59, 3868.33 examples/s]Running tokenizer on dataset (num_proc=64): 297443 examples [00:59, 5955.86 examples/s]um_proc=64): 287941 examples [00:55, 2739.08 examples/s]Running tokenizer on dataset (num_proc=64): 288858 examples [00:56, 2777.21 examples/s]Running tokenizer on dataset (num_proc=64): 289775 examples [00:56, 2455.44 examples/s]Running tokenizer on dataset (num_proc=64): 290775 examples [00:57, 2258.76 examples/s]Running tokenizer on dataset (num_proc=64): 291775 examples [00:57, 2801.83 examples/s]Running tokenizer on dataset (num_proc=64): 292775 examples [00:57, 2194.50 examples/s]Running tokenizer on dataset (num_proc=64): 293692 examples [00:58, 2728.00 examples/s]Running tokenizer on dataset (num_proc=64): 294609 examples [00:58, 2592.47 examples/s]Running tokenizer on dataset (num_proc=64): 296443 examples [00:58, 2934.23 examples/s]Running tokenizer on dataset (num_proc=64): 297360 examples [00:59, 1975.77 examples/s]Running tokenizer on dataset (num_proc=64): 298360 examples [01:00, 2415.25 examples/s]Running tokenizer on dataset (num_proc=64): 299360 examples [01:00, 2417.47 examples/s]unning tokenizer on dataset (num_proc=64): 287941 examples [00:55, 1756.49 examples/s]Running tokenizer on dataset (num_proc=64): 288941 examples [00:55, 2028.01 examples/s]Running tokenizer on dataset (num_proc=64): 289858 examples [00:55, 2236.63 examples/s]Running tokenizer on dataset (num_proc=64): 290775 examples [00:55, 2628.33 examples/s]Running tokenizer on dataset (num_proc=64): 291692 examples [00:56, 2550.04 examples/s]Running tokenizer on dataset (num_proc=64): 292692 examples [00:57, 1931.56 examples/s]Running tokenizer on dataset (num_proc=64): 294609 examples [00:57, 2679.60 examples/s]Running tokenizer on dataset (num_proc=64): 296526 examples [00:57, 3531.77 examples/s]Running tokenizer on dataset (num_proc=64): 297443 examples [00:58, 2845.72 examples/s]Running tokenizer on dataset (num_proc=64): 299277 examples [00:58, 3269.54 examples/s]Running tokenizer on dataset (num_proc=64): 302194 examples [00:59, 4104.09 examples/s]Running tokenizer on dataset (num_proc=64): 303194 examplunning tokenizer on dataset (num_proc=64): 296609 examples [00:57, 2974.55 examples/s]Running tokenizer on dataset (num_proc=64): 298443 examples [00:58, 3046.71 examples/s]Running tokenizer on dataset (num_proc=64): 299360 examples [00:58, 2482.89 examples/s]Running tokenizer on dataset (num_proc=64): 301194 examples [00:59, 2318.37 examples/s]Running tokenizer on dataset (num_proc=64): 302194 examples [00:59, 2791.65 examples/s]Running tokenizer on dataset (num_proc=64): 305028 examples [01:00, 3805.41 examples/s]Running tokenizer on dataset (num_proc=64): 305945 examples [01:00, 4126.80 examples/s]Running tokenizer on dataset (num_proc=64): 306862 examples [01:00, 4621.18 examples/s]Running tokenizer on dataset (num_proc=64): 307779 examples [01:00, 5133.95 examples/s]Running tokenizer on dataset (num_proc=64): 309613 examples [01:00, 5613.13 examples/s]Running tokenizer on dataset (num_proc=64): 310530 examples [01:01, 5795.67 examples/s]Running tokenizer on dataset (num_proc=64): 311447 examplm_proc=64): 290858 examples [00:56, 2525.43 examples/s]Running tokenizer on dataset (num_proc=64): 291775 examples [00:57, 1760.60 examples/s]Running tokenizer on dataset (num_proc=64): 293609 examples [00:58, 2463.74 examples/s]Running tokenizer on dataset (num_proc=64): 294526 examples [00:58, 2651.17 examples/s]Running tokenizer on dataset (num_proc=64): 296360 examples [00:58, 2910.07 examples/s]Running tokenizer on dataset (num_proc=64): 297277 examples [00:58, 3358.55 examples/s]Running tokenizer on dataset (num_proc=64): 298277 examples [00:59, 3559.54 examples/s]Running tokenizer on dataset (num_proc=64): 300111 examples [00:59, 5111.95 examples/s]Running tokenizer on dataset (num_proc=64): 302028 examples [00:59, 6742.76 examples/s]Running tokenizer on dataset (num_proc=64): 303945 examples [01:00, 5030.46 examples/s]Running tokenizer on dataset (num_proc=64): 304945 examples [01:00, 3248.12 examples/s]Running tokenizer on dataset (num_proc=64): 306779 examples [01:01, 3484.67 examples/s]es [00:57, 3486.11 examples/s]Running tokenizer on dataset (num_proc=64): 312530 examples [00:57, 4887.09 examples/s]Running tokenizer on dataset (num_proc=64): 313530 examples [00:58, 3592.86 examples/s]Running tokenizer on dataset (num_proc=64): 316281 examples [00:58, 5058.31 examples/s]Running tokenizer on dataset (num_proc=64): 317198 examples [00:58, 4522.76 examples/s]Running tokenizer on dataset (num_proc=64): 318115 examples [00:59, 2611.88 examples/s]Running tokenizer on dataset (num_proc=64): 319032 examples [01:00, 1894.37 examples/s]Running tokenizer on dataset (num_proc=64): 320866 examples [01:00, 2887.60 examples/s]Running tokenizer on dataset (num_proc=64): 322700 examples [01:00, 4020.15 examples/s]Running tokenizer on dataset (num_proc=64): 325700 examples [01:01, 5225.14 examples/s]Running tokenizer on dataset (num_proc=64): 327617 examples [01:01, 5864.01 examples/s]Running tokenizer on dataset (num_proc=64): 328534 examples [01:02, 4078.62 examples/s]Running tokenizer on dataRunning tokenizer on dataset (num_proc=64): 299277 examples [00:59, 3598.40 examples/s]Running tokenizer on dataset (num_proc=64): 300194 examples [01:00, 3498.15 examples/s]Running tokenizer on dataset (num_proc=64): 301111 examples [01:00, 3726.61 examples/s]Running tokenizer on dataset (num_proc=64): 303111 examples [01:00, 5486.23 examples/s]Running tokenizer on dataset (num_proc=64): 304945 examples [01:00, 6214.58 examples/s]Running tokenizer on dataset (num_proc=64): 305862 examples [01:00, 6132.02 examples/s]Running tokenizer on dataset (num_proc=64): 306779 examples [01:01, 3816.73 examples/s]Running tokenizer on dataset (num_proc=64): 308613 examples [01:01, 5379.61 examples/s]Running tokenizer on dataset (num_proc=64): 309613 examples [01:02, 3379.24 examples/s]Running tokenizer on dataset (num_proc=64): 310530 examples [01:02, 3382.26 examples/s]Running tokenizer on dataset (num_proc=64): 311447 examples [01:02, 3281.52 examples/s]Running tokenizer on dataset (num_proc=64): 312364 exampRunning tokenizer on dataset (num_proc=64): 298277 examples [00:59, 2161.43 examples/s]Running tokenizer on dataset (num_proc=64): 299277 examples [00:59, 2647.18 examples/s]Running tokenizer on dataset (num_proc=64): 300194 examples [00:59, 2577.50 examples/s]Running tokenizer on dataset (num_proc=64): 301194 examples [00:59, 3208.47 examples/s]Running tokenizer on dataset (num_proc=64): 303194 examples [01:00, 4956.30 examples/s]Running tokenizer on dataset (num_proc=64): 304194 examples [01:00, 2952.64 examples/s]Running tokenizer on dataset (num_proc=64): 305111 examples [01:01, 2656.74 examples/s]Running tokenizer on dataset (num_proc=64): 306028 examples [01:01, 2779.53 examples/s]Running tokenizer on dataset (num_proc=64): 307862 examples [01:02, 2882.17 examples/s]Running tokenizer on dataset (num_proc=64): 308779 examples [01:02, 2400.09 examples/s]Running tokenizer on dataset (num_proc=64): 309696 examples [01:03, 2434.66 examples/s]Running tokenizer on dataset (num_proc=64): 311530 exampRunning tokenizer on dataset (num_proc=64): 301111 examples [00:59, 3294.94 examples/s]Running tokenizer on dataset (num_proc=64): 303028 examples [01:00, 2406.21 examples/s]Running tokenizer on dataset (num_proc=64): 303945 examples [01:01, 2590.92 examples/s]Running tokenizer on dataset (num_proc=64): 304862 examples [01:01, 2551.12 examples/s]Running tokenizer on dataset (num_proc=64): 305862 examples [01:01, 2932.40 examples/s]Running tokenizer on dataset (num_proc=64): 306779 examples [01:02, 2589.54 examples/s]Running tokenizer on dataset (num_proc=64): 309530 examples [01:02, 4588.20 examples/s]Running tokenizer on dataset (num_proc=64): 310447 examples [01:02, 4621.93 examples/s]Running tokenizer on dataset (num_proc=64): 311447 examples [01:03, 3693.47 examples/s]Running tokenizer on dataset (num_proc=64): 312364 examples [01:03, 3624.97 examples/s]Running tokenizer on dataset (num_proc=64): 314198 examples [01:03, 3929.54 examples/s]Running tokenizer on dataset (num_proc=64): 315115 exampRunning tokenizer on dataset (num_proc=64): 307696 examples [01:01, 2929.37 examples/s]Running tokenizer on dataset (num_proc=64): 308613 examples [01:02, 2338.42 examples/s]Running tokenizer on dataset (num_proc=64): 310447 examples [01:02, 3333.48 examples/s]Running tokenizer on dataset (num_proc=64): 311364 examples [01:02, 3853.33 examples/s]Running tokenizer on dataset (num_proc=64): 313198 examples [01:02, 5340.76 examples/s]Running tokenizer on dataset (num_proc=64): 314198 examples [01:03, 3832.32 examples/s]Running tokenizer on dataset (num_proc=64): 315115 examples [01:03, 4035.42 examples/s]Running tokenizer on dataset (num_proc=64): 316032 examples [01:03, 3535.15 examples/s]Running tokenizer on dataset (num_proc=64): 316949 examples [01:03, 4192.17 examples/s]Running tokenizer on dataset (num_proc=64): 317866 examples [01:04, 4044.88 examples/s]Running tokenizer on dataset (num_proc=64): 319700 examples [01:04, 4831.13 examples/s]Running tokenizer on dataset (num_proc=64): 320700 exampes [01:01, 5707.33 examples/s]Running tokenizer on dataset (num_proc=64): 312364 examples [01:01, 3943.00 examples/s]Running tokenizer on dataset (num_proc=64): 313281 examples [01:01, 4020.42 examples/s]Running tokenizer on dataset (num_proc=64): 314198 examples [01:02, 4379.21 examples/s]Running tokenizer on dataset (num_proc=64): 315198 examples [01:02, 4795.97 examples/s]Running tokenizer on dataset (num_proc=64): 317115 examples [01:03, 3001.02 examples/s]Running tokenizer on dataset (num_proc=64): 318115 examples [01:03, 3443.11 examples/s]Running tokenizer on dataset (num_proc=64): 319115 examples [01:03, 2734.74 examples/s]Running tokenizer on dataset (num_proc=64): 320949 examples [01:04, 3952.55 examples/s]Running tokenizer on dataset (num_proc=64): 321866 examples [01:04, 3967.98 examples/s]Running tokenizer on dataset (num_proc=64): 322783 examples [01:04, 4404.79 examples/s]Running tokenizer on dataset (num_proc=64): 323700 examples [01:04, 4364.98 examples/s]Running tokenizer on dataRunning tokenizer on dataset (num_proc=64): 300277 examples [01:01, 1852.51 examples/s]Running tokenizer on dataset (num_proc=64): 301194 examples [01:01, 2345.21 examples/s]Running tokenizer on dataset (num_proc=64): 302194 examples [01:01, 2704.06 examples/s]Running tokenizer on dataset (num_proc=64): 303111 examples [01:01, 3221.24 examples/s]Running tokenizer on dataset (num_proc=64): 305945 examples [01:02, 5425.91 examples/s]Running tokenizer on dataset (num_proc=64): 306862 examples [01:03, 2148.02 examples/s]Running tokenizer on dataset (num_proc=64): 308779 examples [01:03, 2859.40 examples/s]Running tokenizer on dataset (num_proc=64): 309696 examples [01:03, 3055.64 examples/s]Running tokenizer on dataset (num_proc=64): 310613 examples [01:04, 3548.49 examples/s]Running tokenizer on dataset (num_proc=64): 312530 examples [01:04, 4704.19 examples/s]Running tokenizer on dataset (num_proc=64): 313530 examples [01:04, 4935.99 examples/s]Running tokenizer on dataset (num_proc=64): 315364 exames [01:00, 2307.51 examples/s]Running tokenizer on dataset (num_proc=64): 306028 examples [01:00, 3297.71 examples/s]Running tokenizer on dataset (num_proc=64): 308945 examples [01:01, 4154.65 examples/s]Running tokenizer on dataset (num_proc=64): 310862 examples [01:01, 4066.78 examples/s]Running tokenizer on dataset (num_proc=64): 311862 examples [01:02, 3323.95 examples/s]Running tokenizer on dataset (num_proc=64): 313696 examples [01:02, 4139.49 examples/s]Running tokenizer on dataset (num_proc=64): 314613 examples [01:02, 3926.03 examples/s]Running tokenizer on dataset (num_proc=64): 316447 examples [01:02, 5265.96 examples/s]Running tokenizer on dataset (num_proc=64): 317364 examples [01:03, 3610.99 examples/s]Running tokenizer on dataset (num_proc=64): 318281 examples [01:04, 2906.02 examples/s]Running tokenizer on dataset (num_proc=64): 319198 examples [01:04, 2782.25 examples/s]Running tokenizer on dataset (num_proc=64): 321032 examples [01:04, 4050.38 examples/s]Running tokenizer on dataset (num_proc=64): 329451 examples [01:02, 4195.14 examples/s]Running tokenizer on dataset (num_proc=64): 330451 examples [01:02, 4105.97 examples/s]Running tokenizer on dataset (num_proc=64): 332285 examples [01:02, 5016.20 examples/s]Running tokenizer on dataset (num_proc=64): 333202 examples [01:02, 4741.88 examples/s]Running tokenizer on dataset (num_proc=64): 334119 examples [01:03, 3169.89 examples/s]Running tokenizer on dataset (num_proc=64): 335119 examples [01:04, 2739.60 examples/s]Running tokenizer on dataset (num_proc=64): 336036 examples [01:04, 2502.29 examples/s]Running tokenizer on dataset (num_proc=64): 337870 examples [01:04, 3699.97 examples/s]Running tokenizer on dataset (num_proc=64): 338787 examples [01:05, 3188.32 examples/s]Running tokenizer on dataset (num_proc=64): 340704 examples [01:05, 4228.64 examples/s]Running tokenizer on dataset (num_proc=64): 341621 examples [01:05, 3305.58 examples/s]Running tokenizer on dataset (num_proc=64): 342621 examples [01:06, 2788.13 examples [01:03, 3397.98 examples/s]Running tokenizer on dataset (num_proc=64): 313281 examples [01:03, 3723.02 examples/s]Running tokenizer on dataset (num_proc=64): 314281 examples [01:03, 3642.65 examples/s]Running tokenizer on dataset (num_proc=64): 315281 examples [01:03, 4139.61 examples/s]Running tokenizer on dataset (num_proc=64): 317115 examples [01:03, 5461.27 examples/s]Running tokenizer on dataset (num_proc=64): 318032 examples [01:04, 5077.29 examples/s]Running tokenizer on dataset (num_proc=64): 319032 examples [01:04, 5257.17 examples/s]Running tokenizer on dataset (num_proc=64): 321783 examples [01:04, 8127.38 examples/s]Running tokenizer on dataset (num_proc=64): 322783 examples [01:04, 7016.76 examples/s]Running tokenizer on dataset (num_proc=64): 323700 examples [01:06, 2180.39 examples/s]Running tokenizer on dataset (num_proc=64): 325534 examples [01:06, 2661.42 examples/s]Running tokenizer on dataset (num_proc=64): 326451 examples [01:07, 2493.59 examples/s]Running tokenizer on datles [01:03, 3516.01 examples/s]Running tokenizer on dataset (num_proc=64): 312447 examples [01:03, 3980.71 examples/s]Running tokenizer on dataset (num_proc=64): 313364 examples [01:03, 3727.56 examples/s]Running tokenizer on dataset (num_proc=64): 314281 examples [01:03, 4358.47 examples/s]Running tokenizer on dataset (num_proc=64): 315198 examples [01:04, 4100.27 examples/s]Running tokenizer on dataset (num_proc=64): 317198 examples [01:04, 5865.10 examples/s]Running tokenizer on dataset (num_proc=64): 319115 examples [01:04, 6446.82 examples/s]Running tokenizer on dataset (num_proc=64): 321866 examples [01:04, 9154.65 examples/s]Running tokenizer on dataset (num_proc=64): 323783 examples [01:05, 4388.14 examples/s]Running tokenizer on dataset (num_proc=64): 325700 examples [01:05, 5416.28 examples/s]Running tokenizer on dataset (num_proc=64): 327617 examples [01:06, 4773.43 examples/s]Running tokenizer on dataset (num_proc=64): 328534 examples [01:06, 4469.48 examples/s]Running tokenizer on datles [01:03, 3930.99 examples/s]Running tokenizer on dataset (num_proc=64): 316949 examples [01:04, 5590.28 examples/s]Running tokenizer on dataset (num_proc=64): 317949 examples [01:04, 5025.51 examples/s]Running tokenizer on dataset (num_proc=64): 319783 examples [01:04, 5447.93 examples/s]Running tokenizer on dataset (num_proc=64): 322700 examples [01:04, 8728.16 examples/s]Running tokenizer on dataset (num_proc=64): 324534 examples [01:05, 5619.36 examples/s]Running tokenizer on dataset (num_proc=64): 326451 examples [01:06, 3065.75 examples/s]Running tokenizer on dataset (num_proc=64): 327451 examples [01:06, 3185.50 examples/s]Running tokenizer on dataset (num_proc=64): 328451 examples [01:07, 3480.41 examples/s]Running tokenizer on dataset (num_proc=64): 330368 examples [01:07, 3449.62 examples/s]Running tokenizer on dataset (num_proc=64): 331368 examples [01:07, 3519.32 examples/s]Running tokenizer on dataset (num_proc=64): 332285 examples [01:07, 3861.96 examples/s]Running tokenizer on datset (num_proc=64): 321949 examples [01:04, 3823.00 examples/s]Running tokenizer on dataset (num_proc=64): 322866 examples [01:04, 4254.97 examples/s]Running tokenizer on dataset (num_proc=64): 325700 examples [01:05, 6896.58 examples/s]Running tokenizer on dataset (num_proc=64): 326617 examples [01:05, 5419.89 examples/s]Running tokenizer on dataset (num_proc=64): 327534 examples [01:05, 4040.19 examples/s]Running tokenizer on dataset (num_proc=64): 328451 examples [01:06, 3505.13 examples/s]Running tokenizer on dataset (num_proc=64): 329368 examples [01:06, 3314.74 examples/s]Running tokenizer on dataset (num_proc=64): 331285 examples [01:06, 4227.17 examples/s]Running tokenizer on dataset (num_proc=64): 332285 examples [01:07, 3779.63 examples/s]Running tokenizer on dataset (num_proc=64): 333202 examples [01:07, 4310.17 examples/s]Running tokenizer on dataset (num_proc=64): 334202 examples [01:07, 3642.47 examples/s]Running tokenizer on dataset (num_proc=64): 335119 examples [01:08, 2749.14 exampples [01:04, 5277.93 examples/s]Running tokenizer on dataset (num_proc=64): 318198 examples [01:04, 8312.60 examples/s]Running tokenizer on dataset (num_proc=64): 321032 examples [01:05, 9128.24 examples/s]Running tokenizer on dataset (num_proc=64): 322949 examples [01:05, 9990.08 examples/s]Running tokenizer on dataset (num_proc=64): 324783 examples [01:05, 8110.89 examples/s]Running tokenizer on dataset (num_proc=64): 326783 examples [01:06, 6555.27 examples/s]Running tokenizer on dataset (num_proc=64): 328700 examples [01:06, 7985.83 examples/s]Running tokenizer on dataset (num_proc=64): 330534 examples [01:06, 5984.27 examples/s]Running tokenizer on dataset (num_proc=64): 331451 examples [01:07, 4606.47 examples/s]Running tokenizer on dataset (num_proc=64): 332368 examples [01:07, 2770.18 examples/s]Running tokenizer on dataset (num_proc=64): 333285 examples [01:08, 2666.38 examples/s]Running tokenizer on dataset (num_proc=64): 334285 examples [01:09, 2154.91 examples/s]Running tokenizer on daset (num_proc=64): 324700 examples [01:04, 4691.24 examples/s]Running tokenizer on dataset (num_proc=64): 326534 examples [01:04, 5825.11 examples/s]Running tokenizer on dataset (num_proc=64): 327451 examples [01:05, 3284.93 examples/s]Running tokenizer on dataset (num_proc=64): 328368 examples [01:05, 3397.81 examples/s]Running tokenizer on dataset (num_proc=64): 329285 examples [01:06, 3080.91 examples/s]Running tokenizer on dataset (num_proc=64): 330202 examples [01:07, 2173.23 examples/s]Running tokenizer on dataset (num_proc=64): 332119 examples [01:07, 3224.44 examples/s]Running tokenizer on dataset (num_proc=64): 333119 examples [01:07, 3372.22 examples/s]Running tokenizer on dataset (num_proc=64): 334036 examples [01:07, 3808.25 examples/s]Running tokenizer on dataset (num_proc=64): 334953 examples [01:08, 2527.40 examples/s]Running tokenizer on dataset (num_proc=64): 335870 examples [01:09, 1329.92 examples/s]Running tokenizer on dataset (num_proc=64): 336787 examples [01:10, 1537.91 examples [01:04, 5539.02 examples/s]Running tokenizer on dataset (num_proc=64): 321617 examples [01:05, 2646.73 examples/s]Running tokenizer on dataset (num_proc=64): 323617 examples [01:06, 2437.67 examples/s]Running tokenizer on dataset (num_proc=64): 324534 examples [01:07, 2061.46 examples/s]Running tokenizer on dataset (num_proc=64): 325451 examples [01:07, 2512.33 examples/s]Running tokenizer on dataset (num_proc=64): 327368 examples [01:07, 3447.23 examples/s]Running tokenizer on dataset (num_proc=64): 328368 examples [01:07, 3462.28 examples/s]Running tokenizer on dataset (num_proc=64): 331202 examples [01:07, 5768.21 examples/s]Running tokenizer on dataset (num_proc=64): 333119 examples [01:08, 7211.79 examples/s]Running tokenizer on dataset (num_proc=64): 335119 examples [01:09, 3291.66 examples/s]Running tokenizer on dataset (num_proc=64): 336953 examples [01:10, 2662.19 examples/s]Running tokenizer on dataset (num_proc=64): 337870 examples [01:10, 2505.83 examples/s]Running tokenizer on dataset (num_proc=64): 327368 examples [01:07, 2994.37 examples/s]Running tokenizer on dataset (num_proc=64): 328368 examples [01:07, 3378.14 examples/s]Running tokenizer on dataset (num_proc=64): 329368 examples [01:07, 2909.49 examples/s]Running tokenizer on dataset (num_proc=64): 330368 examples [01:08, 2743.31 examples/s]Running tokenizer on dataset (num_proc=64): 331368 examples [01:09, 1645.24 examples/s]Running tokenizer on dataset (num_proc=64): 332285 examples [01:09, 1951.76 examples/s]Running tokenizer on dataset (num_proc=64): 333285 examples [01:10, 2136.15 examples/s]Running tokenizer on dataset (num_proc=64): 334202 examples [01:10, 2427.19 examples/s]Running tokenizer on dataset (num_proc=64): 335119 examples [01:10, 3020.95 examples/s]Running tokenizer on dataset (num_proc=64): 337036 examples [01:10, 4843.53 examples/s]Running tokenizer on dataset (num_proc=64): 337953 examples [01:11, 2592.05 examples/s]Running tokenizer on dataset (num_proc=64): 339787 examples [01:11, 3932.78 examaset (num_proc=64): 333202 examples [01:08, 3974.21 examples/s]Running tokenizer on dataset (num_proc=64): 334202 examples [01:09, 2486.89 examples/s]Running tokenizer on dataset (num_proc=64): 335119 examples [01:09, 2174.97 examples/s]Running tokenizer on dataset (num_proc=64): 336119 examples [01:10, 2156.42 examples/s]Running tokenizer on dataset (num_proc=64): 337036 examples [01:10, 1686.37 examples/s]Running tokenizer on dataset (num_proc=64): 337953 examples [01:11, 2112.78 examples/s]Running tokenizer on dataset (num_proc=64): 338953 examples [01:11, 1896.50 examples/s]Running tokenizer on dataset (num_proc=64): 339870 examples [01:11, 2349.42 examples/s]Running tokenizer on dataset (num_proc=64): 340787 examples [01:12, 1893.61 examples/s]Running tokenizer on dataset (num_proc=64): 341704 examples [01:13, 1652.33 examples/s]Running tokenizer on dataset (num_proc=64): 342704 examples [01:13, 2221.39 examples/s]Running tokenizer on dataset (num_proc=64): 343621 examples [01:14, 1344.37 examaset (num_proc=64): 330368 examples [01:07, 2405.94 examples/s]Running tokenizer on dataset (num_proc=64): 331368 examples [01:08, 2640.31 examples/s]Running tokenizer on dataset (num_proc=64): 332285 examples [01:08, 3010.41 examples/s]Running tokenizer on dataset (num_proc=64): 333202 examples [01:08, 2667.21 examples/s]Running tokenizer on dataset (num_proc=64): 334119 examples [01:09, 3000.54 examples/s]Running tokenizer on dataset (num_proc=64): 335036 examples [01:09, 2420.86 examples/s]Running tokenizer on dataset (num_proc=64): 335953 examples [01:11, 1145.24 examples/s]Running tokenizer on dataset (num_proc=64): 336870 examples [01:11, 1330.91 examples/s]Running tokenizer on dataset (num_proc=64): 338704 examples [01:12, 1978.82 examples/s]Running tokenizer on dataset (num_proc=64): 339704 examples [01:13, 1527.61 examples/s]Running tokenizer on dataset (num_proc=64): 340704 examples [01:14, 1449.61 examples/s]Running tokenizer on dataset (num_proc=64): 341704 examples [01:15, 1193.10 examtaset (num_proc=64): 335202 examples [01:09, 2586.49 examples/s]Running tokenizer on dataset (num_proc=64): 336202 examples [01:09, 2133.65 examples/s]Running tokenizer on dataset (num_proc=64): 337119 examples [01:10, 1893.21 examples/s]Running tokenizer on dataset (num_proc=64): 338036 examples [01:10, 2066.59 examples/s]Running tokenizer on dataset (num_proc=64): 338953 examples [01:11, 1764.88 examples/s]Running tokenizer on dataset (num_proc=64): 341870 examples [01:12, 2582.28 examples/s]Running tokenizer on dataset (num_proc=64): 342787 examples [01:12, 2532.60 examples/s]Running tokenizer on dataset (num_proc=64): 343787 examples [01:13, 2554.15 examples/s]Running tokenizer on dataset (num_proc=64): 344704 examples [01:14, 1724.05 examples/s]Running tokenizer on dataset (num_proc=64): 345621 examples [01:14, 1731.28 examples/s]Running tokenizer on dataset (num_proc=64): 346621 examples [01:16, 1252.26 examples/s]Running tokenizer on dataset (num_proc=64): 347538 examples [01:16, 1371.78 exales/s]Running tokenizer on dataset (num_proc=64): 336036 examples [01:08, 2670.96 examples/s]Running tokenizer on dataset (num_proc=64): 336953 examples [01:08, 3268.96 examples/s]Running tokenizer on dataset (num_proc=64): 337870 examples [01:10, 1327.79 examples/s]Running tokenizer on dataset (num_proc=64): 338787 examples [01:11, 1296.56 examples/s]Running tokenizer on dataset (num_proc=64): 339704 examples [01:11, 1582.52 examples/s]Running tokenizer on dataset (num_proc=64): 340621 examples [01:11, 1684.51 examples/s]Running tokenizer on dataset (num_proc=64): 341621 examples [01:12, 2132.11 examples/s]Running tokenizer on dataset (num_proc=64): 343538 examples [01:12, 3191.61 examples/s]Running tokenizer on dataset (num_proc=64): 344538 examples [01:12, 3594.57 examples/s]Running tokenizer on dataset (num_proc=64): 346538 examples [01:13, 3241.50 examples/s]Running tokenizer on dataset (num_proc=64): 347538 examples [01:14, 1672.78 examples/s]Running tokenizer on dataset (num_proc=64): 34753les/s]Running tokenizer on dataset (num_proc=64): 343538 examples [01:06, 3260.12 examples/s]Running tokenizer on dataset (num_proc=64): 344538 examples [01:06, 3850.95 examples/s]Running tokenizer on dataset (num_proc=64): 345538 examples [01:10, 839.75 examples/s] Running tokenizer on dataset (num_proc=64): 346538 examples [01:10, 959.27 examples/s]Running tokenizer on dataset (num_proc=64): 347538 examples [01:13, 696.09 examples/s]Running tokenizer on dataset (num_proc=64): 348538 examples [01:19, 333.78 examples/s]Running tokenizer on dataset (num_proc=64): 349538 examples [01:23, 325.62 examples/s]Running tokenizer on dataset (num_proc=64): 350455 examples [01:23, 410.58 examples/s]Running tokenizer on dataset (num_proc=64): 351455 examples [01:25, 449.67 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:27, 508.98 examples/s]Running tokenizer on dataset (num_proc=64): 353455 examples [01:27, 657.01 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examplles/s]Running tokenizer on dataset (num_proc=64): 337787 examples [01:10, 1921.86 examples/s]Running tokenizer on dataset (num_proc=64): 339704 examples [01:11, 1932.52 examples/s]Running tokenizer on dataset (num_proc=64): 340704 examples [01:11, 2397.49 examples/s]Running tokenizer on dataset (num_proc=64): 341704 examples [01:11, 2476.83 examples/s]Running tokenizer on dataset (num_proc=64): 342704 examples [01:12, 2140.27 examples/s]Running tokenizer on dataset (num_proc=64): 343621 examples [01:12, 2684.86 examples/s]Running tokenizer on dataset (num_proc=64): 344621 examples [01:13, 1996.02 examples/s]Running tokenizer on dataset (num_proc=64): 345538 examples [01:13, 2100.17 examples/s]Running tokenizer on dataset (num_proc=64): 346538 examples [01:18, 598.27 examples/s] Running tokenizer on dataset (num_proc=64): 347538 examples [01:19, 667.41 examples/s]Running tokenizer on dataset (num_proc=64): 348538 examples [01:27, 287.84 examples/s]Running tokenizer on dataset (num_proc=64): 349538 aset (num_proc=64): 338787 examples [01:11, 2511.70 examples/s]Running tokenizer on dataset (num_proc=64): 339787 examples [01:11, 2398.09 examples/s]Running tokenizer on dataset (num_proc=64): 340704 examples [01:12, 2180.84 examples/s]Running tokenizer on dataset (num_proc=64): 341704 examples [01:12, 2418.51 examples/s]Running tokenizer on dataset (num_proc=64): 342704 examples [01:12, 2434.99 examples/s]Running tokenizer on dataset (num_proc=64): 343621 examples [01:13, 1944.40 examples/s]Running tokenizer on dataset (num_proc=64): 344538 examples [01:14, 1454.17 examples/s]Running tokenizer on dataset (num_proc=64): 345538 examples [01:15, 1135.99 examples/s]Running tokenizer on dataset (num_proc=64): 346538 examples [01:16, 1511.15 examples/s]Running tokenizer on dataset (num_proc=64): 347538 examples [01:16, 1381.27 examples/s]Running tokenizer on dataset (num_proc=64): 348455 examples [01:28, 241.19 examples/s] Running tokenizer on dataset (num_proc=64): 349455 examples [01:29, 308.53 exampples/s]Running tokenizer on dataset (num_proc=64): 342621 examples [01:15, 1448.64 examples/s]Running tokenizer on dataset (num_proc=64): 343538 examples [01:15, 1726.38 examples/s]Running tokenizer on dataset (num_proc=64): 344538 examples [01:16, 2278.43 examples/s]Running tokenizer on dataset (num_proc=64): 345538 examples [01:16, 2078.19 examples/s]Running tokenizer on dataset (num_proc=64): 346538 examples [01:17, 1454.78 examples/s]Running tokenizer on dataset (num_proc=64): 347538 examples [01:18, 1461.16 examples/s]Running tokenizer on dataset (num_proc=64): 348455 examples [01:27, 300.22 examples/s] Running tokenizer on dataset (num_proc=64): 349455 examples [01:28, 396.91 examples/s]Running tokenizer on dataset (num_proc=64): 350455 examples [01:29, 492.48 examples/s]Running tokenizer on dataset (num_proc=64): 351455 examples [01:34, 348.72 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:34, 486.64 examples/s]Running tokenizer on dataset (num_proc=64): 353455 eples/s]Running tokenizer on dataset (num_proc=64): 341704 examples [01:12, 2651.81 examples/s]Running tokenizer on dataset (num_proc=64): 342621 examples [01:13, 2653.30 examples/s]Running tokenizer on dataset (num_proc=64): 343621 examples [01:13, 2220.80 examples/s]Running tokenizer on dataset (num_proc=64): 344538 examples [01:14, 1732.26 examples/s]Running tokenizer on dataset (num_proc=64): 345538 examples [01:15, 1277.97 examples/s]Running tokenizer on dataset (num_proc=64): 346538 examples [01:18, 775.15 examples/s] Running tokenizer on dataset (num_proc=64): 347538 examples [01:20, 628.71 examples/s]Running tokenizer on dataset (num_proc=64): 348538 examples [01:28, 306.08 examples/s]Running tokenizer on dataset (num_proc=64): 349455 examples [01:30, 339.00 examples/s]Running tokenizer on dataset (num_proc=64): 350455 examples [01:32, 377.47 examples/s]Running tokenizer on dataset (num_proc=64): 351455 examples [01:33, 427.09 examples/s]Running tokenizer on dataset (num_proc=64): 352455 exples/s]Running tokenizer on dataset (num_proc=64): 344538 examples [01:15, 1524.36 examples/s]Running tokenizer on dataset (num_proc=64): 345538 examples [01:18, 621.69 examples/s] Running tokenizer on dataset (num_proc=64): 346538 examples [01:19, 753.96 examples/s]Running tokenizer on dataset (num_proc=64): 347538 examples [01:20, 892.32 examples/s]Running tokenizer on dataset (num_proc=64): 348538 examples [01:22, 745.16 examples/s]Running tokenizer on dataset (num_proc=64): 349455 examples [01:28, 343.39 examples/s]Running tokenizer on dataset (num_proc=64): 350455 examples [01:35, 242.26 examples/s]Running tokenizer on dataset (num_proc=64): 351455 examples [01:35, 341.58 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:36, 423.51 examples/s]Running tokenizer on dataset (num_proc=64): 353455 examples [01:37, 514.83 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:38, 627.69 examples/s]Running tokenizer on dataset (num_proc=64): 355455 exampl8 examples [01:25, 1672.78 examples/s]Running tokenizer on dataset (num_proc=64): 348538 examples [01:26, 308.99 examples/s] Running tokenizer on dataset (num_proc=64): 349538 examples [01:28, 339.22 examples/s]Running tokenizer on dataset (num_proc=64): 350538 examples [01:30, 371.32 examples/s]Running tokenizer on dataset (num_proc=64): 351538 examples [01:31, 449.97 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:31, 554.41 examples/s]Running tokenizer on dataset (num_proc=64): 353455 examples [01:32, 679.08 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:36, 479.09 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:37, 564.09 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:37, 713.82 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:40, 757.10 examples/s]Running tokenizer on dataset (num_proc=64): 359455 examples [01:41, 757.83 examples/s]Running tokenizer on datasemples/s]Running tokenizer on dataset (num_proc=64): 348538 examples [01:22, 445.53 examples/s] Running tokenizer on dataset (num_proc=64): 349455 examples [01:32, 216.27 examples/s]Running tokenizer on dataset (num_proc=64): 350455 examples [01:32, 304.00 examples/s]Running tokenizer on dataset (num_proc=64): 351455 examples [01:35, 319.35 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:35, 441.59 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:36, 663.69 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:37, 804.04 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:38, 803.47 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:39, 1199.88 examples/s]Running tokenizer on dataset (num_proc=64): 359455 examples [01:43, 631.18 examples/s] Running tokenizer on dataset (num_proc=64): 360455 examples [01:44, 691.22 examples/s]Running tokenizer on dataset (num_proc=64): 361372 examles/s]Running tokenizer on dataset (num_proc=64): 350455 examples [01:33, 290.22 examples/s]Running tokenizer on dataset (num_proc=64): 351455 examples [01:37, 284.08 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:38, 362.82 examples/s]Running tokenizer on dataset (num_proc=64): 353455 examples [01:38, 493.13 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:40, 515.14 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:41, 645.02 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:41, 813.39 examples/s]Running tokenizer on dataset (num_proc=64): 357455 examples [01:42, 1058.34 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:42, 1059.06 examples/s]Running tokenizer on dataset (num_proc=64): 359455 examples [01:43, 1361.36 examples/s]Running tokenizer on dataset (num_proc=64): 360455 examples [01:46, 622.21 examples/s] Running tokenizer on dataset (num_proc=64): 361455 exampexamples [01:29, 320.94 examples/s]Running tokenizer on dataset (num_proc=64): 350455 examples [01:32, 325.28 examples/s]Running tokenizer on dataset (num_proc=64): 351455 examples [01:33, 393.56 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:34, 487.02 examples/s]Running tokenizer on dataset (num_proc=64): 353455 examples [01:36, 540.04 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:39, 457.40 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:39, 621.21 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:40, 683.22 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:45, 548.36 examples/s]Running tokenizer on dataset (num_proc=64): 359455 examples [01:45, 697.14 examples/s]Running tokenizer on dataset (num_proc=64): 360455 examples [01:48, 494.74 examples/s]Running tokenizer on dataset (num_proc=64): 361372 examples [01:51, 476.57 examples/s]Running tokenizer on dataset (nes [01:28, 770.33 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:31, 527.23 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:32, 615.21 examples/s]Running tokenizer on dataset (num_proc=64): 357455 examples [01:33, 757.55 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:35, 648.62 examples/s]Running tokenizer on dataset (num_proc=64): 359455 examples [01:36, 706.91 examples/s]Running tokenizer on dataset (num_proc=64): 360455 examples [01:37, 716.60 examples/s]Running tokenizer on dataset (num_proc=64): 361455 examples [01:38, 827.91 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:40, 693.09 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:47, 317.67 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [01:48, 398.92 examples/s]Running tokenizer on dataset (num_proc=64): 365123 examples [01:51, 331.57 examples/s]Running tokenizer on dataset (num_proxamples [01:35, 546.99 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:37, 556.61 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:42, 355.75 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:43, 422.94 examples/s]Running tokenizer on dataset (num_proc=64): 357455 examples [01:46, 391.74 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:47, 468.35 examples/s]Running tokenizer on dataset (num_proc=64): 359455 examples [01:48, 588.72 examples/s]Running tokenizer on dataset (num_proc=64): 360455 examples [01:49, 732.55 examples/s]Running tokenizer on dataset (num_proc=64): 361455 examples [01:49, 839.07 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:52, 657.18 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:52, 879.47 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [02:02, 242.85 examples/s]Running tokenizer on dataset (nuamples [01:36, 383.03 examples/s]Running tokenizer on dataset (num_proc=64): 353455 examples [01:37, 537.27 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:37, 713.10 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:40, 562.85 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:41, 582.91 examples/s]Running tokenizer on dataset (num_proc=64): 357455 examples [01:43, 545.14 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:45, 560.86 examples/s]Running tokenizer on dataset (num_proc=64): 359455 examples [01:48, 475.82 examples/s]Running tokenizer on dataset (num_proc=64): 360455 examples [01:50, 456.70 examples/s]Running tokenizer on dataset (num_proc=64): 361372 examples [01:51, 560.12 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:53, 697.23 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [02:00, 330.64 examples/s]Running tokenizer on dataset (numc=64): 366040 examples [01:52, 455.34 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [01:53, 504.06 examples/s]Running tokenizer on dataset (num_proc=64): 367874 examples [01:56, 449.75 examples/s]Running tokenizer on dataset (num_proc=64): 368791 examples [01:58, 414.12 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [01:58, 578.21 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:00, 564.07 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:01, 717.37 examples/s]Running tokenizer on dataset (num_proc=64): 372459 examples [02:01, 977.35 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:02, 827.77 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:03, 1512.03 examples/s]
es [01:39, 613.63 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:40, 831.05 examples/s]Running tokenizer on dataset (num_proc=64): 357455 examples [01:43, 540.75 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:44, 683.10 examples/s]Running tokenizer on dataset (num_proc=64): 359372 examples [01:45, 699.40 examples/s]Running tokenizer on dataset (num_proc=64): 360372 examples [01:45, 886.65 examples/s]Running tokenizer on dataset (num_proc=64): 361372 examples [01:52, 355.82 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:52, 479.20 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:58, 301.07 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [02:04, 232.37 examples/s]Running tokenizer on dataset (num_proc=64): 365123 examples [02:07, 268.02 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [02:07, 360.54 examples/s]Running tokenizer on dataset (num_prot (num_proc=64): 360455 examples [01:44, 603.26 examples/s]Running tokenizer on dataset (num_proc=64): 361455 examples [01:46, 539.95 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:48, 497.78 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:49, 552.52 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [01:55, 334.11 examples/s]Running tokenizer on dataset (num_proc=64): 365123 examples [01:59, 298.62 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [01:59, 399.84 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [02:04, 300.99 examples/s]Running tokenizer on dataset (num_proc=64): 367874 examples [02:04, 406.46 examples/s]Running tokenizer on dataset (num_proc=64): 368791 examples [02:07, 387.76 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [02:07, 508.61 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:11, 494.53 examples/s]Runningles [01:47, 684.08 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:52, 387.82 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:57, 313.10 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [02:03, 228.73 examples/s]Running tokenizer on dataset (num_proc=64): 365123 examples [02:06, 252.66 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [02:07, 333.29 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [02:08, 421.10 examples/s]Running tokenizer on dataset (num_proc=64): 367874 examples [02:10, 390.77 examples/s]Running tokenizer on dataset (num_proc=64): 368791 examples [02:11, 541.21 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [02:11, 734.51 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:12, 778.37 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:13, 850.01 examples/s]Running tokenizer on dataset (num_prples [01:45, 680.33 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:46, 703.98 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:56, 253.29 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [02:04, 190.65 examples/s]Running tokenizer on dataset (num_proc=64): 365123 examples [02:04, 261.26 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [02:05, 360.21 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [02:06, 425.33 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [02:06, 908.05 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:06, 1045.91 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:10, 641.64 examples/s] Running tokenizer on dataset (num_proc=64): 373376 examples [02:15, 505.31 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:15, 1374.75 examples/s]
 tokenizer on dataset (num_proc=64): 372459 examples [02:12, 571.30 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:14, 515.98 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:15, 1374.76 examples/s]
oc=64): 372459 examples [02:14, 708.88 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:15, 908.81 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:15, 1372.81 examples/s]
um_proc=64): 362372 examples [01:51, 625.01 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:53, 541.10 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [02:00, 306.36 examples/s]Running tokenizer on dataset (num_proc=64): 365123 examples [02:05, 252.29 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [02:06, 329.48 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [02:06, 456.43 examples/s]Running tokenizer on dataset (num_proc=64): 368791 examples [02:08, 560.45 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [02:09, 593.52 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:13, 461.10 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:14, 528.16 examples/s]Running tokenizer on dataset (num_proc=64): 372459 examples [02:16, 499.88 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:17, 575.01 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:18, 1352.32 examples/s]
m_proc=64): 365123 examples [02:03, 322.99 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [02:03, 439.59 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [02:03, 581.33 examples/s]Running tokenizer on dataset (num_proc=64): 367874 examples [02:08, 364.14 examples/s]Running tokenizer on dataset (num_proc=64): 368791 examples [02:14, 267.94 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [02:15, 336.28 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:15, 455.70 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:16, 585.18 examples/s]Running tokenizer on dataset (num_proc=64): 372459 examples [02:16, 806.42 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:17, 827.40 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:18, 1350.75 examples/s]
_proc=64): 365123 examples [02:03, 334.86 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [02:05, 361.18 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [02:08, 329.13 examples/s]Running tokenizer on dataset (num_proc=64): 367874 examples [02:11, 330.82 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:12, 658.89 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:13, 628.94 examples/s]Running tokenizer on dataset (num_proc=64): 372459 examples [02:17, 481.95 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:18, 558.03 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:19, 1342.57 examples/s]
c=64): 368791 examples [02:07, 791.44 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [02:11, 507.71 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:12, 604.87 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:13, 624.35 examples/s]Running tokenizer on dataset (num_proc=64): 372459 examples [02:18, 411.23 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:19, 445.47 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:20, 1327.60 examples/s]
[INFO|configuration_utils.py:763] 2025-10-04 12:05:15,908 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 12:05:15,908 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 12:05:15,908 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 12:05:15,908 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 12:05:15,908 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 12:05:15,908 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 12:05:15,909 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|configuration_utils.py:839] 2025-10-04 12:05:15,909 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-04 12:05:15,909 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|configuration_utils.py:839] 2025-10-04 12:05:15,909 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|configuration_utils.py:839] 2025-10-04 12:05:15,909 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|configuration_utils.py:839] 2025-10-04 12:05:15,909 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-04 12:05:15,918 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 12:05:15,919 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 12:05:15,919 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-04 12:05:15,920 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|modeling_utils.py:1277] 2025-10-04 12:05:16,672 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 12:05:16,672 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 12:05:16,672 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 12:05:16,672 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 12:05:16,672 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 12:05:16,672 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 12:05:16,672 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-04 12:05:16,673 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 12:05:16,673 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 12:05:16,673 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 12:05:16,673 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 12:05:16,673 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 12:05:16,673 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 12:05:16,673 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-04 12:05:16,684 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 12:05:16,684 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 12:05:16,686 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 12:05:16,686 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 12:05:16,687 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 12:05:16,694 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 12:05:16,696 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|modeling_utils.py:1277] 2025-10-04 12:05:16,825 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-04 12:05:16,827 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-04 12:05:16,843 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.73s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:10,  8.84s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.99s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:08<01:09,  8.74s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:17<01:02,  8.96s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]L51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]L51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]L51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]L51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]L51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]L51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.62s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:34<00:41,  8.35s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.19s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]L51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:26<00:51,  8.61s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:33<00:41,  8.34s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:41<00:32,  8.17s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.69s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
[INFO|modeling_utils.py:5724] 2025-10-04 12:06:28,035 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 12:06:28,036 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]

Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██[INFO|modeling_utils.py:5724] 2025-10-04 12:06:28,036 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
[INFO|modeling_utils.py:5724] 2025-10-04 12:06:28,036 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 12:06:28,036 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
[INFO|modeling_utils.py:5732] 2025-10-04 12:06:28,036 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██oading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:49<00:23,  7.97s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:57<00:15,  7.95s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [01:04<00:07,  7.83s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]

Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it][INFO|modeling_utils.py:5724] 2025-10-04 12:06:28,036 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
[INFO|modeling_utils.py:5732] 2025-10-04 12:06:28,036 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
[INFO|modeling_utils.py:5724] 2025-10-04 12:06:28,036 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5724] 2025-10-04 12:06:28,036 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 12:06:28,037 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|modeling_utils.py:5732] 2025-10-04 12:06:28,036 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|modeling_utils.py:5724] 2025-10-04 12:06:28,037 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 12:06:28,037 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1008] 2025-10-04 12:06:28,037 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-04 12:06:28,037 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-04 12:06:28,038 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1008] 2025-10-04 12:06:28,038 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-04 12:06:28,038 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1055] 2025-10-04 12:06:28,038 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-04 12:06:28,038 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1008] 2025-10-04 12:06:28,038 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-04 12:06:28,038 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-04 12:06:28,038 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-04 12:06:28,038 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-04 12:06:28,038 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-04 12:06:28,039 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1055] 2025-10-04 12:06:28,039 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  6.73s/it]Loading checkpoint shards: 100%|██████████| 9/9 [01:09<00:00,  7.70s/it]
[INFO|modeling_utils.py:5724] 2025-10-04 12:06:28,068 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 12:06:28,068 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1008] 2025-10-04 12:06:28,070 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-04 12:06:28,070 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|trainer.py:757] 2025-10-04 12:06:32,345 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 12:06:32,346 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 12:06:32,350 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 12:06:32,350 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 12:06:32,351 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 12:06:32,351 >> Using auto half precision backend
[INFO|trainer.py:757] 2025-10-04 12:06:32,351 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 12:06:32,351 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 12:06:32,351 >> Using auto half precision backend
[WARNING|trainer.py:985] 2025-10-04 12:06:32,351 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 12:06:32,352 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 12:06:32,352 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 12:06:32,361 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 12:06:32,362 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 12:06:32,373 >> Using auto half precision backend
[WARNING|trainer.py:985] 2025-10-04 12:06:32,374 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 12:06:32,629 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 12:06:32,631 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 12:06:32,635 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[INFO|deepspeed.py:380] 2025-10-04 12:06:32,637 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[INFO|deepspeed.py:380] 2025-10-04 12:06:32,637 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 12:06:32,644 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 12:06:32,648 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 4. Using DeepSpeed's value.
[INFO|deepspeed.py:380] 2025-10-04 12:06:32,677 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[rank9]:W1004 12:06:33.384000 2456885 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank9]:W1004 12:06:33.384000 2456885 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank25]:W1004 12:06:33.433000 2535265 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank25]:W1004 12:06:33.433000 2535265 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank10]:W1004 12:06:33.464000 2456886 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank10]:W1004 12:06:33.464000 2456886 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank24]:W1004 12:06:33.490000 2535264 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank24]:W1004 12:06:33.490000 2535264 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank7]:W1004 12:06:33.505000 169750 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank7]:W1004 12:06:33.505000 169750 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank31]:W1004 12:06:33.529000 3060555 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank31]:W1004 12:06:33.529000 3060555 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank23]:W1004 12:06:33.566000 3903460 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank23]:W1004 12:06:33.566000 3903460 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank0]:W1004 12:06:33.668000 2824047 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank0]:W1004 12:06:33.668000 2824047 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank21]:W1004 12:06:33.727000 3903458 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank21]:W1004 12:06:33.727000 3903458 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank22]:W1004 12:06:33.747000 3903459 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank22]:W1004 12:06:33.747000 3903459 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank2]:W1004 12:06:33.763000 2824049 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank2]:W1004 12:06:33.763000 2824049 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[INFO|trainer.py:2523] 2025-10-04 12:06:36,105 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-04 12:06:36,105 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-04 12:06:36,105 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-04 12:06:36,106 >>   Num examples = 186,688
[INFO|trainer.py:2524] 2025-10-04 12:06:36,106 >>   Num examples = 186,688
[INFO|trainer.py:2524] 2025-10-04 12:06:36,106 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-04 12:06:36,106 >>   Num Epochs = 1
[INFO|trainer.py:2525] 2025-10-04 12:06:36,106 >>   Num Epochs = 1
[INFO|trainer.py:2525] 2025-10-04 12:06:36,106 >>   Num Epochs = 1
[INFO|trainer.py:2523] 2025-10-04 12:06:36,106 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-04 12:06:36,106 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-04 12:06:36,106 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-04 12:06:36,106 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2523] 2025-10-04 12:06:36,106 >> ***** Running training *****
[INFO|trainer.py:2526] 2025-10-04 12:06:36,106 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-04 12:06:36,106 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2529] 2025-10-04 12:06:36,106 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-04 12:06:36,106 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2530] 2025-10-04 12:06:36,106 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-04 12:06:36,106 >>   Total optimization steps = 1,459
[INFO|trainer.py:2526] 2025-10-04 12:06:36,106 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2524] 2025-10-04 12:06:36,106 >>   Num examples = 186,688
[INFO|trainer.py:2531] 2025-10-04 12:06:36,106 >>   Total optimization steps = 1,459
[INFO|trainer.py:2529] 2025-10-04 12:06:36,106 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2525] 2025-10-04 12:06:36,106 >>   Num Epochs = 1
[INFO|trainer.py:2530] 2025-10-04 12:06:36,106 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-04 12:06:36,106 >>   Total optimization steps = 1,459
[INFO|trainer.py:2526] 2025-10-04 12:06:36,106 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2526] 2025-10-04 12:06:36,106 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2523] 2025-10-04 12:06:36,106 >> ***** Running training *****
[INFO|trainer.py:2529] 2025-10-04 12:06:36,106 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2529] 2025-10-04 12:06:36,106 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2524] 2025-10-04 12:06:36,106 >>   Num examples = 186,688
[INFO|trainer.py:2530] 2025-10-04 12:06:36,106 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2530] 2025-10-04 12:06:36,106 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2525] 2025-10-04 12:06:36,106 >>   Num Epochs = 1
[INFO|trainer.py:2531] 2025-10-04 12:06:36,106 >>   Total optimization steps = 1,459
[INFO|trainer.py:2531] 2025-10-04 12:06:36,106 >>   Total optimization steps = 1,459
[INFO|trainer.py:2526] 2025-10-04 12:06:36,106 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-04 12:06:36,106 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-04 12:06:36,106 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-04 12:06:36,106 >>   Total optimization steps = 1,459
[INFO|trainer.py:2523] 2025-10-04 12:06:36,106 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-04 12:06:36,106 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-04 12:06:36,106 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-04 12:06:36,106 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-04 12:06:36,106 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-04 12:06:36,107 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-04 12:06:36,107 >>   Total optimization steps = 1,459
[INFO|trainer.py:2532] 2025-10-04 12:06:36,107 >>   Number of trainable parameters = 3,981,312
[INFO|trainer.py:2532] 2025-10-04 12:06:36,107 >>   Number of trainable parameters = 3,981,312
[INFO|trainer.py:2532] 2025-10-04 12:06:36,107 >>   Number of trainable parameters = 3,981,312
[INFO|trainer.py:2532] 2025-10-04 12:06:36,107 >>   Number of trainable parameters = 3,981,312
[INFO|trainer.py:2532] 2025-10-04 12:06:36,107 >>   Number of trainable parameters = 3,981,312
[INFO|trainer.py:2532] 2025-10-04 12:06:36,108 >>   Number of trainable parameters = 3,981,312
[INFO|trainer.py:2532] 2025-10-04 12:06:36,108 >>   Number of trainable parameters = 3,981,312
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2523] 2025-10-04 12:06:36,314 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-04 12:06:36,314 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-04 12:06:36,314 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-04 12:06:36,314 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-04 12:06:36,314 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-04 12:06:36,314 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-04 12:06:36,314 >>   Total optimization steps = 1,459
[INFO|trainer.py:2532] 2025-10-04 12:06:36,316 >>   Number of trainable parameters = 3,981,312
  0%|          | 0/1459 [00:00<?, ?it/s]/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
  0%|          | 1/1459 [00:32<13:17:27, 32.82s/it]  0%|          | 2/1459 [00:54<10:37:29, 26.25s/it]  0%|          | 3/1459 [01:15<9:43:38, 24.05s/it]   0%|          | 4/1459 [01:44<10:23:44, 25.72s/it]  0%|          | 5/1459 [02:03<9:25:30, 23.34s/it]   0%|          | 6/1459 [02:27<9:32:01, 23.62s/it]  0%|          | 7/1459 [02:47<8:59:42, 22.30s/it]  1%|          | 8/1459 [03:08<8:51:39, 21.98s/it]  1%|          | 9/1459 [03:28<8:37:17, 21.41s/it]  1%|          | 10/1459 [03:50<8:41:22, 21.59s/it]                                                     1%|          | 10/1459 [03:50<8:41:22, 21.59s/it]  1%|          | 11/1459 [04:09<8:25:12, 20.93s/it]  1%|          | 12/1459 [04:29<8:17:40, 20.64s/it]  1%|          | 13/1459 [04:49<8:11:59, 20.41s/it]  1%|          | 14/1459 [05:08<8:02:46, 20.05s/it]  1%|          | 15/1459 [05:29<8:07:54, 20.27s/it]  1%|          | 16/1459 [05:51<8:15:54, 20.62s/it]  1%|          | 17/1459 [06:16<8:51:22, 22.11s/it]  1%|          | 18/1459 [06:37<8:44:10, 21.83s/it]  1%|▏         | 19/1459 [06:59<8:42:20, 21.76s/it]  1%|▏         | 20/1459 [07:23<8:55:10, 22.31s/it]                                                     1%|▏         | 20/1459 [07:23<8:55:10, 22.31s/it]  1%|▏         | 21/1459 [07:40<8:21:40, 20.93s/it]  2%|▏         | 22/1459 [07:57<7:53:11, 19.76s/it]  2%|▏         | 23/1459 [08:16<7:42:42, 19.33s/it]  2%|▏         | 24/1459 [08:43<8:37:12, 21.63s/it]  2%|▏         | 25/1459 [09:04<8:36:32, 21.61s/it]  2%|▏         | 26/1459 [09:28<8:48:01, 22.11s/it]  2%|▏         | 27/1459 [09:56<9:32:10, 23.97s/it]  2%|▏         | 28/1459 [10:14<8:48:31, 22.16s/it]  2%|▏         | 29/1459 [10:34<8:37:16, 21.70s/it]  2%|▏         | 30/1459 [11:01<9:14:30, 23.28s/it]                                                     2%|▏         | 30/1459 [11:01<9:14:30, 23.28s/it]  2%|▏         | 31/1459 [11:27<9:27:30, 23.84s/it]  2%|▏         | 32/1459 [11:49<9:19:49, 23.54s/it]  2%|▏         | 33/1459 [12:07<8:38:23, 21.81s/it]  2%|▏         | 34/1459 [12:29<8:41:18, 21.95s/it]  2%|▏         | 35/1459 [12:48<8:16:19, 20.91s/it]  2%|▏         | 36/1459 [13:05<7:45:45, 19.64s/it]  3%|▎         | 37/1459 [13:23<7:33:05, 19.12s/it]  3%|▎         | 38/1459 [13:42<7:33:15, 19.14s/it]  3%|▎         | 39/1459 [13:59<7:19:11, 18.56s/it]  3%|▎         | 40/1459 [14:20<7:34:06, 19.20s/it]                                                     3%|▎         | 40/1459 [14:20<7:34:06, 19.20s/it]  3%|▎         | 41/1459 [14:40<7:41:53, 19.54s/it]  3%|▎         | 42/1459 [15:01<7:51:24, 19.96s/it]  3%|▎         | 43/1459 [15:20<7:45:30, 19.72s/it]  3%|▎         | 44/1459 [15:36<7:20:03, 18.66s/it]  3%|▎         | 45/1459 [15:58<7:38:55, 19.47s/it]  3%|▎         | 46/1459 [16:19<7:53:18, 20.10s/it]  3%|▎         | 47/1459 [16:39<7:54:19, 20.16s/it]  3%|▎         | 48/1459 [17:00<7:56:52, 20.28s/it]  3%|▎         | 49/1459 [17:19<7:43:55, 19.74s/it]  3%|▎         | 50/1459 [17:41<8:00:31, 20.46s/it]                                                     3%|▎         | 50/1459 [17:41<8:00:31, 20.46s/it]  3%|▎         | 51/1459 [18:01<8:02:27, 20.56s/it]  4%|▎         | 52/1459 [18:19<7:42:35, 19.73s/it]  4%|▎         | 53/1459 [18:39<7:43:24, 19.78s/it]  4%|▎         | 54/1459 [18:56<7:24:58, 19.00s/it]  4%|▍         | 55/1459 [19:17<7:35:34, 19.47s/it]  4%|▍         | 56/1459 [19:37<7:41:32, 19.74s/it]  4%|▍         | 57/1459 [20:05<8:34:13, 22.01s/it]  4%|▍         | 58/1459 [20:27<8:40:00, 22.27s/it]  4%|▍         | 59/1459 [20:50<8:38:25, 22.22s/it]  4%|▍         | 60/1459 [21:06<7:59:28, 20.56s/it]                                                     4%|▍         | 60/1459 [21:06<7:59:28, 20.56s/it]  4%|▍         | 61/1459 [21:24<7:37:53, 19.65s/it]  4%|▍         | 62/1459 [21:45<7:46:03, 20.02s/it]  4%|▍         | 63/1459 [22:06<7:57:40, 20.53s/it]  4%|▍         | 64/1459 [22:27<7:58:56, 20.60s/it]  4%|▍         | 65/1459 [22:49<8:05:29, 20.90s/it]  5%|▍         | 66/1459 [23:11<8:14:40, 21.31s/it]  5%|▍         | 67/1459 [23:28<7:44:22, 20.02s/it]  5%|▍         | 68/1459 [23:47<7:38:05, 19.76s/it]  5%|▍         | 69/1459 [24:08<7:46:31, 20.14s/it]  5%|▍         | 70/1459 [24:32<8:15:13, 21.39s/it]                                                     5%|▍         | 70/1459 [24:32<8:15:13, 21.39s/it]  5%|▍         | 71/1459 [24:49<7:42:53, 20.01s/it]  5%|▍         | 72/1459 [25:07<7:28:24, 19.40s/it]  5%|▌         | 73/1459 [25:28<7:35:03, 19.70s/it]  5%|▌         | 74/1459 [25:49<7:43:03, 20.06s/it]  5%|▌         | 75/1459 [26:08<7:38:55, 19.90s/it]  5%|▌         | 76/1459 [26:27<7:34:16, 19.71s/it]  5%|▌         | 77/1459 [26:49<7:46:54, 20.27s/it]  5%|▌         | 78/1459 [27:08<7:41:50, 20.07s/it]  5%|▌         | 79/1459 [27:27<7:30:10, 19.57s/it]  5%|▌         | 80/1459 [27:47<7:36:27, 19.86s/it]                                                     5%|▌         | 80/1459 [27:47<7:36:27, 19.86s/it]  6%|▌         | 81/1459 [28:09<7:44:54, 20.24s/it]  6%|▌         | 82/1459 [28:25<7:20:46, 19.21s/it]  6%|▌         | 83/1459 [28:42<7:03:23, 18.46s/it]  6%|▌         | 84/1459 [29:03<7:18:05, 19.12s/it]  6%|▌         | 85/1459 [29:25<7:42:22, 20.19s/it]  6%|▌         | 86/1459 [29:50<8:14:21, 21.60s/it]  6%|▌         | 87/1459 [30:14<8:27:47, 22.21s/it]  6%|▌         | 88/1459 [30:40<8:52:48, 23.32s/it]  6%|▌         | 89/1459 [30:59<8:21:06, 21.95s/it]  6%|▌         | 90/1459 [31:20<8:19:19, 21.88s/it]                                                     6%|▌         | 90/1459 [31:20<8:19:19, 21.88s/it]  6%|▌         | 91/1459 [31:37<7:44:46, 20.38s/it]  6%|▋         | 92/1459 [32:01<8:06:02, 21.33s/it]  6%|▋         | 93/1459 [32:27<8:38:17, 22.77s/it]  6%|▋         | 94/1459 [32:49<8:33:34, 22.57s/it]  7%|▋         | 95/1459 [33:11<8:31:07, 22.48s/it]  7%|▋         | 96/1459 [33:26<7:40:17, 20.26s/it]  7%|▋         | 97/1459 [33:46<7:34:22, 20.02s/it]  7%|▋         | 98/1459 [34:05<7:26:23, 19.68s/it]  7%|▋         | 99/1459 [34:26<7:35:35, 20.10s/it]  7%|▋         | 100/1459 [34:46<7:34:26, 20.06s/it]                                                      7%|▋         | 100/1459 [34:46<7:34:26, 20.06s/it]  7%|▋         | 101/1459 [35:04<7:21:58, 19.53s/it]  7%|▋         | 102/1459 [35:31<8:10:47, 21.70s/it]  7%|▋         | 103/1459 [35:48<7:42:10, 20.45s/it]  7%|▋         | 104/1459 [36:09<7:42:28, 20.48s/it]  7%|▋         | 105/1459 [36:27<7:27:44, 19.84s/it]  7%|▋         | 106/1459 [36:51<7:55:49, 21.10s/it]  7%|▋         | 107/1459 [37:11<7:46:04, 20.68s/it]  7%|▋         | 108/1459 [37:32<7:47:33, 20.77s/it]  7%|▋         | 109/1459 [37:53<7:47:24, 20.77s/it]  8%|▊         | 110/1459 [38:16<8:05:17, 21.58s/it]                                                      8%|▊         | 110/1459 [38:16<8:05:17, 21.58s/it]  8%|▊         | 111/1459 [38:41<8:26:31, 22.55s/it]  8%|▊         | 112/1459 [39:01<8:06:51, 21.69s/it]  8%|▊         | 113/1459 [39:18<7:38:44, 20.45s/it]  8%|▊         | 114/1459 [39:43<8:04:28, 21.61s/it]  8%|▊         | 115/1459 [40:09<8:34:33, 22.97s/it]  8%|▊         | 116/1459 [40:30<8:19:49, 22.33s/it]  8%|▊         | 117/1459 [40:53<8:26:39, 22.65s/it]  8%|▊         | 118/1459 [41:12<8:04:30, 21.68s/it]  8%|▊         | 119/1459 [41:33<7:58:56, 21.45s/it]  8%|▊         | 120/1459 [41:52<7:42:27, 20.72s/it]                                                      8%|▊         | 120/1459 [41:52<7:42:27, 20.72s/it]  8%|▊         | 121/1459 [42:13<7:43:45, 20.80s/it]  8%|▊         | 122/1459 [42:32<7:30:55, 20.24s/it]  8%|▊         | 123/1459 [42:50<7:12:01, 19.40s/it]  8%|▊         | 124/1459 [43:14<7:47:06, 20.99s/it]  9%|▊         | 125/1459 [43:38<8:01:54, 21.68s/it]  9%|▊         | 126/1459 [43:59<7:59:33, 21.59s/it]  9%|▊         | 127/1459 [44:19<7:47:03, 21.04s/it]  9%|▉         | 128/1459 [44:39<7:44:50, 20.95s/it]  9%|▉         | 129/1459 [44:56<7:14:43, 19.61s/it]  9%|▉         | 130/1459 [45:17<7:22:57, 20.00s/it]                                                      9%|▉         | 130/1459 [45:17<7:22:57, 20.00s/it]  9%|▉         | 131/1459 [45:40<7:45:08, 21.02s/it]  9%|▉         | 132/1459 [46:03<7:54:31, 21.46s/it]  9%|▉         | 133/1459 [46:29<8:22:59, 22.76s/it]  9%|▉         | 134/1459 [46:49<8:07:48, 22.09s/it]  9%|▉         | 135/1459 [47:13<8:17:29, 22.55s/it]  9%|▉         | 136/1459 [47:34<8:12:06, 22.32s/it]  9%|▉         | 137/1459 [47:53<7:47:50, 21.23s/it]  9%|▉         | 138/1459 [48:13<7:36:48, 20.75s/it] 10%|▉         | 139/1459 [48:30<7:15:09, 19.78s/it] 10%|▉         | 140/1459 [48:54<7:37:20, 20.80s/it]                                                     10%|▉         | 140/1459 [48:54<7:37:20, 20.80s/it] 10%|▉         | 141/1459 [49:12<7:22:53, 20.16s/it] 10%|▉         | 142/1459 [49:31<7:10:55, 19.63s/it] 10%|▉         | 143/1459 [50:02<8:31:25, 23.32s/it] 10%|▉         | 144/1459 [50:23<8:12:42, 22.48s/it] 10%|▉         | 145/1459 [50:48<8:25:36, 23.09s/it] 10%|█         | 146/1459 [51:08<8:10:43, 22.42s/it] 10%|█         | 147/1459 [51:28<7:54:05, 21.68s/it] 10%|█         | 148/1459 [51:51<7:57:09, 21.84s/it] 10%|█         | 149/1459 [52:13<7:59:53, 21.98s/it] 10%|█         | 150/1459 [52:40<8:35:33, 23.63s/it]                                                     10%|█         | 150/1459 [52:40<8:35:33, 23.63s/it] 10%|█         | 151/1459 [52:58<7:54:57, 21.79s/it] 10%|█         | 152/1459 [53:19<7:52:44, 21.70s/it] 10%|█         | 153/1459 [53:40<7:42:44, 21.26s/it] 11%|█         | 154/1459 [53:59<7:33:25, 20.85s/it] 11%|█         | 155/1459 [54:18<7:19:10, 20.21s/it] 11%|█         | 156/1459 [54:36<7:05:29, 19.59s/it] 11%|█         | 157/1459 [54:57<7:09:37, 19.80s/it] 11%|█         | 158/1459 [55:14<6:50:56, 18.95s/it] 11%|█         | 159/1459 [55:32<6:47:54, 18.83s/it] 11%|█         | 160/1459 [55:55<7:13:07, 20.01s/it]                                                     11%|█         | 160/1459 [55:55<7:13:07, 20.01s/it] 11%|█         | 161/1459 [56:13<7:01:29, 19.48s/it] 11%|█         | 162/1459 [56:32<6:55:38, 19.23s/it] 11%|█         | 163/1459 [56:51<6:55:41, 19.25s/it] 11%|█         | 164/1459 [57:09<6:45:53, 18.81s/it] 11%|█▏        | 165/1459 [57:35<7:30:30, 20.89s/it] 11%|█▏        | 166/1459 [57:54<7:19:33, 20.40s/it] 11%|█▏        | 167/1459 [58:15<7:25:18, 20.68s/it] 12%|█▏        | 168/1459 [58:36<7:24:50, 20.67s/it] 12%|█▏        | 169/1459 [58:55<7:14:19, 20.20s/it] 12%|█▏        | 170/1459 [59:18<7:29:34, 20.93s/it]                                                     12%|█▏        | 170/1459 [59:18<7:29:34, 20.93s/it] 12%|█▏        | 171/1459 [59:37<7:19:01, 20.45s/it] 12%|█▏        | 172/1459 [59:58<7:22:26, 20.63s/it] 12%|█▏        | 173/1459 [1:00:21<7:36:24, 21.29s/it] 12%|█▏        | 174/1459 [1:00:37<7:01:56, 19.70s/it] 12%|█▏        | 175/1459 [1:01:07<8:08:25, 22.82s/it] 12%|█▏        | 176/1459 [1:01:27<7:50:46, 22.02s/it] 12%|█▏        | 177/1459 [1:01:47<7:34:36, 21.28s/it] 12%|█▏        | 178/1459 [1:02:06<7:21:44, 20.69s/it] 12%|█▏        | 179/1459 [1:02:26<7:18:43, 20.57s/it] 12%|█▏        | 180/1459 [1:02:54<8:05:46, 22.79s/it]                                                       12%|█▏        | 180/1459 [1:02:54<8:05:46, 22.79s/it] 12%|█▏        | 181/1459 [1:03:20<8:25:57, 23.75s/it] 12%|█▏        | 182/1459 [1:03:38<7:49:29, 22.06s/it] 13%|█▎        | 183/1459 [1:03:58<7:34:35, 21.38s/it] 13%|█▎        | 184/1459 [1:04:19<7:30:40, 21.21s/it] 13%|█▎        | 185/1459 [1:04:43<7:46:32, 21.97s/it] 13%|█▎        | 186/1459 [1:04:59<7:13:07, 20.41s/it] 13%|█▎        | 187/1459 [1:05:16<6:48:12, 19.25s/it] 13%|█▎        | 188/1459 [1:05:39<7:10:52, 20.34s/it] 13%|█▎        | 189/1459 [1:05:59<7:10:42, 20.35s/it] 13%|█▎        | 190/1459 [1:06:25<7:48:06, 22.13s/it]                                                       13%|█▎        | 190/1459 [1:06:25<7:48:06, 22.13s/it] 13%|█▎        | 191/1459 [1:06:43<7:17:14, 20.69s/it] 13%|█▎        | 192/1459 [1:07:01<7:02:17, 20.00s/it] 13%|█▎        | 193/1459 [1:07:22<7:05:18, 20.16s/it] 13%|█▎        | 194/1459 [1:07:44<7:19:38, 20.85s/it] 13%|█▎        | 195/1459 [1:08:08<7:40:09, 21.84s/it] 13%|█▎        | 196/1459 [1:08:26<7:15:09, 20.67s/it] 14%|█▎        | 197/1459 [1:08:49<7:28:17, 21.31s/it] 14%|█▎        | 198/1459 [1:09:08<7:12:13, 20.57s/it] 14%|█▎        | 199/1459 [1:09:30<7:23:28, 21.12s/it] 14%|█▎        | 200/1459 [1:09:50<7:14:28, 20.71s/it]                                                       14%|█▎        | 200/1459 [1:09:50<7:14:28, 20.71s/it] 14%|█▍        | 201/1459 [1:10:07<6:52:22, 19.67s/it] 14%|█▍        | 202/1459 [1:10:29<7:04:57, 20.28s/it] 14%|█▍        | 203/1459 [1:10:48<6:55:13, 19.84s/it] 14%|█▍        | 204/1459 [1:11:13<7:27:21, 21.39s/it] 14%|█▍        | 205/1459 [1:11:34<7:22:59, 21.20s/it] 14%|█▍        | 206/1459 [1:11:50<6:55:10, 19.88s/it] 14%|█▍        | 207/1459 [1:12:07<6:37:10, 19.03s/it] 14%|█▍        | 208/1459 [1:12:27<6:42:09, 19.29s/it] 14%|█▍        | 209/1459 [1:12:45<6:32:13, 18.83s/it] 14%|█▍        | 210/1459 [1:13:04<6:31:00, 18.78s/it]                                                       14%|█▍        | 210/1459 [1:13:04<6:31:00, 18.78s/it] 14%|█▍        | 211/1459 [1:13:25<6:43:56, 19.42s/it] 15%|█▍        | 212/1459 [1:13:41<6:25:13, 18.54s/it] 15%|█▍        | 213/1459 [1:14:01<6:35:52, 19.06s/it] 15%|█▍        | 214/1459 [1:14:21<6:36:07, 19.09s/it] 15%|█▍        | 215/1459 [1:14:43<6:56:19, 20.08s/it] 15%|█▍        | 216/1459 [1:15:00<6:39:36, 19.29s/it] 15%|█▍        | 217/1459 [1:15:21<6:48:15, 19.72s/it] 15%|█▍        | 218/1459 [1:15:40<6:40:23, 19.36s/it] 15%|█▌        | 219/1459 [1:16:01<6:53:09, 19.99s/it] 15%|█▌        | 220/1459 [1:16:23<7:01:45, 20.42s/it]                                                       15%|█▌        | 220/1459 [1:16:23<7:01:45, 20.42s/it] 15%|█▌        | 221/1459 [1:16:41<6:51:13, 19.93s/it] 15%|█▌        | 222/1459 [1:17:00<6:45:06, 19.65s/it] 15%|█▌        | 223/1459 [1:17:23<7:04:22, 20.60s/it] 15%|█▌        | 224/1459 [1:17:41<6:45:35, 19.70s/it] 15%|█▌        | 225/1459 [1:17:59<6:38:59, 19.40s/it] 15%|█▌        | 226/1459 [1:18:21<6:53:18, 20.11s/it] 16%|█▌        | 227/1459 [1:18:40<6:42:12, 19.59s/it] 16%|█▌        | 228/1459 [1:18:57<6:27:29, 18.89s/it] 16%|█▌        | 229/1459 [1:19:16<6:28:39, 18.96s/it] 16%|█▌        | 230/1459 [1:19:38<6:48:50, 19.96s/it]                                                       16%|█▌        | 230/1459 [1:19:38<6:48:50, 19.96s/it] 16%|█▌        | 231/1459 [1:20:02<7:10:06, 21.02s/it] 16%|█▌        | 232/1459 [1:20:19<6:46:37, 19.88s/it] 16%|█▌        | 233/1459 [1:20:37<6:33:51, 19.28s/it] 16%|█▌        | 234/1459 [1:20:55<6:24:54, 18.85s/it] 16%|█▌        | 235/1459 [1:21:12<6:18:04, 18.53s/it] 16%|█▌        | 236/1459 [1:21:30<6:09:52, 18.15s/it] 16%|█▌        | 237/1459 [1:21:48<6:09:54, 18.16s/it] 16%|█▋        | 238/1459 [1:22:11<6:37:37, 19.54s/it] 16%|█▋        | 239/1459 [1:22:28<6:22:57, 18.83s/it] 16%|█▋        | 240/1459 [1:22:46<6:18:38, 18.64s/it]                                                       16%|█▋        | 240/1459 [1:22:46<6:18:38, 18.64s/it] 17%|█▋        | 241/1459 [1:23:08<6:36:53, 19.55s/it] 17%|█▋        | 242/1459 [1:23:26<6:31:23, 19.30s/it] 17%|█▋        | 243/1459 [1:23:48<6:44:35, 19.96s/it] 17%|█▋        | 244/1459 [1:24:09<6:52:51, 20.39s/it] 17%|█▋        | 245/1459 [1:24:30<6:51:20, 20.33s/it] 17%|█▋        | 246/1459 [1:24:52<7:04:16, 20.99s/it] 17%|█▋        | 247/1459 [1:25:11<6