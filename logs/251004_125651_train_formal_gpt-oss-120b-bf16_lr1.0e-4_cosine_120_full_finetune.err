+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=0 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 16000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
++ head -n 1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=2 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 16000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
++ head -n 1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=5 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 16000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=1 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 16000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=7 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 16000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=3 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 16000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=6 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 16000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=4 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 16000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
W1004 12:57:32.905000 3453486 site-packages/torch/distributed/run.py:774] 
W1004 12:57:32.905000 3453486 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:32.905000 3453486 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:57:32.905000 3453486 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:32.905000 15177 site-packages/torch/distributed/run.py:774] 
W1004 12:57:32.905000 15177 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:32.905000 15177 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:57:32.905000 15177 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:32.906000 3432705 site-packages/torch/distributed/run.py:774] 
W1004 12:57:32.906000 3432705 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:32.906000 3432705 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:57:32.906000 3432705 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:32.908000 1654425 site-packages/torch/distributed/run.py:774] 
W1004 12:57:32.908000 1654425 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:32.908000 1654425 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:57:32.908000 1654425 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:32.946000 3297317 site-packages/torch/distributed/run.py:774] 
W1004 12:57:32.946000 3297317 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:32.946000 3297317 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:57:32.946000 3297317 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:33.022000 2478637 site-packages/torch/distributed/run.py:774] 
W1004 12:57:33.022000 2478637 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:33.022000 2478637 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:57:33.022000 2478637 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:33.286000 191548 site-packages/torch/distributed/run.py:774] 
W1004 12:57:33.286000 191548 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:33.286000 191548 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:57:33.286000 191548 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:33.440000 3535733 site-packages/torch/distributed/run.py:774] 
W1004 12:57:33.440000 3535733 site-packages/torch/distributed/run.py:774] *****************************************
W1004 12:57:33.440000 3535733 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 12:57:33.440000 3535733 site-packages/torch/distributed/run.py:774] *****************************************
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,156 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,156 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,156 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,156 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,156 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,156 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,162 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,162 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,162 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,162 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,162 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,162 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,164 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,164 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,164 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,164 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,164 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,164 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,165 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,165 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,165 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,165 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,165 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,165 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,167 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,167 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,167 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,167 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,167 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,167 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,173 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,173 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,173 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,173 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,173 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,173 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,177 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,177 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,177 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,177 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,177 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,177 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,195 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,195 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,195 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,195 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,195 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,195 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:02,827 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:58:02,828 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 12:58:02,832 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,832 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,832 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,832 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,832 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,832 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,832 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:02,836 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:58:02,836 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:02,838 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:839] 2025-10-04 12:58:02,839 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-04 12:58:02,839 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,839 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,839 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,839 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,839 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,839 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,839 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:02,840 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:58:02,840 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 12:58:02,842 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,843 >> loading file tokenizer.json
[INFO|configuration_utils.py:839] 2025-10-04 12:58:02,843 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,843 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,843 >> loading file added_tokens.json
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,843 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,843 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,843 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,843 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,843 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,843 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,843 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,843 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,843 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:02,846 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:58:02,847 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:02,847 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:58:02,848 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 12:58:02,849 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,849 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,849 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,849 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,849 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,849 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,849 >> loading file chat_template.jinja
[INFO|configuration_utils.py:839] 2025-10-04 12:58:02,850 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,850 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,850 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,850 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,850 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,850 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,850 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:02,863 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:58:02,864 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:02,865 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 12:58:02,866 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 12:58:02,866 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,867 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,867 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,867 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,867 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,867 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,867 >> loading file chat_template.jinja
[INFO|configuration_utils.py:839] 2025-10-04 12:58:02,868 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,869 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,869 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,869 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,869 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,869 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 12:58:02,869 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:03,530 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:03,544 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:03,545 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:03,551 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:03,552 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:03,568 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:03,574 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 12:58:03,597 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 416/186688 [00:01<11:18, 274.64 examples/s]Converting format of dataset (num_proc=64):  12%|█▏        | 21814/186688 [00:01<00:08, 18506.44 examples/s]Converting format of dataset (num_proc=64):  24%|██▍       | 45457/186688 [00:01<00:03, 42127.53 examples/s]Converting format of dataset (num_proc=64):  39%|███▉      | 73269/186688 [00:01<00:01, 73429.71 examples/s]Converting format of dataset (num_proc=64):  51%|█████     | 94645/186688 [00:01<00:00, 95678.40 examples/s]Converting format of dataset (num_proc=64):  64%|██████▍   | 119739/186688 [00:02<00:00, 124512.74 examples/s]Converting format of dataset (num_proc=64):  76%|███████▌  | 142231/186688 [00:02<00:00, 136435.25 examples/s]Converting format of dataset (num_proc=64):  87%|████████▋ | 162904/186688 [00:02<00:00, 1Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 531/186688 [00:01<08:52, 349.90 examples/s]Converting format of dataset (num_proc=64):   8%|▊         | 14180/186688 [00:01<00:14, 12034.93 examples/s]Converting format of dataset (num_proc=64):  22%|██▏       | 41442/186688 [00:01<00:03, 40456.52 examples/s]Converting format of dataset (num_proc=64):  37%|███▋      | 69897/186688 [00:01<00:01, 73309.45 examples/s]Converting format of dataset (num_proc=64):  49%|████▉     | 91940/186688 [00:01<00:00, 96955.57 examples/s]Converting format of dataset (num_proc=64):  63%|██████▎   | 117224/186688 [00:02<00:00, 126100.27 examples/s]Converting format of dataset (num_proc=64):  75%|███████▌  | 140074/186688 [00:02<00:00, 135442.90 examples/s]Converting format of dataset (num_proc=64):  86%|████████▌ | 160666/186688 [00:02<00:00, 145Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 517/186688 [00:01<09:17, 333.80 examples/s]Converting format of dataset (num_proc=64):   3%|▎         | 5965/186688 [00:01<00:36, 4894.06 examples/s]Converting format of dataset (num_proc=64):  15%|█▌        | 28318/186688 [00:01<00:05, 28346.68 examples/s]Converting format of dataset (num_proc=64):  29%|██▉       | 55061/186688 [00:01<00:02, 59845.92 examples/s]Converting format of dataset (num_proc=64):  39%|███▉      | 73692/186688 [00:01<00:01, 79727.19 examples/s]Converting format of dataset (num_proc=64):  56%|█████▌    | 104231/186688 [00:02<00:00, 121374.78 examples/s]Converting format of dataset (num_proc=64):  68%|██████▊   | 126554/186688 [00:02<00:00, 133875.19 examples/s]Converting format of dataset (num_proc=64):  80%|████████  | 150143/186688 [00:02<00:00, 156089.34 exampleConverting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 551/186688 [00:01<09:17, 334.01 examples/s]Converting format of dataset (num_proc=64):   2%|▏         | 3527/186688 [00:01<01:08, 2664.32 examples/s]Converting format of dataset (num_proc=64):  19%|█▉        | 35533/186688 [00:01<00:04, 34934.80 examples/s]Converting format of dataset (num_proc=64):  29%|██▉       | 54517/186688 [00:01<00:02, 54573.30 examples/s]Converting format of dataset (num_proc=64):  44%|████▍     | 82110/186688 [00:02<00:01, 88418.48 examples/s]Converting format of dataset (num_proc=64):  56%|█████▌    | 104242/186688 [00:02<00:00, 111615.76 examples/s]Converting format of dataset (num_proc=64):  67%|██████▋   | 125644/186688 [00:02<00:00, 132409.41 examples/s]Converting format of dataset (num_proc=64):  79%|███████▉  | 148157/186688 [00:02<00:00, 153015.11 exampConverting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 324/186688 [00:01<15:51, 195.87 examples/s]Converting format of dataset (num_proc=64):   6%|▌         | 10317/186688 [00:01<00:21, 8123.93 examples/s]Converting format of dataset (num_proc=64):  17%|█▋        | 31909/186688 [00:01<00:05, 29311.87 examples/s]Converting format of dataset (num_proc=64):  30%|██▉       | 55968/186688 [00:01<00:02, 56112.23 examples/s]Converting format of dataset (num_proc=64):  43%|████▎     | 80855/186688 [00:02<00:01, 85366.85 examples/s]Converting format of dataset (num_proc=64):  54%|█████▍    | 100687/186688 [00:02<00:00, 103499.83 examples/s]Converting format of dataset (num_proc=64):  65%|██████▍   | 121071/186688 [00:02<00:00, 123077.60 examples/s]Converting format of dataset (num_proc=64):  77%|███████▋  | 144337/186688 [00:02<00:00, 147136.96 examConverting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 462/186688 [00:01<10:52, 285.35 examples/s]Converting format of dataset (num_proc=64):   5%|▌         | 10049/186688 [00:01<00:22, 8009.88 examples/s]Converting format of dataset (num_proc=64):  24%|██▍       | 44552/186688 [00:01<00:03, 42447.91 examples/s]Converting format of dataset (num_proc=64):  35%|███▌      | 65737/186688 [00:01<00:01, 64046.86 examples/s]Converting format of dataset (num_proc=64):  48%|████▊     | 90188/186688 [00:02<00:01, 91815.69 examples/s]Converting format of dataset (num_proc=64):  61%|██████▏   | 114687/186688 [00:02<00:00, 119321.33 examples/s]Converting format of dataset (num_proc=64):  73%|███████▎  | 137122/186688 [00:02<00:00, 135314.28 examples/s]Converting format of dataset (num_proc=64):  85%|████████▍ | 158124/186688 [00:02<00:00, 1479Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 543/186688 [00:01<09:13, 336.12 examples/s]Converting format of dataset (num_proc=64):   9%|▊         | 16115/186688 [00:01<00:13, 12867.07 examples/s]Converting format of dataset (num_proc=64):  25%|██▌       | 47260/186688 [00:01<00:03, 43620.99 examples/s]Converting format of dataset (num_proc=64):  37%|███▋      | 69086/186688 [00:01<00:01, 66018.55 examples/s]Converting format of dataset (num_proc=64):  50%|████▉     | 93088/186688 [00:02<00:01, 92697.92 examples/s]Converting format of dataset (num_proc=64):  64%|██████▍   | 119579/186688 [00:02<00:00, 123791.17 examples/s]Converting format of dataset (num_proc=64):  76%|███████▋  | 142815/186688 [00:02<00:00, 139462.24 examples/s]Converting format of dataset (num_proc=64):  88%|████████▊ | 164620/186688 [00:02<00:00, 145Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 536/186688 [00:01<08:50, 350.94 examples/s]Converting format of dataset (num_proc=64):   6%|▌         | 11519/186688 [00:01<00:18, 9712.16 examples/s]Converting format of dataset (num_proc=64):  18%|█▊        | 33921/186688 [00:01<00:04, 33037.27 examples/s]Converting format of dataset (num_proc=64):  32%|███▏      | 59041/186688 [00:01<00:02, 62379.67 examples/s]Converting format of dataset (num_proc=64):  44%|████▎     | 81354/186688 [00:01<00:01, 88280.95 examples/s]Converting format of dataset (num_proc=64):  58%|█████▊    | 108635/186688 [00:02<00:00, 122624.37 examples/s]Converting format of dataset (num_proc=64):  70%|███████   | 130906/186688 [00:02<00:00, 90743.90 examples/s] Converting format of dataset (num_proc=64):  79%|███████▉  | 147958/186688 [00:03<00:00, 43344.90 exa44614.64 examples/s]Converting format of dataset (num_proc=64):  98%|█████████▊| 182323/186688 [00:02<00:00, 105005.40 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:03<00:00, 47747.57 examples/s] 
546.94 examples/s]Converting format of dataset (num_proc=64):  97%|█████████▋| 180529/186688 [00:02<00:00, 107353.99 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 46291.76 examples/s] 
718.52 examples/s]Converting format of dataset (num_proc=64):  99%|█████████▉| 184604/186688 [00:02<00:00, 103517.63 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 45295.99 examples/s] 
27.61 examples/s]Converting format of dataset (num_proc=64):  96%|█████████▌| 178395/186688 [00:02<00:00, 120753.80 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 45426.95 examples/s] 
les/s]Converting format of dataset (num_proc=64):  91%|█████████ | 169642/186688 [00:02<00:00, 151146.75 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 45112.06 examples/s] 
s/s]Converting format of dataset (num_proc=64):  92%|█████████▏| 171670/186688 [00:02<00:00, 141568.82 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 44998.60 examples/s] 
ples/s]Converting format of dataset (num_proc=64):  88%|████████▊ | 164978/186688 [00:02<00:00, 156610.57 examples/s]Converting format of dataset (num_proc=64):  99%|█████████▉| 184938/186688 [00:02<00:00, 98693.93 examples/s] Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 44494.25 examples/s]
mples/s]Converting format of dataset (num_proc=64):  86%|████████▌ | 160360/186688 [00:03<00:00, 39202.47 examples/s]Converting format of dataset (num_proc=64):  99%|█████████▉| 184725/186688 [00:03<00:00, 57160.01 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 42901.82 examples/s]
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<42:00, 73.68 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:13<17:57, 171.33 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:14<10:26, 293.20 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<06:55, 439.70 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:15<05:00, 605.14 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▍         | 8000/186688 [00:16<02:15, 1321.98 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:16<01:40, 1750.20 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▌         | 11000/186688 [00:17<01:38, 1785.31 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 13Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<41:46, 74.07 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:13<11:14, 272.17 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:14<07:48, 389.58 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:14<05:41, 532.15 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 6000/186688 [00:15<04:17, 702.58 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:16<03:37, 827.82 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:16<02:21, 1252.50 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:17<02:06, 1393.26 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:14<43:57, 70.41 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<18:50, 163.32 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<07:25, 410.06 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 6000/186688 [00:15<04:15, 706.15 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:16<03:32, 844.68 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:16<02:12, 1340.21 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▌         | 11000/186688 [00:16<01:37, 1792.88 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:17<01:06, 2599.33 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<43:13, 71.60 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<19:05, 161.21 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:16<08:17, 367.47 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:16<05:56, 510.20 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 6000/186688 [00:17<04:51, 619.96 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:17<03:35, 833.98 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:17<02:04, 1422.89 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:17<01:09, 2525.61 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 1300Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:14<44:01, 70.31 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:14<11:52, 257.84 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<08:17, 367.03 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:15<06:02, 501.50 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:16<02:20, 1261.36 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:17<01:35, 1835.56 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 13000/186688 [00:17<01:32, 1881.56 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:17<01:31, 1879.61 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<42:31, 72.77 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<19:14, 159.95 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:15<05:44, 527.87 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▍         | 8000/186688 [00:16<03:17, 903.17 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▌         | 11000/186688 [00:16<02:05, 1395.64 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:17<01:57, 1480.60 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         | 15000/186688 [00:17<01:19, 2157.49 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         | 17000/186688 [00:18<01:08, 2481.99 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         |Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<40:39, 76.11 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<18:25, 167.07 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<07:48, 390.09 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:16<06:21, 476.77 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:16<03:40, 813.67 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:17<02:04, 1423.77 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:17<01:13, 2350.86 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:18<00:51, 3284.93 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 2Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:14<45:43, 67.69 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:15<20:23, 150.90 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:16<08:00, 380.21 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:16<02:16, 1291.54 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:16<01:23, 2069.86 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:17<01:18, 2186.02 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         | 17000/186688 [00:18<01:18, 2159.30 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:19<01:40, 1685.73 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█        0/186688 [00:18<01:04, 2696.37 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:18<00:48, 3553.09 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         | 19000/186688 [00:19<00:40, 4110.89 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 20000/186688 [00:19<00:48, 3419.87 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:19<00:36, 4545.58 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:20<00:39, 4136.19 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:20<00:38, 4239.10 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:20<00:38, 4206.54 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:21<00:41, 3856.90 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:21<00:39, | 19000/186688 [00:19<01:27, 1926.44 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:19<01:02, 2666.11 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:20<00:52, 3092.48 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:20<00:40, 3943.79 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:20<00:37, 4255.31 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:20<00:29, 5445.47 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:21<00:39, 4038.13 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:21<00:36, 4258.65 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:21<00:33, 4663.84 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [0000/186688 [00:17<01:30, 1931.70 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:18<01:11, 2402.23 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:18<01:01, 2763.65 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:19<00:26, 6199.44 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:19<00:31, 5146.80 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:20<00:32, 4851.59 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:20<00:39, 4032.59 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:21<00:45, 3432.94 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:21<00:42, 3702.35 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:22<00000/186688 [00:17<00:58, 2915.15 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         | 17000/186688 [00:18<01:03, 2688.29 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:19<00:44, 3659.88 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:19<00:43, 3779.99 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:20<00:42, 3823.09 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:20<00:47, 3343.84 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:21<00:53, 2972.71 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:22<00:53, 2950.59 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:22<00:45, 3371.47 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:22<| 15000/186688 [00:18<01:31, 1886.22 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:19<01:00, 2768.72 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         | 19000/186688 [00:19<00:57, 2895.89 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:19<00:50, 3261.33 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:20<00:47, 3436.62 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:20<00:45, 3545.43 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:21<00:45, 3528.27 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:21<00:52, 3023.58 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:22<00:59, 2647.76 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:23<000/186688 [00:17<01:17, 2246.09 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         | 15000/186688 [00:18<01:05, 2630.18 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:18<01:09, 2461.28 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:19<00:59, 2813.46 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 20000/186688 [00:19<00:54, 3079.23 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:20<00:42, 3836.32 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:20<00:35, 4491.95 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:21<00:32, 4865.95 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:23<01:03, 2448.76 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:23<01:08, 18000/186688 [00:19<01:24, 1995.93 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:19<00:59, 2763.42 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:20<00:37, 4321.69 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:20<00:37, 4273.89 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:21<00:37, 4197.99 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:21<00:43, 3620.62 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:22<00:35, 4274.88 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:24<01:22, 1833.94 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:25<01:41, 1477.74 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:1000/186688 [00:19<00:53, 3121.12 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:19<00:44, 3635.84 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:20<00:35, 4507.61 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:20<00:36, 4343.89 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:22<00:52, 2966.72 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:25<02:05, 1235.08 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:26<01:58, 1289.89 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:27<01:26, 1743.54 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:27<01:18, 1896.01 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:01:25, 1842.82 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:23<01:07, 2299.29 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:24<01:26, 1787.18 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:25<01:19, 1926.64 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:25<00:49, 3048.13 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:25<00:45, 3316.74 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:26<00:59, 2509.31 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:27<01:20, 1827.96 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:27<01:15, 1947.04 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:29<01:40, 1433.0:21<00:30, 5053.53 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:22<00:57, 2674.79 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:23<00:46, 3289.92 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:26<02:40, 941.75 examples/s] Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:27<02:38, 942.88 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:29<02:48, 879.92 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:29<02:19, 1060.07 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:30<02:23, 1022.32 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:30<02:00, 1212.66 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:31<01:43, :46, 3325.17 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:22<00:41, 3694.66 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:24<01:39, 1530.20 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:28<03:40, 687.07 examples/s] Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:29<03:13, 777.70 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:29<02:50, 877.14 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:30<01:45, 1401.76 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:30<01:51, 1313.46 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:31<01:29, 1633.01 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:31<01:16, 1899.85 4029.07 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:21<00:27, 5593.49 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:23<00:59, 2587.41 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:23<01:10, 2185.75 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:26<02:48, 908.80 examples/s] Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:28<02:55, 863.05 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:29<02:51, 877.07 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:29<02:16, 1100.40 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:30<01:57, 1263.95 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:31<02:37, 939.31 examples/s00:48, 3175.65 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:23<01:02, 2437.94 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:24<01:29, 1701.46 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:25<01:26, 1748.73 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:27<02:30, 992.86 examples/s] Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:28<02:30, 988.27 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:30<02:50, 863.68 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:30<02:10, 1121.50 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:31<02:00, 1212.54 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:32<01:54, 1259. 2274.66 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:24<01:01, 2519.32 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:24<00:40, 3690.81 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:26<01:25, 1756.46 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:26<01:08, 2142.60 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:27<01:02, 2338.20 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:30<02:39, 915.53 examples/s] Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:31<02:36, 924.70 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:32<02:21, 1017.42 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:33<02:25, 981.2527<02:28, 1011.03 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:28<02:19, 1063.41 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:28<01:48, 1357.28 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:29<01:52, 1308.35 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:32<03:22, 718.79 examples/s] Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:32<02:29, 967.14 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:33<01:54, 1257.25 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:33<01:54, 1248.12 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:34<01:32, 1529.17 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:34<28<01:47, 1382.47 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:29<01:49, 1349.82 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:30<01:25, 1703.05 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:30<01:20, 1799.81 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:31<01:27, 1641.01 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:32<01:24, 1686.39 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<01:06, 2138.56 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:33<01:27, 1612.17 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<01:09, 2012.80 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:45 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:32<01:04, 2189.30 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:33<00:45, 3021.82 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:33<00:42, 3256.98 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:33<00:36, 3694.78 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:33<00:38, 3521.81 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:34<00:47, 2815.03 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:34<00:55, 2424.19 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:34<00:43, 3024.86 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:35<00:43, 305 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:31<01:02, 2293.55 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:32<01:16, 1859.60 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<01:12, 1962.11 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:33<01:16, 1832.57 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<00:58, 2392.40 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:33<00:52, 2643.72 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:34<00:47, 2887.57 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:34<01:02, 2188.70 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:35<01:04, 2099.1392.80 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:31<01:02, 2288.86 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<01:22, 1715.55 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:33<01:46, 1316.56 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:34<01:21, 1718.32 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:34<01:10, 1974.87 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:34<01:07, 2040.88 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:35<00:59, 2288.49 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:35<00:52, 2579.62 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:35<00:41] Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:32<02:08, 1145.27 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:33<02:13, 1087.83 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:33<01:54, 1262.31 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:33<01:28, 1621.08 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:34<01:00, 2333.59 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:34<00:51, 2732.86 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:35<01:11, 1967.27 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:35<01:00, 2276.83 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:35<00:51, 2683.54 example79 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:30<01:50, 1306.15 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:31<01:35, 1488.86 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:33<02:33, 923.05 examples/s] Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:34<02:43, 861.91 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:35<02:07, 1096.96 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:35<01:15, 1825.91 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:36<01:29, 1522.82 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:36<01:18, 1723.59 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:36<01:01, 219900:54, 2542.73 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:35<01:09, 1996.35 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:36<01:25, 1618.34 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:36<01:08, 1994.49 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:36<00:42, 3190.49 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<00:35, 3714.53 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:36<00:33, 3987.63 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:37<00:32, 3991.46 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:37<00:38, 3374.93 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:3 examples/s] Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:34<02:00, 1177.43 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:36<02:50, 826.65 examples/s] Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:36<01:25, 1604.90 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:36<01:11, 1916.35 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:36<00:50, 2655.63 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:37<00:49, 2693.86 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:38<00:49, 2656.08 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:38<00:47, 2750.11 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:38<00:40, 320140 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:36<01:12, 1845.37 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:36<00:38, 3441.08 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:36<00:32, 3974.86 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:36<00:33, 3857.74 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:36<00:20, 6314.20 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:37<00:19, 6377.73 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:37<00:21, 5816.63 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:37<00:27, 4494.00 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:38<34<01:45, 1317.33 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:35<01:10, 1925.67 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:35<01:05, 2065.30 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:35<00:43, 3098.50 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:37<01:09, 1896.28 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:37<00:38, 3325.47 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:37<00:39, 3240.74 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:38<00:42, 3030.58 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:38<00:44, 2845.50 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186s/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:36<01:05, 2093.94 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:36<00:53, 2517.73 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:37<00:51, 2599.61 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:37<00:49, 2690.11 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:37<00:46, 2839.82 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:38<00:45, 2915.88 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:38<01:03, 2069.98 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:39<00:49, 2598.45 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:39<01:03, 2032.80 examp.05 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<00:48, 2768.12 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:37<00:39, 3336.74 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:37<00:33, 3884.59 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:37<00:37, 3470.55 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:38<00:38, 3353.27 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:38<00:41, 3056.01 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:39<00:40, 3073.94 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61917/186688 [00:39<00:40, 3075.23 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 62917/186688 [00:39<07.31 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:36<01:00, 2142.75 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:36<00:46, 2774.30 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:37<01:13, 1743.76 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:37<01:02, 2058.85 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:38<01:25, 1481.89 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:38<01:10, 1788.72 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:39<01:11, 1742.14 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:40<00:54, 2259.50 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00, 3284.33 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<00:57, 2305.13 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:36<00:52, 2549.36 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:36<00:42, 3126.24 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:36<00:36, 3616.47 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:37<01:05, 1968.15 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:39<01:56, 1106.42 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:40<01:25, 1484.56 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:40<01:06, 1879.84 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:.61 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:39<00:36, 3502.86 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:39<00:45, 2791.40 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:39<00:37, 3331.08 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:40<00:37, 3320.63 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:40<00:53, 2307.09 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:40<00:32, 3697.01 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:41<00:29, 4136.26 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:41<00:25, 4722.89 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/18668688 [00:38<00:40, 3135.75 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:39<00:56, 2205.22 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:39<00:30, 3966.97 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:39<00:27, 4424.51 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:39<00:24, 4825.57 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:40<00:36, 3254.63 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69917/186688 [00:40<00:25, 4497.00 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 71917/186688 [00:41<00:26, 4340.72 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 72917/186688 [00:41<00:35, 3224.99 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███8<00:52, 2436.46 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:38<00:53, 2384.01 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:39<00:49, 2584.73 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:40<00:56, 2194.07 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:40<00:40, 2976.36 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:41<00:36, 3286.86 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:41<00:24, 4777.56 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:42<00:22, 4996.70 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:42<00:21, 5369.19 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      les/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:40<00:53, 2367.63 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:40<00:41, 3069.36 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:40<00:43, 2898.00 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:40<00:37, 3339.49 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:41<00:40, 2992.70 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:41<00:42, 2869.22 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:42<00:29, 4102.84 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:42<00:28, 4153.13 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:43<41<00:43, 2834.62 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:41<00:42, 2847.84 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:41<00:39, 3083.82 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:42<00:30, 3875.69 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:42<00:29, 4018.74 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:42<00:22, 5033.14 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:42<00:22, 5186.56 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:42<00:19, 5783.79 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:43<00:16, 6748.00 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████     :40<00:35, 3440.26 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:40<00:30, 3875.93 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:40<00:26, 4401.44 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:41<00:39, 2990.35 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:41<00:44, 2595.16 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:42<00:43, 2638.95 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:42<00:42, 2695.27 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:42<00:36, 3140.48 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:42<00:35, 3157.37 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████    00:38, 3142.04 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:39<00:50, 2351.12 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:40<00:46, 2529.71 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:40<00:49, 2397.88 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:40<00:47, 2446.03 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:41<00:50, 2311.96 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:41<00:44, 2558.81 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:42<00:47, 2350.96 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:43<00:49, 2243.61 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 8 [00:41<00:22, 5250.39 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:41<00:18, 6372.65 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:42<00:28, 4097.16 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:42<00:22, 4974.53 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:42<00:31, 3526.37 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:43<00:34, 3284.16 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:43<00:29, 3776.85 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76917/186688 [00:43<00:33, 3232.77 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 77917/186688 [00:44<00:31, 3473.95 examples/s]Running tokenizer on dataset (num_proc=64):  42%|███0:43, 2833.95 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 65917/186688 [00:41<00:48, 2488.77 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 66917/186688 [00:42<00:59, 2006.26 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 67917/186688 [00:42<00:55, 2129.81 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 70917/186688 [00:42<00:32, 3548.00 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 71917/186688 [00:43<00:35, 3222.04 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 72917/186688 [00:43<00:33, 3390.98 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 74917/186688 [00:44<00:35, 3131.80 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 75917/186688 [00:44<00:34, 3244.52 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 700:40, 2876.28 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:43<00:37, 3116.34 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:43<00:26, 4272.90 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:43<00:24, 4530.65 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:44<00:15, 7245.65 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 79000/186688 [00:44<00:20, 5370.21 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80000/186688 [00:44<00:18, 5648.75 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 81000/186688 [00:44<00:18, 5847.08 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82000/186688 [00:45<00:27, 3837.96 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████| 74000/186688 [00:42<00:33, 3380.66 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:43<00:39, 2812.60 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:43<00:27, 3958.39 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 79000/186688 [00:43<00:19, 5534.52 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 81000/186688 [00:44<00:18, 5588.12 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82000/186688 [00:44<00:17, 5938.42 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 83000/186688 [00:44<00:20, 5023.01 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 84000/186688 [00:46<00:52, 1969.64 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 86000/186688 [00:46<00:37, 2655.27 examples/s]Running tokenizer on dataset (num_p█▏     | 78917/186688 [00:44<00:27, 3920.80 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80917/186688 [00:44<00:27, 3847.77 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 81917/186688 [00:45<00:25, 4170.13 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82917/186688 [00:45<00:24, 4280.23 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83917/186688 [00:45<00:27, 3723.65 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84917/186688 [00:45<00:25, 3937.38 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 86917/186688 [00:45<00:16, 5938.10 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87917/186688 [00:46<00:15, 6196.64 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 88834/186688 [00:46<00:21, 4519.07 examples/s]Running tokenizer on  | 75000/186688 [00:43<00:40, 2726.45 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:43<00:35, 3155.93 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:43<00:34, 3223.21 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 79000/186688 [00:44<00:25, 4276.18 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80000/186688 [00:44<00:38, 2798.89 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80917/186688 [00:45<00:35, 3012.14 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 81834/186688 [00:45<00:29, 3538.85 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83834/186688 [00:45<00:27, 3729.99 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85834/186688 [00:46<00:28, 3547.99 examples/s]Running tokenizer on dataset (num▉      | 73917/186688 [00:42<00:35, 3161.46 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 75917/186688 [00:43<00:42, 2587.61 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76917/186688 [00:43<00:50, 2181.85 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 77917/186688 [00:45<01:14, 1453.75 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78917/186688 [00:45<01:00, 1775.55 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 79917/186688 [00:45<00:47, 2263.96 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80917/186688 [00:45<00:41, 2565.61 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 81917/186688 [00:46<00:35, 2954.30 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83917/186688 [00:46<00:31, 3236.74 examples/s]Running tokenizer on datas | 77000/186688 [00:43<00:15, 6894.69 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78000/186688 [00:43<00:20, 5423.45 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 79000/186688 [00:44<00:28, 3768.93 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 79917/186688 [00:44<00:27, 3937.58 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80917/186688 [00:44<00:23, 4425.53 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 81917/186688 [00:44<00:29, 3585.17 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82917/186688 [00:45<00:29, 3557.41 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84834/186688 [00:45<00:21, 4823.87 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85834/186688 [00:45<00:28, 3493.52 examples/s]Running tokenizer on dataset (6917/186688 [00:44<00:29, 3721.78 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78917/186688 [00:44<00:23, 4603.92 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 79917/186688 [00:45<00:33, 3200.80 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80917/186688 [00:45<00:29, 3570.13 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 81917/186688 [00:46<00:44, 2364.60 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82917/186688 [00:46<00:35, 2927.10 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83917/186688 [00:47<00:34, 2942.81 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84834/186688 [00:47<00:29, 3437.52 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85834/186688 [00:47<00:26, 3739.40 examples/s]Running tokenizer on dataset (num_77000/186688 [00:43<00:33, 3232.14 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78000/186688 [00:44<00:42, 2559.84 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 79000/186688 [00:44<00:37, 2900.25 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80000/186688 [00:44<00:37, 2868.24 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 81000/186688 [00:45<00:45, 2346.45 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82917/186688 [00:46<00:49, 2113.99 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83917/186688 [00:46<00:46, 2196.86 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85834/186688 [00:47<00:38, 2622.64 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▋     | 86751/186688 [00:48<00:58, 1698.16 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 86834/186688 [00:47<00:35, 2783.05 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87751/186688 [00:47<00:32, 3059.00 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 88751/186688 [00:47<00:27, 3543.09 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89751/186688 [00:47<00:30, 3160.28 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90751/186688 [00:48<00:53, 1794.95 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91668/186688 [00:49<00:48, 1967.23 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92585/186688 [00:49<00:46, 2002.79 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93502/186688 [00:49<00:40, 2314.60 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95419/186688 [00:50<00:31, 2928.00 examplenum_proc=64):  47%|████▋     | 86834/186688 [00:47<00:51, 1941.08 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 88751/186688 [00:47<00:35, 2729.51 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89751/186688 [00:47<00:31, 3049.24 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91668/186688 [00:48<00:26, 3526.53 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92585/186688 [00:49<00:54, 1722.23 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93502/186688 [00:50<01:10, 1317.93 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94502/186688 [00:50<00:54, 1690.20 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95502/186688 [00:51<00:45, 2023.60 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96419/186688 [00:51<00:36, 2502.98 exet (num_proc=64):  46%|████▌     | 85917/186688 [00:47<00:28, 3543.87 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87751/186688 [00:47<00:28, 3518.67 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 88751/186688 [00:48<00:45, 2150.00 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89668/186688 [00:48<00:38, 2511.67 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91668/186688 [00:50<00:58, 1631.01 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92585/186688 [00:51<00:50, 1858.38 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93585/186688 [00:51<00:43, 2128.43 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95502/186688 [00:51<00:29, 3045.22 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97419/186688 [00:51<00:21, 4129.2_proc=64):  47%|████▋     | 87751/186688 [00:48<00:55, 1786.79 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 88668/186688 [00:49<00:49, 1999.32 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89668/186688 [00:49<00:41, 2364.78 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90668/186688 [00:50<00:49, 1931.45 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91668/186688 [00:50<00:41, 2269.72 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92668/186688 [00:50<00:33, 2785.49 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93668/186688 [00:50<00:26, 3460.40 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94585/186688 [00:51<00:47, 1956.51 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96419/186688 [00:51<00:28, 3200.87 exampproc=64):  47%|████▋     | 87751/186688 [00:47<00:20, 4729.64 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 88751/186688 [00:48<00:24, 3921.69 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89668/186688 [00:48<00:32, 2988.69 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90668/186688 [00:49<00:43, 2210.03 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92585/186688 [00:50<00:44, 2095.27 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93502/186688 [00:51<00:54, 1705.60 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94502/186688 [00:51<00:45, 2015.56 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95419/186688 [00:51<00:36, 2499.74 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96336/186688 [00:52<00:44, 2026.13 exampl▍     | 83000/186688 [00:45<00:23, 4323.39 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 84000/186688 [00:47<01:13, 1391.97 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85000/186688 [00:47<00:59, 1714.50 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85917/186688 [00:48<01:11, 1417.59 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87834/186688 [00:49<00:51, 1901.88 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 88751/186688 [00:50<01:01, 1583.93 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89668/186688 [00:50<00:54, 1792.11 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90585/186688 [00:51<00:54, 1753.14 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91502/186688 [00:52<01:08, 1382.46 examples/s]Running tokenizer on daroc=64):  47%|████▋     | 87000/186688 [00:46<00:32, 3038.28 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 88000/186688 [00:46<00:30, 3207.29 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89000/186688 [00:49<01:41, 959.09 examples/s] Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89917/186688 [00:50<01:33, 1031.70 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90834/186688 [00:50<01:15, 1269.79 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91834/186688 [00:51<01:17, 1216.25 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92751/186688 [00:52<01:20, 1161.38 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93668/186688 [00:53<01:08, 1363.19 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94585/186688 [00:53<00:56, 1619.35 examples/s/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97336/186688 [00:51<00:41, 2128.17 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98253/186688 [00:51<00:36, 2435.61 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99253/186688 [00:52<00:35, 2440.64 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100253/186688 [00:52<00:31, 2720.49 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101170/186688 [00:52<00:27, 3096.10 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102170/186688 [00:52<00:22, 3812.46 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103087/186688 [00:53<00:40, 2068.71 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104004/186688 [00:54<00:34, 2395.92 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█les/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97419/186688 [00:52<00:25, 3454.85 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98336/186688 [00:52<00:28, 3122.22 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99336/186688 [00:52<00:29, 2944.39 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100253/186688 [00:53<00:26, 3320.34 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101170/186688 [00:53<00:24, 3481.97 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102170/186688 [00:53<00:23, 3634.52 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103170/186688 [00:53<00:19, 4268.15 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104087/186688 [00:53<00:20, 4045.03 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█ dataset (num_proc=64):  48%|████▊     | 89834/186688 [00:46<00:27, 3468.25 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90834/186688 [00:47<00:33, 2829.20 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91834/186688 [00:49<01:16, 1244.20 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93751/186688 [00:51<01:22, 1126.56 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95668/186688 [00:51<00:53, 1713.03 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96668/186688 [00:51<00:45, 1985.54 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97585/186688 [00:52<00:53, 1678.96 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99502/186688 [00:52<00:33, 2585.36 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 100419/186688 [00:55<taset (num_proc=64):  50%|████▉     | 92502/186688 [00:52<00:51, 1839.69 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93419/186688 [00:52<00:49, 1875.51 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95419/186688 [00:53<00:30, 3026.20 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96336/186688 [00:53<00:39, 2262.14 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97253/186688 [00:54<00:44, 1989.00 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98170/186688 [00:54<00:39, 2265.54 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99087/186688 [00:55<00:44, 1989.64 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100087/186688 [00:55<00:36, 2362.47 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101087/186688 [00:5amples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97336/186688 [00:51<00:41, 2134.60 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98253/186688 [00:52<00:58, 1501.68 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100253/186688 [00:53<00:39, 2197.81 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101170/186688 [00:53<00:32, 2610.51 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102170/186688 [00:54<00:52, 1597.18 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104087/186688 [00:54<00:32, 2542.09 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 105004/186688 [00:55<00:34, 2361.53 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 105921/186688 [00:55<00:32, 2448.13 examples/s]Running tokenizer on dataset (num_proc=64):  58%s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95502/186688 [00:53<00:55, 1641.16 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96419/186688 [00:54<00:52, 1709.37 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97336/186688 [00:54<00:40, 2225.74 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98253/186688 [00:54<00:40, 2192.10 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99253/186688 [00:55<00:42, 2045.46 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101253/186688 [00:55<00:27, 3163.06 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102170/186688 [00:56<00:28, 2931.56 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103087/186688 [00:56<00:31, 2669.04 examples/s]Running tokenizer on dataset (num_proc=64):  56%|███es/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97253/186688 [00:52<00:38, 2346.42 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98253/186688 [00:52<00:33, 2676.89 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100253/186688 [00:52<00:21, 4080.65 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101170/186688 [00:53<00:19, 4395.19 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102087/186688 [00:53<00:36, 2342.56 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103004/186688 [00:54<00:31, 2643.48 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104004/186688 [00:54<00:34, 2430.60 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104921/186688 [00:56<01:15, 1088.64 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█0 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99336/186688 [00:53<00:46, 1877.04 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100336/186688 [00:53<00:39, 2194.13 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101336/186688 [00:54<00:38, 2220.14 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103253/186688 [00:54<00:32, 2574.66 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104170/186688 [00:56<00:53, 1546.04 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106004/186688 [00:56<00:35, 2266.38 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 107004/186688 [00:56<00:33, 2345.29 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108004/186688 [00:57<00:29, 2696.99 examples/s]Running tokenizer on dataset (num_proc=64):███▋    | 106004/186688 [00:54<00:25, 3162.95 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 107921/186688 [00:54<00:19, 3938.69 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108838/186688 [00:55<00:28, 2693.16 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109755/186688 [00:55<00:26, 2866.49 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111672/186688 [00:56<00:23, 3192.37 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113506/186688 [00:56<00:18, 3981.73 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115506/186688 [00:56<00:14, 4790.12 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116423/186688 [00:57<00:22, 3101.17 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117340/186688 [00:57<00:22, 308████▋    | 105087/186688 [00:54<00:37, 2197.08 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106921/186688 [00:55<00:25, 3145.13 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108838/186688 [00:55<00:19, 4086.01 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110755/186688 [00:55<00:15, 5026.60 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112672/186688 [00:55<00:12, 6091.09 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113589/186688 [00:56<00:23, 3170.60 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114506/186688 [00:57<00:25, 2836.91 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115423/186688 [00:57<00:24, 2968.82 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116340/186688 [00:58<00:30, 201:20, 1073.37 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101336/186688 [00:55<01:05, 1310.83 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103253/186688 [00:56<00:51, 1627.86 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104170/186688 [00:56<00:41, 1982.47 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▋    | 105087/186688 [00:56<00:36, 2256.63 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106004/186688 [00:56<00:31, 2601.62 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106921/186688 [00:57<00:29, 2659.99 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108838/186688 [00:57<00:18, 4111.77 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109755/186688 [00:57<00:17, 4350.50 examples/s]Running tokenizer on dataset 5<00:30, 2779.27 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102004/186688 [00:55<00:24, 3387.62 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104004/186688 [00:56<00:25, 3227.37 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104921/186688 [00:56<00:25, 3181.11 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 105921/186688 [00:56<00:21, 3827.84 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106838/186688 [00:57<00:20, 3962.24 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 107838/186688 [00:57<00:17, 4546.29 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108838/186688 [00:57<00:15, 4909.65 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109838/186688 [00:57<00:22, 3429.28 examples/s]Running tokenizer on datase|█████▊    | 107838/186688 [00:56<00:24, 3178.03 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108755/186688 [00:56<00:21, 3607.33 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109755/186688 [00:56<00:20, 3706.20 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110755/186688 [00:56<00:19, 3811.52 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111672/186688 [00:57<00:30, 2439.23 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112672/186688 [00:57<00:26, 2758.92 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114672/186688 [00:58<00:23, 3060.07 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115589/186688 [00:58<00:19, 3589.80 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117423/186688 [00:59<00:2████▋    | 106921/186688 [00:56<00:42, 1868.88 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 107838/186688 [00:57<00:40, 1954.21 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109755/186688 [00:57<00:28, 2721.64 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110672/186688 [00:58<00:32, 2307.38 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112672/186688 [00:58<00:25, 2923.36 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113672/186688 [00:58<00:23, 3148.31 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114672/186688 [00:59<00:22, 3245.79 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116506/186688 [00:59<00:16, 4258.57 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119340/186688 [00:59<00:09, 6█▌    | 104087/186688 [00:56<00:26, 3065.15 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 105921/186688 [00:56<00:18, 4369.98 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106838/186688 [00:57<00:22, 3532.40 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108755/186688 [00:57<00:16, 4760.61 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111589/186688 [00:58<00:13, 5540.39 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113423/186688 [00:59<00:21, 3439.87 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114423/186688 [00:59<00:18, 3912.77 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117174/186688 [00:59<00:12, 5493.64 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118091/186688 [00:59<00:13, 5156.25 e  58%|█████▊    | 109004/186688 [00:57<00:26, 2909.31 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109921/186688 [00:57<00:28, 2698.59 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110921/186688 [00:58<00:44, 1698.37 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111838/186688 [00:59<00:34, 2176.91 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112755/186688 [00:59<00:27, 2650.99 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113755/186688 [00:59<00:27, 2612.44 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114672/186688 [00:59<00:23, 3130.14 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116506/186688 [01:00<00:19, 3593.25 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117423/186688 [01:00(num_proc=64):  59%|█████▉    | 110755/186688 [00:58<00:24, 3037.49 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111672/186688 [00:58<00:23, 3215.03 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112589/186688 [00:58<00:22, 3265.37 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113506/186688 [00:58<00:25, 2912.69 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114423/186688 [00:59<00:26, 2707.77 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116340/186688 [00:59<00:19, 3549.50 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117257/186688 [01:00<00:23, 2951.38 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118174/186688 [01:00<00:21, 3209.45 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 114.74 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118257/186688 [00:58<00:30, 2273.43 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119174/186688 [00:58<00:28, 2372.98 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121091/186688 [00:59<00:21, 3068.04 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122008/186688 [00:59<00:18, 3481.19 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 122925/186688 [00:59<00:19, 3237.44 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 123842/186688 [01:00<00:22, 2803.60 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125759/186688 [01:00<00:15, 4046.76 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126676/186688 [01:00<00:13, 4291.89 examples/s]Running tokenizer on da341.56 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117340/186688 [00:58<00:36, 1886.75 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118340/186688 [00:58<00:28, 2369.79 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119340/186688 [01:00<00:45, 1473.22 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120257/186688 [01:00<00:35, 1889.73 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121257/186688 [01:00<00:28, 2268.97 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122174/186688 [01:00<00:24, 2687.71 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 124008/186688 [01:00<00:14, 4319.11 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125842/186688 [01:01<00:11, 5258.57 examples/s]Running tokenizer on t (num_proc=64):  59%|█████▉    | 110838/186688 [00:58<00:19, 3975.20 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112672/186688 [00:58<00:16, 4412.46 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113672/186688 [00:58<00:17, 4125.55 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114589/186688 [00:58<00:15, 4582.53 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116506/186688 [00:59<00:14, 4861.57 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118423/186688 [00:59<00:17, 4006.27 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120340/186688 [01:00<00:19, 3448.02 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 122340/186688 [01:00<00:15, 4070.54 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   <00:18, 3727.58 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118423/186688 [01:01<00:24, 2792.80 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119340/186688 [01:01<00:20, 3251.34 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120340/186688 [01:01<00:17, 3710.25 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121257/186688 [01:01<00:14, 4420.19 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122174/186688 [01:01<00:14, 4403.48 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 123174/186688 [01:01<00:12, 5186.34 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 124091/186688 [01:02<00:13, 4491.34 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125008/186688 [01:02<00:12, 4784.26 examples/s]Running toke738.11 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121257/186688 [00:59<00:08, 7660.05 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 123091/186688 [01:00<00:10, 5949.68 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 124008/186688 [01:00<00:10, 6222.24 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124925/186688 [01:01<00:18, 3375.89 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125842/186688 [01:01<00:18, 3372.94 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126759/186688 [01:01<00:17, 3512.97 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128676/186688 [01:01<00:10, 5335.33 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130510/186688 [01:02<00:13, 4025.61 examples/s]Running tokenizer on xamples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120008/186688 [01:00<00:14, 4707.31 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 121925/186688 [01:00<00:14, 4556.93 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 122842/186688 [01:01<00:18, 3399.09 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124759/186688 [01:01<00:14, 4331.73 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125676/186688 [01:01<00:16, 3698.06 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128510/186688 [01:01<00:09, 5848.01 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130427/186688 [01:02<00:09, 5664.75 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131344/186688 [01:02<00:09, 6017.28 examples/s]Running tokenizer on dataset 0, 3326.64 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118340/186688 [00:59<00:18, 3686.41 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119340/186688 [01:00<00:27, 2421.70 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120257/186688 [01:00<00:34, 1926.49 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122091/186688 [01:01<00:22, 2820.91 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 123008/186688 [01:01<00:30, 2110.49 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 123925/186688 [01:02<00:30, 2032.70 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125842/186688 [01:02<00:20, 2975.57 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127676/186688 [01:02<00:16, 3597.30 examples/s]Running tokenizer9091/186688 [01:00<00:23, 2934.46 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120008/186688 [01:00<00:19, 3486.98 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 120925/186688 [01:01<00:15, 4148.73 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 122759/186688 [01:01<00:15, 4062.31 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 123759/186688 [01:02<00:19, 3209.05 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125593/186688 [01:02<00:14, 4152.40 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126510/186688 [01:02<00:19, 3048.05 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128510/186688 [01:02<00:12, 4547.12 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129427/186688 [01:03<00:11, 4778.78 exampldataset (num_proc=64):  68%|██████▊   | 127759/186688 [01:01<00:09, 6514.45 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128759/186688 [01:01<00:08, 6862.97 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130593/186688 [01:01<00:06, 8450.65 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132510/186688 [01:02<00:09, 6019.41 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134344/186688 [01:02<00:12, 4162.54 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135344/186688 [01:03<00:14, 3475.51 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136261/186688 [01:03<00:13, 3745.82 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▎  | 137261/186688 [01:03<00:12, 3866.57 examples/s]Running tokenizer on dataset (num_proc=64):  75%|█nizer on dataset (num_proc=64):  68%|██████▊   | 126842/186688 [01:02<00:08, 6937.10 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128676/186688 [01:02<00:07, 7699.14 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129593/186688 [01:02<00:08, 6620.59 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131510/186688 [01:02<00:07, 7820.13 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133427/186688 [01:03<00:05, 9584.54 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135344/186688 [01:03<00:06, 8216.49 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▎  | 137261/186688 [01:03<00:06, 7765.13 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138178/186688 [01:04<00:10, 4771.21 examples/s]Running tokenizer on dataset (num_proc=64): taset (num_proc=64):  68%|██████▊   | 127593/186688 [01:01<00:19, 3089.18 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128510/186688 [01:01<00:20, 2889.40 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129510/186688 [01:01<00:18, 3116.21 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130427/186688 [01:02<00:17, 3136.69 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131344/186688 [01:02<00:16, 3358.94 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132261/186688 [01:02<00:21, 2552.60 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133178/186688 [01:03<00:26, 2028.29 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135012/186688 [01:03<00:16, 3120.35 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███ on dataset (num_proc=64):  69%|██████▉   | 128676/186688 [01:03<00:14, 3925.10 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129593/186688 [01:03<00:12, 4397.69 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130510/186688 [01:03<00:11, 4935.21 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131427/186688 [01:03<00:10, 5377.80 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132344/186688 [01:03<00:10, 5121.36 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133261/186688 [01:03<00:09, 5632.08 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134178/186688 [01:03<00:09, 5548.96 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135095/186688 [01:04<00:08, 6091.80 examples/s]Running tokenizer on dataset (num_proc=64):  73%|| 123257/186688 [01:01<00:20, 3022.59 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125091/186688 [01:02<00:18, 3261.02 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 126008/186688 [01:03<00:29, 2077.32 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126925/186688 [01:03<00:25, 2356.49 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128842/186688 [01:03<00:19, 2959.16 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130676/186688 [01:04<00:16, 3404.13 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132510/186688 [01:04<00:11, 4662.26 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133427/186688 [01:04<00:12, 4241.46 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135261/186688 [01:04<00:12, 4231.2es/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131344/186688 [01:03<00:08, 6400.62 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132261/186688 [01:03<00:08, 6383.01 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133178/186688 [01:03<00:09, 5521.18 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135095/186688 [01:03<00:07, 6898.70 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136012/186688 [01:03<00:07, 6834.56 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 137929/186688 [01:04<00:06, 7449.27 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138846/186688 [01:04<00:09, 4865.72 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140763/186688 [01:05<00:09, 4661.04 examples/s]Running tokenizer on ddataset (num_proc=64):  70%|███████   | 131427/186688 [01:02<00:12, 4419.29 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132344/186688 [01:02<00:15, 3585.27 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133261/186688 [01:03<00:17, 3095.45 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134261/186688 [01:03<00:15, 3368.89 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135261/186688 [01:03<00:15, 3276.92 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136178/186688 [01:04<00:23, 2154.09 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 137095/186688 [01:04<00:18, 2711.97 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138012/186688 [01:05<00:20, 2375.85 examples/s]Running tokenizer on dataset (num_proc=64):  74%|██████▎  | 136095/186688 [01:04<00:17, 2921.20 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138012/186688 [01:05<00:13, 3672.31 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138929/186688 [01:05<00:13, 3493.38 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139846/186688 [01:05<00:13, 3416.80 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141846/186688 [01:05<00:09, 4730.58 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 142846/186688 [01:06<00:10, 3996.95 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143763/186688 [01:06<00:10, 4203.42 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144680/186688 [01:06<00:08, 4833.14 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  ███▍  | 137846/186688 [01:04<00:16, 3014.88 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138846/186688 [01:05<00:15, 3099.24 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139763/186688 [01:05<00:21, 2147.56 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140763/186688 [01:06<00:17, 2628.27 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142597/186688 [01:06<00:11, 3869.87 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143514/186688 [01:06<00:16, 2545.22 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144431/186688 [01:07<00:14, 2910.76 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146431/186688 [01:07<00:10, 3973.83 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147431(num_proc=64):  71%|███████   | 132261/186688 [01:02<00:11, 4900.23 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133261/186688 [01:02<00:10, 4875.89 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135095/186688 [01:03<00:12, 4275.80 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 137012/186688 [01:03<00:09, 5177.17 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138846/186688 [01:04<00:17, 2708.75 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139763/186688 [01:05<00:19, 2424.91 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141680/186688 [01:07<00:27, 1645.33 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142597/186688 [01:07<00:25, 1738.04 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▍  | 140012/186688 [01:04<00:09, 4680.64 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141846/186688 [01:04<00:08, 5152.45 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143763/186688 [01:04<00:06, 6670.89 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145597/186688 [01:04<00:05, 8133.43 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147514/186688 [01:04<00:04, 8088.97 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149348/186688 [01:06<00:14, 2571.74 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 150348/186688 [01:07<00:15, 2278.80 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151348/186688 [01:07<00:14, 2478.21 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ ███████▍  | 138929/186688 [01:05<00:21, 2269.49 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139846/186688 [01:06<00:29, 1593.05 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140763/186688 [01:07<00:26, 1728.87 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141680/186688 [01:07<00:23, 1907.05 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142597/186688 [01:07<00:18, 2328.13 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143514/186688 [01:08<00:25, 1707.45 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144431/186688 [01:09<00:23, 1795.15 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145348/186688 [01:09<00:18, 2217.65 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊ 75%|███████▍  | 139095/186688 [01:04<00:11, 4116.89 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 140012/186688 [01:04<00:12, 3883.60 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140929/186688 [01:05<00:12, 3789.54 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141846/186688 [01:05<00:11, 3813.42 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142763/186688 [01:05<00:10, 4065.43 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143680/186688 [01:05<00:10, 4260.12 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145514/186688 [01:06<00:10, 3920.04 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147348/186688 [01:07<00:19, 1990.40 examples/s]Running tokenizer on dataset (num_proc=64):  79%|██████| 145680/186688 [01:07<00:12, 3220.41 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▊  | 146597/186688 [01:07<00:12, 3161.72 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147514/186688 [01:08<00:18, 2161.06 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 148514/186688 [01:08<00:13, 2768.55 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 149431/186688 [01:08<00:12, 3035.78 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 150348/186688 [01:08<00:10, 3390.45 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151265/186688 [01:09<00:11, 3161.81 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153099/186688 [01:09<00:08, 4083.65 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154099/186688 [01:5 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138095/186688 [01:05<00:07, 6558.76 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 140012/186688 [01:05<00:08, 5619.16 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140929/186688 [01:06<00:13, 3270.84 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141846/186688 [01:07<00:18, 2451.04 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143680/186688 [01:07<00:12, 3436.94 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144680/186688 [01:07<00:14, 2994.15 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145597/186688 [01:07<00:12, 3417.02 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146514/186688 [01:08<00:11, 3412.10 examples/s]Running to█████▋  | 143514/186688 [01:07<00:20, 2115.69 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145514/186688 [01:08<00:17, 2396.68 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147431/186688 [01:09<00:14, 2672.29 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148348/186688 [01:09<00:14, 2585.43 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149348/186688 [01:09<00:12, 3055.73 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 150348/186688 [01:09<00:10, 3375.23 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151265/186688 [01:10<00:10, 3350.17 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 152182/186688 [01:10<00:12, 2665.28 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ /186688 [01:07<00:08, 4461.32 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148348/186688 [01:07<00:07, 5052.11 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149348/186688 [01:07<00:07, 5178.31 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150265/186688 [01:08<00:11, 3267.30 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151265/186688 [01:08<00:11, 3135.03 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153099/186688 [01:09<00:08, 3883.03 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 154016/186688 [01:09<00:10, 2971.69 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 155933/186688 [01:10<00:15, 2038.11 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156933/186688 [01:11<0ataset (num_proc=64):  76%|███████▋  | 142597/186688 [01:05<00:09, 4480.77 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144514/186688 [01:06<00:12, 3430.74 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146348/186688 [01:06<00:10, 3694.76 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147348/186688 [01:07<00:16, 2361.61 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148348/186688 [01:08<00:14, 2609.11 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149265/186688 [01:08<00:13, 2682.67 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150182/186688 [01:11<00:36, 997.77 examples/s] Running tokenizer on dataset (num_proc=64):  81%|████████  | 151182/186688 [01:11<00:28, 1229.22 examples/s]Running tokenizer on dataset (num_proc=64):  8▉  | 148348/186688 [01:10<00:35, 1094.19 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149265/186688 [01:10<00:30, 1232.00 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150182/186688 [01:10<00:23, 1541.06 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151182/186688 [01:11<00:20, 1725.96 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 152182/186688 [01:11<00:18, 1864.27 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153099/186688 [01:11<00:15, 2226.58 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 154016/186688 [01:11<00:11, 2746.19 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154933/186688 [01:12<00:14, 2135.88 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 15593kenizer on dataset (num_proc=64):  79%|███████▉  | 147431/186688 [01:10<00:32, 1192.52 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149265/186688 [01:10<00:21, 1717.04 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150182/186688 [01:10<00:17, 2105.30 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151099/186688 [01:11<00:13, 2544.49 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 152933/186688 [01:11<00:09, 3542.02 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153933/186688 [01:11<00:12, 2727.92 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154933/186688 [01:12<00:10, 3050.88 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 155850/186688 [01:12<00:10, 2811.41 examples/s]Running tokenizer on datas| 152265/186688 [01:08<00:17, 1994.33 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154099/186688 [01:08<00:12, 2699.50 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 155099/186688 [01:09<00:12, 2530.56 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 156099/186688 [01:09<00:10, 2893.14 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 157099/186688 [01:09<00:11, 2561.17 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 158016/186688 [01:11<00:17, 1600.99 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158933/186688 [01:11<00:18, 1483.04 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:13<00:25, 1054.62 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850  | 146265/186688 [01:09<00:17, 2340.07 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147265/186688 [01:09<00:14, 2642.04 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149099/186688 [01:10<00:10, 3649.29 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150099/186688 [01:11<00:17, 2037.33 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151016/186688 [01:11<00:14, 2534.22 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████▏ | 152016/186688 [01:11<00:12, 2781.24 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153016/186688 [01:12<00:13, 2502.63 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154933/186688 [01:13<00:21, 1496.18 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156850/18668| 153182/186688 [01:10<00:12, 2719.01 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154099/186688 [01:11<00:12, 2570.41 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 155099/186688 [01:11<00:12, 2488.08 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156933/186688 [01:12<00:08, 3550.41 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157933/186688 [01:12<00:12, 2250.77 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:13<00:12, 2319.68 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:16<00:31, 854.61 examples/s] Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:18<00:37, 684.38 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/2%|████████▏ | 152182/186688 [01:12<00:30, 1127.04 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153099/186688 [01:12<00:23, 1448.01 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154099/186688 [01:12<00:17, 1899.37 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 156099/186688 [01:13<00:11, 2553.45 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157933/186688 [01:13<00:08, 3531.37 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:13<00:10, 2723.53 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:14<00:09, 2942.24 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:25<00:08, 2942.24 examples/s]Running tokenizer on dataset (num_proc=64):  87%|█3/186688 [01:12<00:11, 2767.20 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156933/186688 [01:13<00:12, 2404.65 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157933/186688 [01:13<00:10, 2839.97 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:13<00:11, 2461.95 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:15<00:22, 1205.01 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:18<00:38, 677.10 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:26<01:22, 300.19 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162850/186688 [01:26<01:01, 389.46 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163850/186688 [10<00:11, 2877.08 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 156016/186688 [01:10<00:10, 2968.42 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 157016/186688 [01:11<00:12, 2320.88 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157933/186688 [01:11<00:10, 2707.57 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:13<00:20, 1342.86 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:14<00:25, 1043.90 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:16<00:32, 795.14 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:19<00:41, 593.39 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:30<01:48,/186688 [01:13<00:20, 1261.49 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:16<00:36, 689.88 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:18<00:36, 656.44 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:20<00:39, 586.22 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:25<00:56, 386.73 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:26<00:45, 462.39 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:28<00:45, 440.65 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:30<00:39, 476.29 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:318 [01:14<00:14, 2015.97 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:14<00:13, 2215.58 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:17<00:19, 1373.19 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:19<00:29, 872.97 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:23<00:46, 531.98 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162850/186688 [01:25<00:43, 542.10 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163850/186688 [01:28<00:47, 482.99 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:30<00:44, 494.57 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:32<00:0:14, 2061.63 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157933/186688 [01:12<00:18, 1578.16 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:14<00:24, 1132.19 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:15<00:29, 908.43 examples/s] Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:16<00:28, 893.88 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:28<01:18, 306.38 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:32<01:20, 285.20 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:33<00:59, 370.96 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:33<00:44, 470.31et (num_proc=64):  84%|████████▍ | 156850/186688 [01:13<00:13, 2195.89 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:14<00:17, 1676.80 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:15<00:14, 1876.27 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:27<01:09, 359.31 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162850/186688 [01:27<00:53, 449.39 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163850/186688 [01:31<00:57, 395.34 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164850/186688 [01:31<00:43, 506.32 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165850/186688 [01:32<00:34, 609.14 examples/s]Running tokenizer on dataset (num_proc 219.85 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:30<01:14, 308.54 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:33<01:10, 312.29 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:34<00:50, 412.77 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:34<00:35, 567.80 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:36<00:33, 560.71 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:37<00:29, 604.54 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:38<00:21, 797.19 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:38<00:14, 1068.15 examp██████▋ | 161850/186688 [01:26<01:08, 364.46 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162850/186688 [01:27<00:57, 417.18 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163850/186688 [01:28<00:44, 516.45 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:32<00:57, 380.85 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:36<01:00, 347.06 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:38<00:51, 383.72 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:38<00:36, 521.46 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:39<00:30, 584.24 examples/s]Running tokenizer on dataset (num_proc=64):  91%|██████186688 [01:23<01:00, 409.17 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162850/186688 [01:28<01:18, 304.74 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163850/186688 [01:30<01:07, 340.30 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:31<00:51, 425.78 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:35<00:57, 360.88 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:36<00:46, 429.78 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:38<00:42, 449.39 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:39<00:34, 525.20 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:40<00<00:33, 532.24 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:33<00:29, 574.42 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170684/186688 [01:34<00:28, 566.82 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171684/186688 [01:36<00:26, 556.26 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 172684/186688 [01:37<00:20, 671.98 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173684/186688 [01:38<00:17, 756.52 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:38<00:12, 979.78 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:41<00:18, 607.58 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:42<001:28<00:50, 448.22 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164850/186688 [01:31<00:56, 387.35 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:31<00:39, 528.86 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:32<00:27, 719.28 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:32<00:21, 888.18 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:38<00:46, 383.12 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:39<00:36, 461.14 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:40<00:25, 619.16 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:44<00:35, examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:35<00:42, 464.24 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:35<00:29, 634.95 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:37<00:27, 644.90 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:38<00:18, 871.62 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:38<00:13, 1087.47 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 172684/186688 [01:39<00:11, 1211.55 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173684/186688 [01:42<00:18, 689.90 examples/s] Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:45<00:21, 550.60 ex41, 501.69 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:40<01:15, 264.18 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:40<00:53, 355.39 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:42<00:44, 405.71 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:43<00:33, 503.05 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:43<00:17, 860.44 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:44<00:16, 821.40 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173684/186688 [01:46<00:18, 707.36 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:47<00:13, 870.=64):  89%|████████▉ | 166850/186688 [01:34<00:33, 584.22 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:36<00:18, 892.25 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:43<00:39, 404.80 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:44<00:29, 507.19 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:44<00:22, 626.23 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:45<00:18, 705.60 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:45<00:12, 941.33 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:50<00:23, 472.84 examples/s]Running tokenizer on dataset (num_proc=64)██ | 169767/186688 [01:40<00:23, 734.07 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:40<00:18, 842.07 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:41<00:14, 1030.87 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:41<00:11, 1167.01 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:43<00:12, 1042.02 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:45<00:15, 791.07 examples/s] Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:50<00:28, 380.88 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:51<00:20, 486.65 examples/s]Running tokenizer on dataset (num_proc=64):  95%|██████ 422.32 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:44<00:25, 550.95 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:45<00:18, 714.24 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:49<00:26, 446.62 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:49<00:13, 759.42 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:57<00:27, 338.38 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:58<00:21, 387.84 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [01:58<00:14, 499.80 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [01:59<00:09,:24, 691.97 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:40<00:17, 919.06 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:41<00:16, 925.58 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:42<00:15, 879.38 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173684/186688 [01:46<00:25, 514.35 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:47<00:20, 592.57 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:53<00:22, 447.05 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:56<00:23, 396.82 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:59<00les/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:40<00:16, 879.39 examples/s] Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:40<00:13, 1023.01 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:41<00:10, 1210.36 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:42<00:10, 1127.15 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:45<00:17, 613.32 examples/s] Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:56<00:47, 210.90 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:59<00:38, 241.09 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:01<00:29, 276.60 0:13, 756.18 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:44<00:13, 679.16 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:49<00:23, 350.38 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [01:53<00:23, 313.40 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [01:56<00:20, 319.10 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [01:57<00:14, 379.61 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:01<00:14, 320.92 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:02<00:08, 420.27 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:02<0amples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:46<00:16, 668.24 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:55<00:39, 253.95 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:58<00:34, 266.16 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:59<00:24, 330.93 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:00<00:18, 395.06 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:04<00:18, 341.96 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:05<00:13, 420.27 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:05<00:07, 582.72 ex0:05, 521.31 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:04<00:03, 539.03 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:05<00:01, 646.11 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:06<00:00, 717.99 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:06<00:00, 1473.90 examples/s]
36 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:48<00:12, 851.35 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:52<00:19, 509.60 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:54<00:18, 494.90 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:00<00:29, 283.05 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:02<00:22, 326.39 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:08<00:26, 245.65 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:08<00:16, 337.92 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:09<00:10, 426.███▌| 177518/186688 [01:56<00:29, 307.04 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:59<00:26, 310.50 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:05<00:30, 240.36 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:06<00:20, 316.28 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:06<00:12, 424.98 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:07<00:08, 568.95 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:07<00:05, 665.03 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:08<00:02, 920.41 examples/s]Running tokenizer on dataset (num_proc=64):  99%|██████:  95%|█████████▍| 176601/186688 [01:52<00:21, 469.95 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:55<00:23, 394.92 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:57<00:21, 391.71 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [01:59<00:16, 446.34 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:01<00:15, 420.13 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:04<00:13, 404.84 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:04<00:08, 518.60 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:10<00:11, 322.36 examples/s]Running tokenizer on dataset (num_proc=64)███▉| 184854/186688 [02:10<00:02, 700.93 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:10<00:01, 897.82 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:10<00:00, 1096.17 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:11<00:00, 1418.12 examples/s]
examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:02<00:13, 459.65 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:04<00:12, 448.09 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:05<00:09, 487.00 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:06<00:06, 581.35 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:07<00:04, 669.28 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:08<00:02, 764.58 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:09<00:01, 842.44 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:12<00:00, 563.34 :  99%|█████████▉| 184854/186688 [02:10<00:03, 580.08 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:11<00:00, 750.46 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:12<00:00, 1409.91 examples/s]
amples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:06<00:05, 721.47 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:06<00:03, 816.49 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:09<00:03, 609.11 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:10<00:01, 594.46 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:11<00:00, 665.79 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:12<00:00, 1406.31 examples/s]
examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:12<00:00, 1405.75 examples/s]
 666.44 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:03<00:12, 432.88 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:07<00:13, 345.23 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:08<00:09, 390.79 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:11<00:07, 391.99 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:11<00:03, 510.20 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:11<00:01, 696.69 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:13<00:00, 593.79 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:14<00:00, 1387.82 examples/s]
03 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:11<00:04, 611.34 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:11<00:02, 766.30 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:12<00:01, 833.31 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:14<00:00, 704.25 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:14<00:00, 1384.12 examples/s]
:23, 358.78 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:04<00:26, 279.23 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:07<00:21, 296.46 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:08<00:09, 486.76 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:08<00:06, 577.56 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:09<00:04, 633.44 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:12<00:01, 638.10 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:14<00:00, 616.86 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:15<00:00, 1382.46 examples/s]
[INFO|configuration_utils.py:763] 2025-10-04 13:00:24,091 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 13:00:24,091 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 13:00:24,091 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 13:00:24,091 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 13:00:24,091 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 13:00:24,091 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 13:00:24,091 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 13:00:24,092 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
[INFO|configuration_utils.py:839] 2025-10-04 13:00:24,092 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
[INFO|configuration_utils.py:839] 2025-10-04 13:00:24,092 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
[INFO|configuration_utils.py:839] 2025-10-04 13:00:24,092 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
[INFO|configuration_utils.py:839] 2025-10-04 13:00:24,092 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
[INFO|configuration_utils.py:839] 2025-10-04 13:00:24,092 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-04 13:00:24,092 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-04 13:00:24,106 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 13:00:24,108 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|modeling_utils.py:1277] 2025-10-04 13:00:24,739 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 13:00:24,739 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 13:00:24,739 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 13:00:24,739 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 13:00:24,739 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 13:00:24,739 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 13:00:24,739 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 13:00:24,739 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-04 13:00:24,740 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 13:00:24,740 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 13:00:24,740 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 13:00:24,740 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 13:00:24,740 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 13:00:24,740 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 13:00:24,740 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 13:00:24,740 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-04 13:00:24,750 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:00:24,750 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:00:24,750 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:00:24,752 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:00:24,758 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:00:24,759 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:00:24,769 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:00:24,770 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:30,  1.25s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:30,  1.25s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:32,  1.29s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:23,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:27,  2.92s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.07s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.07s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:28,  1.23s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:28,  1.23s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:28,  1.23s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:28,  1.23s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.06s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.06s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:29,  1.24s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:29,  1.24s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:29,  1.24s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:29,  1.24s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.89s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.89s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.89s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.06s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.06s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:28,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.06s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.06s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:23,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:23,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:23,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.87s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:23,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.87s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.06s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:26,  1.20s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:23,  2.87s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:23,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:23,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:23,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:23,  2.87s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.06s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.06s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.06s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.06s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.40s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.40s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.40s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.40s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.06s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.06s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.07s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.44s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:57,  3.45s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:51,  3.40s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:51,  3.40s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:50,  3.39s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:15<03:51,  3.40s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.60s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.60s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:19<04:02,  3.61s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:25,  3.11s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:39,  3.38s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.98s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.98s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:32<03         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:27<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:27<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:27<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:27<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:27<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03         | 6/73 [00:19<04:01,  3.61s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:24,  3.11s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:21<03:25,  3.11s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:25<03:39,  3.38s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:27<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:27<03:10,  2.97s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:28<03:10,  2.97s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:32<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:32<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:32<03:26,  3.28s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:32<03:26,  3.28s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:23,  3.28s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:23,  3.28s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.51s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.51s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.44s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:31<03:26,  3.28s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:22,  3.27s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:34,  3.52s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.44s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:31,  3.59s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:31,  3.59s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.48s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.48s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:42<03:26,  3.43s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:46<03:32,  3.60s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:49<03:21,  3.47s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shard73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shard73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shard73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shard73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shard73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shard73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shard73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:53<03:26,  3.62s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:56<03:15,  3.49s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [01:00<03:19,  3.63s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.17s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.17s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33ss:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33ss:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33ss:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33ss:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33ss:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33ss:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:06<03:02,  3.45s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33ss:  26%|██▌       | 19/73 [01:02<02:50,  3.16s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.44s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.44s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:07<03:02,  3.45s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:09<02:39,  3.06s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:49,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:49,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:13<02:50,  3.33s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.50s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.50s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.08s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.08s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:16<02:45,  3.31s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:20<02:51,  3.51s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:22<02:27,  3.07s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:36,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:36,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:26<02:37,  3.34s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:28<02:16,  2.96s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:32<02:27,  3.27s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.95s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.95s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:42<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:42<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:42<02:18,  3.29s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:17,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:17,  3.29s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:42<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:42<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:42<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:42<02:18,  3.29s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:09,  2.94s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:20,  3.27s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:18,  3.29s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:22,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:22,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:46<02:23,  3.49s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:17,  3.43s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:17,  3.43s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:23,  3.49s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:49<02:16,  3.42s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:53<02:19,  3.58s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.47s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.47s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.47s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.47s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:56<02:11,  3.46s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [02:00<02:15,  3.67s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  51%|█████     | 37/73 [02:02<01:54,  3.19s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.38s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.38s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.55s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.55s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  56%|█████▌    | 41/42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  56%|█████▌    | 41/42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  56%|█████▌    | 41/42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  56%|█████▌    | 41/42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  56%|█████▌    | 41/42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  56%|█████▌    | 41/42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  56%|█████▌    | 41/42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:06<01:59,  3.42s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:09<01:54,  3.37s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:13<01:57,  3.56s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:17<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:17<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:17<01:50,  3.45s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.60s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.60s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:21<01:51,  3.61s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.15s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.15s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  60%|██73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:22<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  60%|██73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  60%|██73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  60%|██73 [02:17<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:17<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:17<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:17<01:50,  3.45s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  60%|██73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  60%|██73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  60%|██73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:16<01:50,  3.45s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:20<01:51,  3.61s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:23<01:34,  3.14s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.38s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.38s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:27<01:38,  3.39s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.34s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.34s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:34,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:34,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loa███    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loa███    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loa███    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loa███    | 44/73 [02:27<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:27<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:27<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:27<01:38,  3.39s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loa███    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loa███    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loa███    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:26<01:38,  3.39s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:30<01:33,  3.33s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:34<01:35,  3.52s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.46s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.46s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:37<01:29,  3.45s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.60s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.60s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:41<01:30,  3.61s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.52s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.52s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Load▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Load▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Load▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Load▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Load▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Load▋   | 49/73 [02:44<01:24,  3.51s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Load▋   | 49/73 [02:44<01:24,  3.52s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.64s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:48<01:23,  3.65s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.54s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.54s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:51<01:17,  3.53s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.70s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.70s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.22s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.22s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.43s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.43s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.03s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.03s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:56<01:17,  3.71s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:58<01:04,  3.21s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [03:02<01:05,  3.44s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.30s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.30s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.29s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.29s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.29s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.48s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [03:04<00:54,  3.02s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:08<00:56,  3.31s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:11<00:52,  3.28s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.48s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:15<00:52,  3.49s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:18<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:22<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:33<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:33<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:33<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:25<00:41,  3.47s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.63s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.63s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:29<00:39,  3.64s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:33<00:35,  3.54s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:33<00:35,  3.54s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:32<00:35,  3.53s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:33<00:35,  3.53s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.65s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.65s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.58s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.58s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.68s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.68s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:32<00:35,  3.53s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:32<00:35,  3.53s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:33<00:35,  3.53s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:32<00:35,  3.53s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:32<00:35,  3.53s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:32<00:35,  3.53s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:32<00:35,  3.53s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:36<00:32,  3.66s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:40<00:28,  3.57s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.47s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.47s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.07s/it]Loading checkpoint shards:  95%|██████03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|██████03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|██████03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|██████03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|██████03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|██████03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|██████03:44<00:25,  3.69s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:44<00:25,  3.69s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:46<00:19,  3.20s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:50<00:17,  3.48s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.32s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.32s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:52<00:12,  3.06s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:56<00:09,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:59<00:06,  3.32s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.50s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.50s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]

Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.57s/it][INFO|modeling_utils.py:5724] 2025-10-04 13:04:38,131 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 13:04:38,131 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
[INFO|modeling_utils.py:5724] 2025-10-04 13:04:38,131 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 13:04:38,132 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
[INFO|configuration_utils.py:1008] 2025-10-04 13:04:38,134 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
[INFO|modeling_utils.py:5724] 2025-10-04 13:04:38,134 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 13:04:38,134 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1055] 2025-10-04 13:04:38,134 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-04 13:04:38,134 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-04 13:04:38,134 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
[INFO|configuration_utils.py:1008] 2025-10-04 13:04:38,136 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
[INFO|configuration_utils.py:1055] 2025-10-04 13:04:38,136 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it][INFO|modeling_utils.py:5724] 2025-10-04 13:04:38,140 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 13:04:38,141 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
[INFO|configuration_utils.py:1008] 2025-10-04 13:04:38,143 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]

[INFO|configuration_utils.py:1055] 2025-10-04 13:04:38,144 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]

Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [04:03<00:03,  3.51s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
[INFO|modeling_utils.py:5724] 2025-10-04 13:04:38,144 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5724] 2025-10-04 13:04:38,144 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 13:04:38,144 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|modeling_utils.py:5732] 2025-10-04 13:04:38,144 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]

Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
[INFO|modeling_utils.py:5724] 2025-10-04 13:04:38,144 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 13:04:38,144 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1008] 2025-10-04 13:04:38,146 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-04 13:04:38,147 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-04 13:04:38,147 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
[INFO|configuration_utils.py:1008] 2025-10-04 13:04:38,147 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-04 13:04:38,147 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:04:38,147 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|trainer.py:757] 2025-10-04 13:04:38,161 >> Using auto half precision backend
[INFO|trainer.py:757] 2025-10-04 13:04:38,161 >> Using auto half precision backend
[INFO|trainer.py:757] 2025-10-04 13:04:38,161 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 13:04:38,162 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 13:04:38,162 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 13:04:38,162 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 13:04:38,169 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 13:04:38,170 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 13:04:38,171 >> Using auto half precision backend
[WARNING|trainer.py:985] 2025-10-04 13:04:38,172 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 73/73 [04:07<00:00,  3.39s/it]
[INFO|modeling_utils.py:5724] 2025-10-04 13:04:38,172 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|modeling_utils.py:5732] 2025-10-04 13:04:38,172 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|trainer.py:757] 2025-10-04 13:04:38,173 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 13:04:38,173 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|configuration_utils.py:1008] 2025-10-04 13:04:38,174 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-04 13:04:38,175 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|trainer.py:757] 2025-10-04 13:04:38,177 >> Using auto half precision backend
[WARNING|trainer.py:985] 2025-10-04 13:04:38,178 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 13:04:38,196 >> Using auto half precision backend
[WARNING|trainer.py:985] 2025-10-04 13:04:38,197 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 13:04:38,458 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 13:04:38,460 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 13:04:38,472 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 13:04:38,473 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 13:04:38,475 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 13:04:38,477 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 4. Using DeepSpeed's value.
[INFO|deepspeed.py:380] 2025-10-04 13:04:38,490 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 13:04:38,578 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[rank26]:W1004 13:04:42.432000 3453536 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank26]:W1004 13:04:42.432000 3453536 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank25]:W1004 13:04:42.451000 3453535 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank25]:W1004 13:04:42.451000 3453535 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank9]:W1004 13:04:42.470000 15227 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank9]:W1004 13:04:42.470000 15227 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank4]:W1004 13:04:42.535000 3535781 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank4]:W1004 13:04:42.535000 3535781 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank20]:W1004 13:04:42.603000 1654474 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank20]:W1004 13:04:42.603000 1654474 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank22]:W1004 13:04:42.680000 1654476 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank22]:W1004 13:04:42.680000 1654476 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[INFO|trainer.py:2523] 2025-10-04 13:05:28,308 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-04 13:05:28,309 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-04 13:05:28,309 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-04 13:05:28,308 >>   Num examples = 186,688
[INFO|trainer.py:2523] 2025-10-04 13:05:28,309 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-04 13:05:28,309 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-04 13:05:28,308 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-04 13:05:28,309 >>   Num examples = 186,688
[INFO|trainer.py:2523] 2025-10-04 13:05:28,310 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-04 13:05:28,309 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-04 13:05:28,308 >>   Num Epochs = 1
[INFO|trainer.py:2524] 2025-10-04 13:05:28,309 >>   Num examples = 186,688
[INFO|trainer.py:2524] 2025-10-04 13:05:28,309 >>   Num examples = 186,688
[INFO|trainer.py:2524] 2025-10-04 13:05:28,308 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-04 13:05:28,309 >>   Num Epochs = 1
[INFO|trainer.py:2524] 2025-10-04 13:05:28,310 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-04 13:05:28,310 >>   Num Epochs = 1
[INFO|trainer.py:2525] 2025-10-04 13:05:28,309 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-04 13:05:28,308 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2525] 2025-10-04 13:05:28,309 >>   Num Epochs = 1
[INFO|trainer.py:2525] 2025-10-04 13:05:28,309 >>   Num Epochs = 1
[INFO|trainer.py:2525] 2025-10-04 13:05:28,308 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-04 13:05:28,309 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2526] 2025-10-04 13:05:28,310 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2526] 2025-10-04 13:05:28,309 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-04 13:05:28,308 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2526] 2025-10-04 13:05:28,309 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2526] 2025-10-04 13:05:28,309 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2526] 2025-10-04 13:05:28,308 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-04 13:05:28,309 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2529] 2025-10-04 13:05:28,310 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2529] 2025-10-04 13:05:28,309 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-04 13:05:28,308 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2529] 2025-10-04 13:05:28,309 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2529] 2025-10-04 13:05:28,309 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2529] 2025-10-04 13:05:28,308 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-04 13:05:28,309 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2530] 2025-10-04 13:05:28,310 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2530] 2025-10-04 13:05:28,309 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-04 13:05:28,308 >>   Total optimization steps = 1,459
[INFO|trainer.py:2530] 2025-10-04 13:05:28,309 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2530] 2025-10-04 13:05:28,309 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2530] 2025-10-04 13:05:28,308 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-04 13:05:28,309 >>   Total optimization steps = 1,459
[INFO|trainer.py:2531] 2025-10-04 13:05:28,310 >>   Total optimization steps = 1,459
[INFO|trainer.py:2531] 2025-10-04 13:05:28,309 >>   Total optimization steps = 1,459
[INFO|trainer.py:2532] 2025-10-04 13:05:28,309 >>   Number of trainable parameters = 116,829,156,672
[INFO|trainer.py:2531] 2025-10-04 13:05:28,309 >>   Total optimization steps = 1,459
[INFO|trainer.py:2531] 2025-10-04 13:05:28,309 >>   Total optimization steps = 1,459
[INFO|trainer.py:2531] 2025-10-04 13:05:28,308 >>   Total optimization steps = 1,459
[INFO|trainer.py:2532] 2025-10-04 13:05:28,310 >>   Number of trainable parameters = 116,829,156,672
[INFO|trainer.py:2532] 2025-10-04 13:05:28,311 >>   Number of trainable parameters = 116,829,156,672
[INFO|trainer.py:2532] 2025-10-04 13:05:28,311 >>   Number of trainable parameters = 116,829,156,672
[INFO|trainer.py:2532] 2025-10-04 13:05:28,310 >>   Number of trainable parameters = 116,829,156,672
[INFO|trainer.py:2532] 2025-10-04 13:05:28,309 >>   Number of trainable parameters = 116,829,156,672
[INFO|trainer.py:2532] 2025-10-04 13:05:28,311 >>   Number of trainable parameters = 116,829,156,672
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2523] 2025-10-04 13:05:28,596 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-04 13:05:28,596 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-04 13:05:28,596 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-04 13:05:28,596 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-04 13:05:28,596 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-04 13:05:28,596 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-04 13:05:28,596 >>   Total optimization steps = 1,459
[INFO|trainer.py:2532] 2025-10-04 13:05:28,597 >>   Number of trainable parameters = 116,829,156,672
srun: Job step aborted: Waiting up to 47 seconds for job step to finish.
W1004 13:07:16.880000 3535733 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:07:16.880000 3432705 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:07:16.880000 191548 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:07:16.880000 2478637 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:07:16.881000 3453486 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:07:16.880000 1654425 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:07:16.881000 3535733 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3535781 closing signal SIGTERM
W1004 13:07:16.881000 15177 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:07:16.881000 3432705 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3432753 closing signal SIGTERM
W1004 13:07:16.881000 191548 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 191596 closing signal SIGTERM
W1004 13:07:16.881000 2478637 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2478684 closing signal SIGTERM
W1004 13:07:16.881000 1654425 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1654474 closing signal SIGTERM
W1004 13:07:16.881000 3453486 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3453534 closing signal SIGTERM
W1004 13:07:16.881000 15177 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 15226 closing signal SIGTERM
slurmstepd: error: *** JOB 1178313 ON della-j16g1 CANCELLED AT 2025-10-04T13:07:16 ***
  0%|          | 0/1459 [00:00<?, ?it/s]slurmstepd: error: *** STEP 1178313.0 ON della-j16g1 CANCELLED AT 2025-10-04T13:07:16 ***
W1004 13:07:16.883000 3297317 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:07:16.885000 3297317 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3297365 closing signal SIGTERM
W1004 13:07:16.885000 3535733 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3535782 closing signal SIGTERM
W1004 13:07:16.885000 15177 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 15227 closing signal SIGTERM
W1004 13:07:16.886000 1654425 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1654475 closing signal SIGTERM
W1004 13:07:16.885000 2478637 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2478685 closing signal SIGTERM
W1004 13:07:16.885000 3453486 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3453535 closing signal SIGTERM
W1004 13:07:16.888000 3453486 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3453536 closing signal SIGTERM
W1004 13:07:16.888000 3297317 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3297366 closing signal SIGTERM
W1004 13:07:16.886000 191548 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 191597 closing signal SIGTERM
W1004 13:07:16.888000 191548 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 191598 closing signal SIGTERM
W1004 13:07:16.893000 3297317 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3297367 closing signal SIGTERM
W1004 13:07:16.902000 3535733 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3535783 closing signal SIGTERM
W1004 13:07:16.905000 3297317 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3297368 closing signal SIGTERM
W1004 13:07:16.906000 1654425 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1654476 closing signal SIGTERM
W1004 13:07:16.910000 3432705 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3432754 closing signal SIGTERM
W1004 13:07:16.930000 3535733 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3535784 closing signal SIGTERM
W1004 13:07:16.929000 15177 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 15228 closing signal SIGTERM
W1004 13:07:16.936000 2478637 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2478686 closing signal SIGTERM
W1004 13:07:16.941000 3453486 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3453537 closing signal SIGTERM
W1004 13:07:16.952000 3432705 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3432755 closing signal SIGTERM
W1004 13:07:16.953000 191548 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 191599 closing signal SIGTERM
W1004 13:07:16.958000 3432705 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3432756 closing signal SIGTERM
