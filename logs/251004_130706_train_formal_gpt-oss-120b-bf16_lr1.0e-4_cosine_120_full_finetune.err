+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
++ head -n 1
++ head -n 1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=0 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 32000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=3 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 32000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=6 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 32000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=4 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 32000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
++ head -n 1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=2 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 32000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=1 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 32000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j16g1,della-j17g1,della-k11g[1,3],della-k12g[2-3],della-k13g[1-2]'
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=7 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 32000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=5 --master_addr=della-j16g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 32000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-120b-bf16_lr1.0e-4_cosine_120_full_finetune --logging_steps 10 --save_steps 500 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1.0e-4 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --flash_attn fa2 --enable_liger_kernel true
W1004 13:07:53.558000 3438480 site-packages/torch/distributed/run.py:774] 
W1004 13:07:53.558000 3438480 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:53.558000 3438480 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 13:07:53.558000 3438480 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:53.559000 197289 site-packages/torch/distributed/run.py:774] 
W1004 13:07:53.559000 197289 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:53.559000 197289 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 13:07:53.559000 197289 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:53.561000 1659439 site-packages/torch/distributed/run.py:774] 
W1004 13:07:53.561000 1659439 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:53.561000 1659439 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 13:07:53.561000 1659439 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:53.570000 20568 site-packages/torch/distributed/run.py:774] 
W1004 13:07:53.570000 20568 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:53.570000 20568 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 13:07:53.570000 20568 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:53.599000 2484366 site-packages/torch/distributed/run.py:774] 
W1004 13:07:53.599000 2484366 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:53.599000 2484366 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 13:07:53.599000 2484366 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:53.663000 3459238 site-packages/torch/distributed/run.py:774] 
W1004 13:07:53.663000 3459238 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:53.663000 3459238 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 13:07:53.663000 3459238 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:53.669000 3302548 site-packages/torch/distributed/run.py:774] 
W1004 13:07:53.669000 3302548 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:53.669000 3302548 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 13:07:53.669000 3302548 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:54.540000 3540769 site-packages/torch/distributed/run.py:774] 
W1004 13:07:54.540000 3540769 site-packages/torch/distributed/run.py:774] *****************************************
W1004 13:07:54.540000 3540769 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 13:07:54.540000 3540769 site-packages/torch/distributed/run.py:774] *****************************************
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,729 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,731 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,731 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,731 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,731 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,731 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,731 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,731 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,731 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,731 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,731 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,732 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,732 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,732 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,732 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,732 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,732 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,732 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,732 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,734 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,734 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,734 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,734 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,734 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,734 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,739 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,739 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,739 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,739 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,739 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:21,739 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:22,415 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:22,415 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:22,415 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 13:08:22,416 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 13:08:22,416 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 13:08:22,416 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:22,419 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 13:08:22,420 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 13:08:22,420 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,420 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,420 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,420 >> loading file added_tokens.json
[INFO|configuration_utils.py:839] 2025-10-04 13:08:22,420 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,420 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,420 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,420 >> loading file chat_template.jinja
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,421 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,421 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,421 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,421 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,421 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:839] 2025-10-04 13:08:22,421 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,421 >> loading file chat_template.jinja
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,421 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,422 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,422 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,422 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,422 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,422 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:22,421 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 13:08:22,422 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:22,422 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 13:08:22,423 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 13:08:22,424 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,425 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,425 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,425 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,425 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,425 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,425 >> loading file chat_template.jinja
[INFO|configuration_utils.py:839] 2025-10-04 13:08:22,425 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,425 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,426 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,426 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,426 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,426 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,426 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:22,426 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 13:08:22,426 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 13:08:22,427 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,428 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,428 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,428 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,428 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,428 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,428 >> loading file chat_template.jinja
[INFO|configuration_utils.py:839] 2025-10-04 13:08:22,428 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,429 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,429 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,429 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,429 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,429 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,429 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:22,436 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-04 13:08:22,437 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 13:08:22,439 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,440 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,440 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,440 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,440 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,440 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-04 13:08:22,440 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:23,124 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:23,131 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:23,131 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:23,134 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:23,134 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:23,136 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:23,138 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|tokenization_utils_base.py:2337] 2025-10-04 13:08:23,169 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 546/186688 [00:01<08:28, 366.11 examples/s]Converting format of dataset (num_proc=64):   5%|▍         | 9076/186688 [00:01<00:22, 7788.80 examples/s]Converting format of dataset (num_proc=64):  17%|█▋        | 32010/186688 [00:01<00:04, 32322.99 examples/s]Converting format of dataset (num_proc=64):  32%|███▏      | 60438/186688 [00:01<00:01, 66614.28 examples/s]Converting format of dataset (num_proc=64):  43%|████▎     | 80664/186688 [00:01<00:01, 88724.26 examples/s]Converting format of dataset (num_proc=64):  58%|█████▊    | 108079/186688 [00:01<00:00, 123458.60 examples/s]Converting format of dataset (num_proc=64):  70%|██████▉   | 130227/186688 [00:02<00:00, 136624.38 examples/s]Converting format of dataset (num_proc=64):  81%|████████▏ | 151913/186688 [00:02<00:00, 154262.34 eConverting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 455/186688 [00:01<09:59, 310.55 examples/s]Converting format of dataset (num_proc=64):   8%|▊         | 15471/186688 [00:01<00:12, 13621.17 examples/s]Converting format of dataset (num_proc=64):  23%|██▎       | 43438/186688 [00:01<00:03, 43549.80 examples/s]Converting format of dataset (num_proc=64):  36%|███▋      | 68020/186688 [00:01<00:01, 71474.10 examples/s]Converting format of dataset (num_proc=64):  47%|████▋     | 88504/186688 [00:01<00:01, 92207.82 examples/s]Converting format of dataset (num_proc=64):  62%|██████▏   | 115787/186688 [00:01<00:00, 126121.97 examples/s]Converting format of dataset (num_proc=64):  74%|███████▍  | 138195/186688 [00:02<00:00, 140448.10 examples/s]Converting format of dataset (num_proc=64):  85%|████████▌ | 159581/186688 [00:02<00:00, 149Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 504/186688 [00:01<09:05, 341.37 examples/s]Converting format of dataset (num_proc=64):  10%|█         | 18724/186688 [00:01<00:10, 16369.98 examples/s]Converting format of dataset (num_proc=64):  25%|██▌       | 46858/186688 [00:01<00:03, 45940.60 examples/s]Converting format of dataset (num_proc=64):  38%|███▊      | 70204/186688 [00:01<00:01, 71719.22 examples/s]Converting format of dataset (num_proc=64):  49%|████▊     | 90653/186688 [00:01<00:01, 93107.72 examples/s]Converting format of dataset (num_proc=64):  62%|██████▏   | 116491/186688 [00:01<00:00, 124389.22 examples/s]Converting format of dataset (num_proc=64):  74%|███████▍  | 138663/186688 [00:02<00:00, 130814.72 examples/s]Converting format of dataset (num_proc=64):  88%|████████▊ | 164618/186688 [00:02<00:00, 157Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 451/186688 [00:01<10:54, 284.48 examples/s]Converting format of dataset (num_proc=64):   5%|▍         | 8402/186688 [00:01<00:26, 6826.03 examples/s]Converting format of dataset (num_proc=64):  22%|██▏       | 41492/186688 [00:01<00:03, 41016.00 examples/s]Converting format of dataset (num_proc=64):  32%|███▏      | 59104/186688 [00:01<00:02, 58532.54 examples/s]Converting format of dataset (num_proc=64):  46%|████▋     | 86647/186688 [00:01<00:01, 92516.40 examples/s]Converting format of dataset (num_proc=64):  59%|█████▉    | 110860/186688 [00:02<00:00, 117996.89 examples/s]Converting format of dataset (num_proc=64):  71%|███████   | 132461/186688 [00:02<00:00, 137571.69 examples/s]Converting format of dataset (num_proc=64):  82%|████████▏ | 154003/186688 [00:02<00:00, 154426.19Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 536/186688 [00:01<09:01, 343.67 examples/s]Converting format of dataset (num_proc=64):   4%|▍         | 8031/186688 [00:01<00:27, 6561.76 examples/s]Converting format of dataset (num_proc=64):  23%|██▎       | 43796/186688 [00:01<00:03, 43868.09 examples/s]Converting format of dataset (num_proc=64):  34%|███▍      | 63176/186688 [00:01<00:01, 63107.25 examples/s]Converting format of dataset (num_proc=64):  48%|████▊     | 89461/186688 [00:01<00:01, 94192.07 examples/s]Converting format of dataset (num_proc=64):  61%|██████    | 114205/186688 [00:02<00:00, 122203.85 examples/s]Converting format of dataset (num_proc=64):  73%|███████▎  | 136511/186688 [00:02<00:00, 136377.97 examples/s]Converting format of dataset (num_proc=64):  85%|████████▌ | 158887/186688 [00:02<00:00, 154711.Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 531/186688 [00:01<09:10, 338.37 examples/s]Converting format of dataset (num_proc=64):   5%|▍         | 8856/186688 [00:01<00:24, 7256.05 examples/s]Converting format of dataset (num_proc=64):  22%|██▏       | 40158/186688 [00:01<00:03, 39706.46 examples/s]Converting format of dataset (num_proc=64):  32%|███▏      | 59970/186688 [00:01<00:02, 60297.84 examples/s]Converting format of dataset (num_proc=64):  45%|████▍     | 83484/186688 [00:01<00:01, 87537.14 examples/s]Converting format of dataset (num_proc=64):  59%|█████▉    | 110741/186688 [00:02<00:00, 121451.03 examples/s]Converting format of dataset (num_proc=64):  71%|███████▏  | 133035/186688 [00:02<00:00, 132360.27 examples/s]Converting format of dataset (num_proc=64):  82%|████████▏ | 153374/186688 [00:02<00:00, 146964.Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 538/186688 [00:01<08:30, 364.88 examples/s]Converting format of dataset (num_proc=64):   8%|▊         | 15418/186688 [00:01<00:12, 13389.88 examples/s]Converting format of dataset (num_proc=64):  25%|██▍       | 46334/186688 [00:01<00:03, 46217.21 examples/s]Converting format of dataset (num_proc=64):  38%|███▊      | 70513/186688 [00:01<00:01, 72955.90 examples/s]Converting format of dataset (num_proc=64):  49%|████▉     | 92364/186688 [00:01<00:00, 96632.90 examples/s]Converting format of dataset (num_proc=64):  64%|██████▎   | 118948/186688 [00:01<00:00, 128321.37 examples/s]Converting format of dataset (num_proc=64):  76%|███████▌  | 142033/186688 [00:02<00:00, 114761.77 examples/s]Converting format of dataset (num_proc=64):  86%|████████▌ | 160591/186688 [00:02<00:00, 125Converting format of dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   0%|          | 393/186688 [00:01<12:57, 239.60 examples/s]Converting format of dataset (num_proc=64):  11%|█▏        | 21145/186688 [00:01<00:09, 16797.26 examples/s]Converting format of dataset (num_proc=64):  24%|██▍       | 45591/186688 [00:01<00:03, 40102.70 examples/s]Converting format of dataset (num_proc=64):  37%|███▋      | 69145/186688 [00:01<00:01, 64976.41 examples/s]Converting format of dataset (num_proc=64):  49%|████▉     | 91451/186688 [00:02<00:01, 89116.05 examples/s]Converting format of dataset (num_proc=64):  60%|██████    | 112218/186688 [00:02<00:00, 109517.32 examples/s]Converting format of dataset (num_proc=64):  71%|███████▏  | 133444/186688 [00:02<00:00, 130137.87 examples/s]Converting format of dataset (num_proc=64):  83%|████████▎ | 154291/186688 [00:02<00:00, 146046.29 examples/s]Converting format of dataset (num_proc=64):  96%|█████████▌| 179683/186688 [00:02<00:00, 111373.39 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 46647.66 examples/s] 
xamples/s]Converting format of dataset (num_proc=64):  93%|█████████▎| 172840/186688 [00:02<00:00, 151996.98 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 45516.62 examples/s] 
248.07 examples/s]Converting format of dataset (num_proc=64):  96%|█████████▌| 178653/186688 [00:02<00:00, 126967.78 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 45049.27 examples/s] 
09 examples/s]Converting format of dataset (num_proc=64):  96%|█████████▋| 180109/186688 [00:02<00:00, 124115.38 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 44196.43 examples/s] 
663.84 examples/s]Converting format of dataset (num_proc=64): 100%|█████████▉| 186290/186688 [00:02<00:00, 103462.02 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 44070.45 examples/s] 
297.63 examples/s]Converting format of dataset (num_proc=64):  94%|█████████▎| 174853/186688 [00:02<00:00, 122525.71 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 43784.66 examples/s] 
71 examples/s]Converting format of dataset (num_proc=64):  93%|█████████▎| 173711/186688 [00:02<00:00, 137739.21 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 42596.25 examples/s] 
 examples/s]Converting format of dataset (num_proc=64):  94%|█████████▍| 175646/186688 [00:02<00:00, 131240.04 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:04<00:00, 42412.35 examples/s] 
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<43:00, 71.97 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<19:34, 157.28 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<07:45, 392.65 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:16<05:49, 519.91 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▍         | 8000/186688 [00:16<02:47, 1069.37 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:16<02:22, 1250.34 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:17<01:27, 2005.38 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:17<01:07, 2569.96 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         | 1Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<40:43, 75.99 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:13<17:34, 175.06 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:15<11:35, 263.93 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<07:35, 401.44 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:16<05:23, 562.16 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:16<02:52, 1039.46 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:16<01:35, 1849.84 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▌         | 11000/186688 [00:17<01:51, 1573.79 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         | 17Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<42:09, 73.41 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:14<11:59, 255.19 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<08:53, 342.68 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▍         | 8000/186688 [00:16<03:10, 935.79 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:16<02:50, 1039.41 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:17<02:33, 1152.84 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 13000/186688 [00:17<01:32, 1882.41 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:18<00:59, 2852.09 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<42:01, 73.63 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:15<19:54, 154.57 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:16<06:17, 480.72 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:16<02:27, 1201.60 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:17<01:58, 1475.04 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:17<01:31, 1893.93 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:17<01:10, 2413.79 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:18<01:01, 2724.67 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█        Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:14<43:44, 70.76 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<18:49, 163.56 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:15<10:51, 281.90 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:16<07:45, 392.59 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:16<05:13, 580.36 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 6000/186688 [00:17<04:11, 718.98 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:17<01:38, 1791.93 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:18<01:19, 2184.02 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         | 150Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<43:12, 71.62 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<19:31, 157.67 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:15<05:50, 518.60 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:16<04:14, 705.50 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:17<02:49, 1051.39 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:17<01:46, 1638.22 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         | 15000/186688 [00:18<01:15, 2274.86 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:18<00:58, 2903.06 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:14<44:01, 70.30 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:15<19:54, 154.63 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<07:51, 387.38 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:16<03:36, 831.17 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▌         | 11000/186688 [00:16<01:52, 1555.51 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:17<01:04, 2627.46 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         | 17000/186688 [00:17<01:03, 2693.21 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:19<01:31, 1840.01 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         |Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<41:48, 74.02 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<18:39, 164.99 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:14<05:33, 544.04 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:15<03:33, 840.33 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:16<02:38, 1120.39 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:16<02:29, 1179.28 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:17<01:49, 1601.14 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:18<01:51, 1546.56 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 00/186688 [00:18<00:58, 2910.09 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 20000/186688 [00:19<00:38, 4361.72 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:19<00:42, 3861.23 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:20<00:44, 3686.07 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:20<00:52, 3138.25 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:21<00:47, 3374.25 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:21<00:39, 4058.31 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:22<00:45, 3456.84 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:22<00:29, 5273.79 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:23<00:000/186688 [00:18<00:46, 3657.06 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         | 19000/186688 [00:18<00:40, 4178.94 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 20000/186688 [00:18<00:40, 4128.38 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:19<00:46, 3537.89 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:19<00:54, 3047.58 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:20<00:56, 2881.26 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:21<00:37, 4212.57 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:22<00:45, 3369.47 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:22<00:43, 3514.40 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:23<00:316000/186688 [00:19<01:47, 1583.52 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         | 19000/186688 [00:20<01:07, 2468.91 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:20<01:03, 2612.25 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:20<00:58, 2801.43 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:21<00:59, 2739.52 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:22<01:17, 2108.38 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:22<00:52, 3069.93 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:22<00:52, 3062.83 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:23<01:08, 2328.35 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:24<| 17000/186688 [00:18<01:01, 2759.96 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         | 19000/186688 [00:18<00:53, 3115.79 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:19<00:50, 3258.36 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:20<01:12, 2268.56 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:21<00:40, 3974.36 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:22<00:43, 3591.60 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:22<00:37, 4085.29 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:23<00:43, 3532.35 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:24<01:01, 2469.95 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:220000/186688 [00:19<00:54, 3067.13 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:19<00:44, 3678.02 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:20<00:43, 3728.32 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:20<00:36, 4348.99 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:21<00:37, 4216.91 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:21<00:43, 3606.69 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:24<02:03, 1247.89 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:25<01:50, 1390.76 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:26<01:55, 1323.50 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [007000/186688 [00:18<00:51, 3314.02 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 20000/186688 [00:18<00:43, 3873.48 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:19<00:43, 3811.96 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:19<00:36, 4375.19 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:20<00:34, 4594.95 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:20<00:39, 3972.51 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:22<01:14, 2099.19 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:23<01:04, 2394.86 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:25<01:40, 1515.83 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:26 | 20000/186688 [00:18<00:54, 3047.80 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:19<01:12, 2280.83 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:20<01:15, 2195.83 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:20<00:45, 3559.35 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:20<00:45, 3507.90 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:21<00:32, 4795.93 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:21<00:27, 5611.73 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:22<00:32, 4738.82 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:28<03:10, 790.91 examples/s] Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [001:49, 1443.35 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:25<01:25, 1836.40 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:26<01:42, 1508.21 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:27<01:44, 1468.28 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:28<01:46, 1432.88 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:29<01:40, 1497.08 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:29<01:26, 1731.71 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:30<00:59, 2477.53 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:30<00:59, 2448.80 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:30<00:50, 2877. 20000/186688 [00:19<01:16, 2171.36 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:20<01:05, 2503.39 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:20<00:57, 2810.26 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:21<00:39, 3974.77 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:21<00:45, 3479.43 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:22<00:33, 4641.33 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:24<01:00, 2492.71 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:29<02:21, 1056.40 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:30<02:27, 1006.95 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [0<02:02, 1241.79 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:27<01:41, 1479.90 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:27<01:31, 1641.68 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:28<01:30, 1647.92 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:29<02:13, 1108.03 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:30<02:06, 1157.66 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:31<02:04, 1173.87 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:31<01:32, 1566.67 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:31<01:23, 1720.25 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:32<01:176, 4163.30 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:25<01:27, 1723.18 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:28<02:40, 930.96 examples/s] Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:30<03:09, 782.65 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:30<02:29, 988.67 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:31<01:45, 1377.05 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:31<01:35, 1517.04 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:32<01:22, 1749.52 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:32<01:15, 1884.51 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<01:10, 2021.0:31<02:28, 992.21 examples/s] Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:31<02:08, 1142.51 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:32<01:38, 1471.87 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:32<01:26, 1651.66 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:32<01:11, 2005.56 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:33<00:56, 2493.31 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:33<00:48, 2898.42 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<00:53, 2592.25 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:33<00:43, 3210.04 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 0:29<02:49, 880.62 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:30<02:51, 869.26 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:31<02:36, 946.20 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:31<02:08, 1145.09 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:31<01:19, 1824.79 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:32<01:06, 2146.08 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:32<00:54, 2613.58 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<00:53, 2636.34 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<00:51, 2714.09 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:34<5<01:36, 1561.84 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:26<01:44, 1437.37 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:28<02:12, 1123.92 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:28<01:53, 1303.76 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:28<01:29, 1633.98 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:31<03:06, 780.33 examples/s] Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:32<02:49, 856.03 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:33<02:21, 1015.24 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:33<02:05, 1138.33 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:34<01:33, 4585.39 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:23<00:43, 3431.98 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:23<00:40, 3708.18 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:27<02:29, 995.12 examples/s] Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:29<02:37, 935.86 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:29<02:21, 1036.56 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:30<01:45, 1371.50 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:31<01:51, 1284.22 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:33<02:41, 882.66 examples/s] Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:34<02:33, 921:26<01:42, 1474.97 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:30<03:28, 722.63 examples/s] Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:30<02:43, 914.93 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:30<01:37, 1519.85 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:31<01:36, 1522.62 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:32<01:21, 1785.69 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:32<01:28, 1630.20 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:33<01:21, 1738.51 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:34<01:07, 2089.45 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:35<052 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:31<00:55, 2567.40 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:32<01:00, 2354.94 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<00:59, 2392.40 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:32<00:59, 2376.14 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<00:59, 2347.43 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:33<00:49, 2807.26 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:34<01:13, 1867.98 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:34<01:05, 2096.25 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:35<00:57, 23324 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:33<00:55, 2542.99 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<00:46, 2978.29 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:33<00:40, 3438.24 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:33<00:43, 3130.91 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:33<00:35, 3873.78 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:34<00:59, 2280.21 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:35<01:03, 2099.90 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:36<00:59, 2240.23 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:36<00:42, 306, 1851.36 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:33<01:47, 1320.92 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<01:08, 2050.69 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:34<01:05, 2110.36 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:34<00:54, 2530.91 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:35<01:03, 2143.54 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:35<00:57, 2347.37 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:35<00:47, 2806.61 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<00:48, 2754.65 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:36<00:01:07, 2047.48 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:34<00:46, 2926.22 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:34<00:49, 2735.71 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:35<00:47, 2793.37 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:35<00:45, 2894.23 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:36<00:39, 3288.08 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:36<00:36, 3588.43 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:36<00:33, 3891.52 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:37<00:32, 3923.14 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [25, 1641.82 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:34<01:11, 1947.92 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:35<01:13, 1897.88 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:35<01:10, 1947.68 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:36<01:06, 2068.20 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:36<00:53, 2529.91 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<00:44, 3017.04 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:37<00:50, 2652.93 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:38<00:59, 2199.84 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:38<01:13, 1882.83 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:35<01:00, 2257.92 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:35<00:57, 2358.22 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:36<01:04, 2099.32 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:36<00:52, 2567.17 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<00:42, 3121.44 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:36<00:35, 3739.46 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:38<01:10, 1862.14 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:38<00:55, 2352.21 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:384.83 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<00:59, 2260.64 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:36<00:45, 2869.54 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:36<00:39, 3333.07 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:36<00:27, 4671.42 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:37<00:29, 4401.95 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:37<00:20, 6019.06 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:37<00:19, 6328.59 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:37<00:19, 6274.89 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:3[00:34<00:35, 3885.47 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:34<00:48, 2845.57 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:35<01:00, 2234.36 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:36<01:24, 1591.83 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:36<00:57, 2322.69 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:37<00:51, 2534.84 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:37<00:58, 2216.84 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:38<01:27, 1481.58 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:39<01:22, 1554.36 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/18650, 2620.36 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:36<00:42, 3105.35 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:37<01:14, 1744.61 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:38<01:12, 1799.82 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:38<00:45, 2786.33 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:39<00:46, 2712.71 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:39<00:49, 2514.37 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:39<00:36, 3388.00 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:39<00:24, 4943.73 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186.00 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:34<02:04, 1125.94 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:35<01:34, 1473.35 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:36<01:53, 1221.18 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:36<01:37, 1406.09 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:36<01:13, 1852.97 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:37<01:37, 1387.55 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:38<01:24, 1594.17 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:40<02:18, 966.42 examples/s] Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:40<01:41, 134.59 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:37<01:00, 2152.69 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:37<00:48, 2653.05 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:38<01:08, 1875.04 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:38<01:02, 2014.95 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:39<00:40, 3081.15 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:39<00:40, 3027.74 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:39<00:40, 3000.59 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:40<00:37, 3218.22 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [<00:43, 3012.13 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:38<00:39, 3225.96 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:38<00:33, 3835.27 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:38<00:29, 4344.02 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:39<00:49, 2514.86 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:39<00:45, 2711.23 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:40<00:37, 3332.73 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:40<00:30, 4041.79 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:40<00:35, 3394.53 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 60:49, 2666.95 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:38<00:42, 3009.79 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:39<00:49, 2595.23 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:39<00:41, 3063.95 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:40<00:58, 2123.08 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:41<00:52, 2369.39 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:41<00:43, 2814.59 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:41<00:25, 4780.00 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:41<00:22, 5341.22 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000:37<00:31, 4084.48 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:37<00:45, 2776.60 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:38<00:36, 3349.38 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:39<01:08, 1798.02 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:40<01:20, 1520.71 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:41<01:09, 1734.49 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:41<00:48, 2452.56 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:41<00:41, 2815.18 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:41<00:36, 3230.11 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉  01.18 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:40<01:18, 1669.76 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:40<01:04, 2018.91 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:41<01:01, 2092.22 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:41<00:50, 2530.58 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:41<00:42, 2972.40 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:41<00:34, 3716.03 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:42<00:28, 4385.26 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61917/186688 [00:42<00:29, 4203.72 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 63917/186688 [00:8<01:00, 2029.28 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:39<00:51, 2367.15 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:39<00:40, 2946.31 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:40<00:44, 2692.90 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:40<00:28, 4043.11 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:40<00:24, 4630.49 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:40<00:27, 4182.34 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:41<00:57, 1989.12 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:42<00:32, 3362.88 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      688 [00:39<01:10, 1804.86 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:40<01:03, 1994.10 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:40<00:48, 2575.12 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:41<01:08, 1831.85 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:41<01:03, 1955.86 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:42<01:08, 1788.68 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:42<00:42, 2821.00 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:42<00:27, 4267.05 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:42<00:24, 4756.67 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███00:40<00:48, 2508.29 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:40<00:41, 2880.76 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:41<00:48, 2452.89 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:41<00:31, 3723.69 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:42<00:26, 4392.34 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:42<00:24, 4599.38 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:42<00:31, 3531.01 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:43<00:33, 3282.32 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:43<00:37, 2913.24 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████688 [00:40<00:32, 3740.95 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:40<00:31, 3771.51 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:40<00:28, 4181.18 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:42<01:08, 1712.77 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:42<00:57, 2041.64 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:42<00:34, 3351.61 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:43<00:33, 3410.83 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:43<00:34, 3293.36 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:43<00:31, 3553.33 examples/s]Running tokenizer on dataset (num_proc=64):  41%|███00/186688 [00:41<00:23, 4947.52 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:42<00:21, 5324.82 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:42<00:15, 7371.68 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:42<00:15, 7087.77 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:42<00:15, 7350.82 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76917/186688 [00:42<00:20, 5323.19 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78917/186688 [00:43<00:18, 5709.36 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 79917/186688 [00:43<00:21, 5035.41 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80917/186688 [00:44<00:35, 3014.79 examples/s]Running tokenizer on dataset (num_proc=64):  442<00:24, 4982.33 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 65834/186688 [00:42<00:17, 6747.05 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 66834/186688 [00:43<00:20, 5828.16 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 68834/186688 [00:43<00:15, 7564.18 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69834/186688 [00:43<00:16, 7113.53 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 70834/186688 [00:43<00:28, 4122.58 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71834/186688 [00:44<00:24, 4690.00 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 73834/186688 [00:44<00:20, 5465.41 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 74834/186688 [00:44<00:20, 5563.13 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████     6000/186688 [00:40<00:37, 3239.24 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:41<00:18, 6452.36 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:41<00:23, 4852.49 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:41<00:24, 4590.72 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:42<00:39, 2869.84 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:42<00:32, 3455.59 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:43<00:47, 2338.59 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:44<00:57, 1894.21 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80000/186688 [00:45<00:32, 3308.04 examples/s]Running tokenizer on dataset (num_proc=64):  44%▋      | 70000/186688 [00:43<00:28, 4151.56 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:43<00:30, 3783.26 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:43<00:36, 3122.98 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:44<00:40, 2840.25 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:44<00:43, 2564.16 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:45<00:39, 2846.50 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76917/186688 [00:45<00:27, 3932.70 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78917/186688 [00:45<00:19, 5641.45 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 79917/186688 [00:45<00:24, 4333.22 examples/s]Running tokenizer on dataset (num_    | 73000/186688 [00:42<00:23, 4769.53 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:42<00:23, 4868.54 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:42<00:25, 4427.50 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:42<00:27, 4025.93 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:43<00:34, 3172.55 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78000/186688 [00:43<00:38, 2826.39 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 79000/186688 [00:44<00:36, 2936.22 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 81000/186688 [00:44<00:35, 2965.38 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82000/186688 [00:45<00:40, 2560.58 examples/s]Running tokenizer on dataset (num_p| 77000/186688 [00:43<00:42, 2581.28 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78000/186688 [00:43<00:43, 2497.37 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 79000/186688 [00:44<00:50, 2127.61 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80000/186688 [00:44<00:44, 2417.02 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 81000/186688 [00:44<00:36, 2924.31 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 83000/186688 [00:45<00:39, 2637.45 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 84000/186688 [00:45<00:41, 2487.63 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85000/186688 [00:46<00:33, 3000.14 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 86000/186688 [00:46<00:41, 2421.43 examples/s]Running tokenizer on dataset (n | 76834/186688 [00:44<00:15, 6919.41 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 77834/186688 [00:44<00:17, 6306.25 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78751/186688 [00:44<00:16, 6674.77 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80751/186688 [00:45<00:24, 4307.68 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82751/186688 [00:46<00:25, 4071.59 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83751/186688 [00:46<00:26, 3937.51 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85668/186688 [00:46<00:19, 5212.68 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▋     | 86668/186688 [00:47<00:24, 4054.38 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87668/186688 [00:48<00:44, 2227.16 examples/s]Running tokenizer on dataset (proc=64):  43%|████▎     | 80834/186688 [00:45<00:22, 4806.21 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 81834/186688 [00:46<00:28, 3682.24 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83751/186688 [00:46<00:24, 4281.95 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84751/186688 [00:47<00:23, 4318.84 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85751/186688 [00:47<00:20, 4948.25 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▋     | 86668/186688 [00:47<00:18, 5277.84 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87668/186688 [00:48<00:38, 2600.94 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 88668/186688 [00:48<00:30, 3255.51 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90668/186688 [00:48<00:20, 4601.04 examples█      | 76000/186688 [00:44<00:36, 3005.02 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76917/186688 [00:44<00:30, 3657.17 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 77917/186688 [00:44<00:25, 4201.07 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 79917/186688 [00:44<00:17, 6115.47 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80917/186688 [00:44<00:15, 6703.50 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82917/186688 [00:45<00:29, 3497.14 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84917/186688 [00:46<00:30, 3308.17 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 86917/186688 [00:47<00:37, 2664.90 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87917/186688 [00:48<00:56, 1747.72 examples/s]Running tokenizer on dat     | 78000/186688 [00:43<00:30, 3514.66 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 79000/186688 [00:44<00:45, 2373.54 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 81000/186688 [00:45<00:35, 2953.46 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82917/186688 [00:45<00:27, 3723.13 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83917/186688 [00:45<00:32, 3118.01 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84917/186688 [00:46<00:39, 2577.66 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85917/186688 [00:46<00:38, 2618.25 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 86834/186688 [00:47<00:39, 2550.94 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87834/186688 [00:47<00:40, 2454.28 examples/s]Running tokenizer on data4%|████▍     | 81917/186688 [00:44<00:29, 3521.49 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82917/186688 [00:45<00:45, 2282.95 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83917/186688 [00:46<00:59, 1733.95 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84917/186688 [00:46<00:53, 1885.02 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85917/186688 [00:47<00:48, 2087.03 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 86917/186688 [00:47<00:54, 1826.10 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87917/186688 [00:47<00:41, 2394.54 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89917/186688 [00:48<00:33, 2852.80 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90834/186688 [00:49<00:57, 1674.92 examples/s]Running |████▍     | 81917/186688 [00:45<00:28, 3615.41 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82834/186688 [00:45<00:29, 3542.87 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83834/186688 [00:46<00:41, 2468.59 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84834/186688 [00:47<00:39, 2552.64 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▋     | 86751/186688 [00:47<00:35, 2797.20 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87751/186688 [00:48<00:35, 2766.38 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 88668/186688 [00:48<00:30, 3244.75 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89668/186688 [00:48<00:42, 2297.42 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90668/186688 [00:49<00:41, 2289.54 examples/s]Running toroc=64):  45%|████▍     | 84000/186688 [00:46<00:49, 2060.14 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85000/186688 [00:47<00:47, 2129.53 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 86917/186688 [00:47<00:31, 3120.14 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 88834/186688 [00:47<00:25, 3907.68 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90668/186688 [00:48<00:30, 3166.32 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91585/186688 [00:49<00:55, 1718.94 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92585/186688 [00:51<01:08, 1383.01 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93585/186688 [00:51<01:01, 1510.59 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94585/186688 [00:52<00:55, 1663.33 examples/um_proc=64):  47%|████▋     | 88000/186688 [00:47<00:36, 2702.68 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89000/186688 [00:47<00:32, 2984.14 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90834/186688 [00:48<00:30, 3141.52 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91751/186688 [00:48<00:35, 2643.83 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92668/186688 [00:49<00:39, 2403.21 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94585/186688 [00:49<00:30, 3044.06 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95502/186688 [00:50<00:49, 1848.01 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96419/186688 [00:51<01:00, 1484.95 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97336/186688 [00:52<00:58, 1517.68 easet (num_proc=64):  48%|████▊     | 88834/186688 [00:48<00:51, 1917.00 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90751/186688 [00:49<00:36, 2626.78 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91751/186688 [00:49<00:41, 2311.37 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92668/186688 [00:49<00:34, 2702.17 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93585/186688 [00:50<00:29, 3160.35 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94502/186688 [00:50<00:40, 2251.34 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95419/186688 [00:51<00:43, 2082.95 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96336/186688 [00:51<00:36, 2455.02 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97336/186688 [00:52<00:58, 15set (num_proc=64):  48%|████▊     | 88834/186688 [00:49<01:11, 1370.62 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89751/186688 [00:50<01:34, 1028.59 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90751/186688 [00:51<01:33, 1025.19 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91668/186688 [00:51<01:09, 1357.65 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92585/186688 [00:52<00:56, 1658.33 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93502/186688 [00:52<00:45, 2060.52 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94419/186688 [00:52<00:41, 2207.63 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95336/186688 [00:52<00:34, 2639.47 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96336/186688 [00:52<00:26, 3401.num_proc=64):  48%|████▊     | 89668/186688 [00:48<00:28, 3453.84 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90668/186688 [00:48<00:25, 3830.26 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91668/186688 [00:48<00:23, 3990.46 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92668/186688 [00:48<00:22, 4241.94 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93668/186688 [00:49<00:19, 4694.25 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95668/186688 [00:50<00:30, 3020.85 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97585/186688 [00:52<00:54, 1646.83 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98585/186688 [00:52<00:45, 1924.60 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99585/186688 [00:53<01:00, 1429.9kenizer on dataset (num_proc=64):  49%|████▉     | 91585/186688 [00:50<00:55, 1700.10 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92585/186688 [00:50<00:50, 1876.36 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93502/186688 [00:51<01:01, 1510.64 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94419/186688 [00:52<01:00, 1531.62 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95419/186688 [00:52<00:50, 1792.71 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96336/186688 [00:52<00:48, 1877.78 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97336/186688 [00:53<00:56, 1572.41 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99253/186688 [00:53<00:32, 2651.89 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100253/1866xamples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98253/186688 [00:52<00:46, 1919.02 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99253/186688 [00:52<00:34, 2511.35 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100170/186688 [00:52<00:37, 2311.63 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102087/186688 [00:53<00:25, 3273.42 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103087/186688 [00:53<00:26, 3098.79 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104004/186688 [00:54<00:31, 2636.58 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 105004/186688 [00:54<00:32, 2503.31 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106004/186688 [00:54<00:28, 2794.98 examples/s]Running tokenizer on dataset (num_proc=64):  5725 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99253/186688 [00:53<00:14, 5899.62 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101087/186688 [00:54<00:22, 3740.03 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102087/186688 [00:54<00:20, 4182.90 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103087/186688 [00:54<00:22, 3721.51 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104004/186688 [00:54<00:20, 4121.69 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104921/186688 [00:54<00:20, 3992.02 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 105838/186688 [00:55<00:18, 4312.01 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106755/186688 [00:55<00:16, 4862.78 examples/s]Running tokenizer on dataset (num_proc=64)tokenizer on dataset (num_proc=64):  49%|████▉     | 91834/186688 [00:50<00:53, 1757.67 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92834/186688 [00:50<00:41, 2257.08 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93751/186688 [00:50<00:43, 2150.91 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94668/186688 [00:50<00:35, 2623.02 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96585/186688 [00:51<00:31, 2848.06 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97502/186688 [00:52<00:50, 1750.42 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98502/186688 [00:53<00:57, 1529.06 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99419/186688 [00:54<01:09, 1262.30 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 100419//s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92668/186688 [00:50<00:45, 2059.25 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93668/186688 [00:51<01:06, 1402.68 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95502/186688 [00:52<00:46, 1951.43 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96502/186688 [00:52<00:48, 1841.10 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98419/186688 [00:52<00:32, 2746.44 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99336/186688 [00:53<00:30, 2888.35 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100253/186688 [00:54<00:48, 1765.02 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101170/186688 [00:55<01:02, 1363.91 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████22.61 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98253/186688 [00:53<00:52, 1694.53 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99170/186688 [00:53<00:45, 1944.77 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100170/186688 [00:53<00:39, 2181.16 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101170/186688 [00:54<00:52, 1642.21 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102170/186688 [00:55<00:46, 1812.13 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103087/186688 [00:55<00:42, 1945.63 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104004/186688 [00:55<00:34, 2380.91 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 105004/186688 [00:55<00:29, 2726.78 examples/s]Running tokenizer on dataset (num_proc=6s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95502/186688 [00:52<00:57, 1594.07 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97336/186688 [00:52<00:38, 2350.02 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98336/186688 [00:53<00:30, 2879.64 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99336/186688 [00:53<00:31, 2765.08 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 100253/186688 [00:53<00:34, 2511.16 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101170/186688 [00:55<00:52, 1616.69 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102170/186688 [00:55<00:41, 2057.22 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103170/186688 [00:56<00:52, 1591.32 examples/s]Running tokenizer on dataset (num_proc=64):  56%|███%|█████▋    | 106921/186688 [00:55<00:28, 2790.57 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108755/186688 [00:55<00:18, 4114.40 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109755/186688 [00:56<00:30, 2513.73 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111589/186688 [00:56<00:22, 3329.85 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112589/186688 [00:56<00:18, 3909.00 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113589/186688 [00:56<00:16, 4509.28 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116423/186688 [00:56<00:10, 6703.26 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117340/186688 [00:57<00:16, 4223.14 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118257/186688 [00:57<00:7 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101419/186688 [00:54<00:46, 1824.91 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102336/186688 [00:54<00:44, 1884.28 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103336/186688 [00:55<00:43, 1937.25 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▋    | 105253/186688 [00:55<00:32, 2537.65 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106170/186688 [00:56<00:38, 2089.51 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 107087/186688 [00:57<00:47, 1686.14 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108004/186688 [00:57<00:42, 1865.86 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109838/186688 [00:58<00:41, 1865.35 examples/s]Running tokenizer on dataset (num_proc=64)186688 [00:55<01:12, 1182.75 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102336/186688 [00:55<00:43, 1950.43 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103253/186688 [00:56<00:41, 2013.53 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104170/186688 [00:57<00:49, 1654.24 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▋    | 105087/186688 [00:57<00:38, 2097.40 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106004/186688 [00:57<00:32, 2512.50 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108004/186688 [00:57<00:24, 3165.07 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 109004/186688 [00:58<00:24, 3136.77 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110921/186688 [00:58<00:16, 4619.81 examples/s]Running tokeniz4):  57%|█████▋    | 106838/186688 [00:56<00:18, 4360.45 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 107838/186688 [00:56<00:25, 3143.56 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109755/186688 [00:57<00:19, 3848.26 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111755/186688 [00:57<00:13, 5404.02 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112672/186688 [00:57<00:15, 4930.12 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113589/186688 [00:57<00:19, 3833.38 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114506/186688 [00:58<00:21, 3357.05 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115506/186688 [00:58<00:19, 3594.42 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116423/186688 [00█▌    | 105004/186688 [00:56<00:31, 2628.71 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 105921/186688 [00:56<00:28, 2845.96 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 107838/186688 [00:56<00:23, 3367.26 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108838/186688 [00:57<00:25, 3001.96 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110672/186688 [00:57<00:23, 3262.83 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111589/186688 [00:57<00:20, 3709.21 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112589/186688 [00:58<00:19, 3851.19 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113589/186688 [00:58<00:25, 2891.37 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114589/186688 [00:59<00:22, 3181.51 exam:  58%|█████▊    | 108755/186688 [00:55<00:13, 5696.69 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109755/186688 [00:55<00:18, 4184.39 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111755/186688 [00:56<00:24, 3050.61 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112672/186688 [00:58<00:42, 1734.16 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113589/186688 [00:58<00:35, 2046.31 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114506/186688 [00:58<00:32, 2244.48 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116423/186688 [00:58<00:20, 3471.07 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117340/186688 [00:58<00:17, 4005.48 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120174/186688 [00▍    | 102087/186688 [00:55<00:54, 1556.59 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103004/186688 [00:56<00:45, 1827.44 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104004/186688 [00:56<00:45, 1824.84 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 105004/186688 [00:56<00:34, 2363.55 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106838/186688 [00:57<00:32, 2471.73 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 107755/186688 [00:57<00:29, 2677.50 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108672/186688 [00:58<00:32, 2402.38 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110589/186688 [00:58<00:25, 2927.57 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111589/186688 [00:59<00:30, 2462.22 examples/s]88 [00:54<00:43, 1990.06 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101170/186688 [00:55<00:40, 2112.90 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102087/186688 [00:55<00:35, 2404.02 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104087/186688 [00:56<00:32, 2530.03 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106004/186688 [00:57<00:36, 2231.81 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 107004/186688 [00:57<00:30, 2642.61 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 107921/186688 [00:58<00:46, 1703.87 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108838/186688 [00:58<00:40, 1924.14 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109755/186688 [00:58<00:33, 2268.89 examples/s]Running tokenizer o14, 4733.65 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120091/186688 [00:57<00:12, 5512.48 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121008/186688 [00:58<00:16, 4045.84 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 122842/186688 [00:58<00:11, 5349.19 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 123759/186688 [00:58<00:16, 3705.20 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124759/186688 [00:59<00:16, 3700.47 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125676/186688 [00:59<00:15, 3848.37 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126593/186688 [00:59<00:14, 4177.91 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127510/186688 [00:59<00:12, 4674.04 examples/s]Running tokenizeer on dataset (num_proc=64):  60%|█████▉    | 111921/186688 [00:58<00:16, 4453.86 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113838/186688 [00:59<00:18, 3954.16 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114755/186688 [00:59<00:18, 3814.86 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115672/186688 [00:59<00:16, 4289.52 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117506/186688 [00:59<00:14, 4618.74 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118423/186688 [01:00<00:18, 3741.24 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119423/186688 [01:00<00:20, 3243.10 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120340/186688 [01:00<00:18, 3670.45 examples/s]Running tokenizer on dataset (num_proc=64):  65%|███n dataset (num_proc=64):  60%|██████    | 112672/186688 [00:59<00:19, 3725.44 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114506/186688 [00:59<00:15, 4674.75 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115506/186688 [00:59<00:14, 5007.58 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116423/186688 [00:59<00:13, 5096.37 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118340/186688 [01:00<00:10, 6445.54 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119340/186688 [01:00<00:12, 5320.90 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120257/186688 [01:00<00:16, 3957.98 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122174/186688 [01:01<00:15, 4148.43 examples/s]Running tokenizer on dataset (num_proc=64):  66%|████:58<00:21, 3300.78 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117423/186688 [00:58<00:18, 3844.84 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118340/186688 [00:59<00:16, 4043.04 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119257/186688 [00:59<00:20, 3291.74 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120174/186688 [01:00<00:27, 2455.19 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121091/186688 [01:00<00:25, 2608.30 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122008/186688 [01:00<00:22, 2925.57 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 122925/186688 [01:01<00:25, 2518.61 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124842/186688 [01:01<00:17, 3493.23 examples/s]Running tples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116423/186688 [00:59<00:15, 4650.66 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117423/186688 [00:59<00:13, 5195.08 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119257/186688 [00:59<00:16, 4042.19 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120257/186688 [01:00<00:14, 4515.00 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122091/186688 [01:00<00:11, 5427.13 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 123925/186688 [01:00<00:10, 5922.72 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124925/186688 [01:00<00:09, 6329.45 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125842/186688 [01:00<00:12, 4818.14 examples/s]Running tokenizer on dataset (nu:  59%|█████▉    | 110838/186688 [00:58<00:32, 2299.45 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111755/186688 [00:59<00:43, 1709.97 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114589/186688 [00:59<00:24, 2912.12 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115589/186688 [00:59<00:21, 3317.66 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118340/186688 [01:00<00:14, 4771.61 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120174/186688 [01:01<00:25, 2567.13 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121091/186688 [01:01<00:24, 2695.68 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122008/186688 [01:02<00:28, 2251.49 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 122925/1866r on dataset (num_proc=64):  69%|██████▉   | 129427/186688 [00:59<00:09, 6036.68 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130344/186688 [01:00<00:13, 4029.30 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131261/186688 [01:00<00:12, 4432.12 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132178/186688 [01:00<00:12, 4371.57 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133178/186688 [01:00<00:10, 5026.67 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134178/186688 [01:01<00:21, 2424.04 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135095/186688 [01:02<00:18, 2803.90 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136012/186688 [01:02<00:15, 3321.64 examples/s]Running tokenizer on dataset (num_proc=64):  73%Running tokenizer on dataset (num_proc=64):  60%|██████    | 112589/186688 [00:59<00:35, 2113.88 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114506/186688 [01:00<00:24, 2951.32 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115423/186688 [01:00<00:21, 3383.06 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116340/186688 [01:00<00:20, 3366.03 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117257/186688 [01:00<00:21, 3235.97 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120008/186688 [01:02<00:23, 2858.04 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 120925/186688 [01:02<00:28, 2311.72 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 123842/186688 [01:02<00:15, 4016.98 examples/s]Running tokenizer on dataset (num_proc=64:59<00:10, 6262.22 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122091/186688 [00:59<00:12, 5084.52 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 123925/186688 [01:00<00:17, 3635.56 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124925/186688 [01:00<00:18, 3295.76 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125842/186688 [01:01<00:18, 3271.79 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127676/186688 [01:01<00:17, 3460.47 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128676/186688 [01:02<00:19, 2992.53 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129593/186688 [01:02<00:21, 2716.33 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130593/186688 [01:03<00:24, 2323.83 examples/s]Running tm_proc=64):  68%|██████▊   | 127676/186688 [01:01<00:17, 3368.28 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128593/186688 [01:01<00:15, 3666.45 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129510/186688 [01:02<00:14, 3819.96 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131427/186688 [01:02<00:09, 5680.36 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132344/186688 [01:02<00:11, 4844.07 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133344/186688 [01:02<00:10, 4911.21 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134261/186688 [01:03<00:12, 4110.78 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135178/186688 [01:03<00:12, 4140.64 examples/s]Running tokenizer on dataset (num_proc=64):  73%|█████okenizer on dataset (num_proc=64):  67%|██████▋   | 125759/186688 [01:01<00:18, 3370.73 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126676/186688 [01:01<00:15, 3868.27 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127676/186688 [01:02<00:14, 4117.50 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128593/186688 [01:02<00:13, 4361.08 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129510/186688 [01:02<00:11, 4811.48 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130427/186688 [01:03<00:18, 2965.86 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131344/186688 [01:03<00:18, 2943.57 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132344/186688 [01:03<00:16, 3265.46 examples/s]Running tokenizer on dataset (num_proc=64):  72%|██▌   | 122174/186688 [01:01<00:16, 3916.02 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 123091/186688 [01:01<00:18, 3454.92 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124925/186688 [01:01<00:14, 4186.79 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125842/186688 [01:02<00:14, 4146.70 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127676/186688 [01:02<00:10, 5437.46 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129510/186688 [01:02<00:09, 5742.90 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130427/186688 [01:03<00:18, 2980.97 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131344/186688 [01:03<00:17, 3149.31 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133178/186688 [01:03<0██▌   | 123091/186688 [01:01<00:14, 4399.00 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 124091/186688 [01:01<00:12, 4851.43 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125008/186688 [01:02<00:19, 3120.12 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▊   | 127925/186688 [01:02<00:13, 4488.05 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 130759/186688 [01:02<00:09, 5598.27 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 131759/186688 [01:02<00:09, 6002.67 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 133593/186688 [01:03<00:07, 7191.03 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136427/186688 [01:03<00:09, 5317.29 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138261/186688 [01:04|███████▎  | 136929/186688 [01:02<00:21, 2297.22 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 137846/186688 [01:03<00:19, 2475.84 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139763/186688 [01:03<00:11, 4068.01 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140763/186688 [01:03<00:09, 4732.52 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141763/186688 [01:03<00:08, 5065.81 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142680/186688 [01:03<00:09, 4405.65 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143597/186688 [01:04<00:11, 3718.75 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144597/186688 [01:04<00:14, 2869.49 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████88 [01:02<00:25, 2497.45 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124759/186688 [01:03<00:21, 2894.68 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125676/186688 [01:03<00:21, 2887.04 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127510/186688 [01:03<00:14, 4157.40 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129427/186688 [01:04<00:12, 4581.85 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132178/186688 [01:04<00:10, 5358.00 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133095/186688 [01:04<00:10, 5199.45 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134012/186688 [01:05<00:11, 4473.86 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 135846/186688 [01:05<00:10, 4800.38 examples/okenizer on dataset (num_proc=64):  70%|███████   | 131510/186688 [01:03<00:19, 2790.79 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132427/186688 [01:03<00:23, 2353.01 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134261/186688 [01:04<00:14, 3650.72 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135261/186688 [01:04<00:12, 4002.33 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136178/186688 [01:04<00:11, 4504.98 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 137095/186688 [01:04<00:11, 4468.66 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138012/186688 [01:04<00:11, 4395.08 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138929/186688 [01:05<00:13, 3669.96 examples/s]Running tokenizer on dataset (num_pro):  67%|██████▋   | 125759/186688 [01:03<00:14, 4219.95 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127676/186688 [01:03<00:14, 3944.01 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128593/186688 [01:04<00:14, 3885.17 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129510/186688 [01:04<00:14, 4063.83 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130427/186688 [01:04<00:18, 3117.82 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131344/186688 [01:05<00:16, 3286.20 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132344/186688 [01:05<00:17, 3078.08 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134178/186688 [01:05<00:14, 3619.82 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 13█▎  | 136095/186688 [01:03<00:15, 3243.20 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138012/186688 [01:03<00:10, 4824.84 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138929/186688 [01:04<00:13, 3605.94 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139929/186688 [01:04<00:16, 2783.23 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140846/186688 [01:05<00:14, 3230.87 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141763/186688 [01:05<00:18, 2407.63 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142680/186688 [01:05<00:15, 2885.01 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143680/186688 [01:06<00:13, 3233.26 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144680/1866880:13, 4075.94 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134178/186688 [01:04<00:12, 4350.10 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135095/186688 [01:04<00:11, 4474.34 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136012/186688 [01:04<00:15, 3367.49 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136929/186688 [01:05<00:15, 3237.14 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138763/186688 [01:05<00:13, 3559.21 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139680/186688 [01:06<00:19, 2454.36 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141680/186688 [01:06<00:11, 3891.19 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142680/186688 [01:06<00:10, 4352.07 examples/ss]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 137763/186688 [01:05<00:08, 6085.21 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138763/186688 [01:05<00:10, 4592.48 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139763/186688 [01:06<00:09, 4940.38 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141680/186688 [01:06<00:08, 5541.25 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142597/186688 [01:06<00:09, 4634.68 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143597/186688 [01:06<00:08, 5215.02 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144514/186688 [01:07<00:08, 4771.30 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145431/186688 [01:07<00:11, 3716.88 examples/s]Running tokenizer on ███████▏  | 135095/186688 [01:03<00:09, 5284.86 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 137012/186688 [01:04<00:09, 5312.90 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 137929/186688 [01:04<00:14, 3408.04 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138846/186688 [01:05<00:14, 3348.93 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139846/186688 [01:06<00:26, 1760.52 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140763/186688 [01:06<00:23, 1941.12 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141680/186688 [01:07<00:24, 1823.39 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142597/186688 [01:07<00:22, 1996.00 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋<00:08, 5871.70 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139178/186688 [01:04<00:09, 4948.09 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140095/186688 [01:04<00:10, 4428.74 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141929/186688 [01:05<00:10, 4358.75 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 142846/186688 [01:05<00:09, 4778.16 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143763/186688 [01:07<00:27, 1542.71 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144680/186688 [01:07<00:29, 1447.76 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146514/186688 [01:08<00:18, 2204.94 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147431/186688 [01:08<00:18, 2120.69 examples7095/186688 [01:06<00:08, 5645.58 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138012/186688 [01:06<00:10, 4748.05 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 139012/186688 [01:06<00:10, 4484.34 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140846/186688 [01:06<00:09, 4767.40 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141763/186688 [01:07<00:09, 4828.53 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142763/186688 [01:07<00:11, 3947.12 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143680/186688 [01:08<00:13, 3095.64 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144597/186688 [01:08<00:14, 2954.12 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145597/186688 [01:08<00:15c=64):  75%|███████▍  | 139929/186688 [01:05<00:20, 2262.04 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140846/186688 [01:06<00:22, 2045.79 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 142846/186688 [01:07<00:17, 2543.96 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143763/186688 [01:07<00:19, 2228.47 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145680/186688 [01:08<00:16, 2529.51 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147514/186688 [01:08<00:11, 3475.02 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 149431/186688 [01:09<00:12, 3022.52 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 150431/186688 [01:09<00:11, 3193.53 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143597/186688 [01:07<00:12, 3491.77 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144514/186688 [01:07<00:15, 2719.87 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145431/186688 [01:07<00:12, 3282.23 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146431/186688 [01:07<00:09, 4067.21 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147348/186688 [01:08<00:13, 2843.57 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148265/186688 [01:08<00:15, 2515.70 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150182/186688 [01:09<00:10, 3535.83 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151099/186688 [01:09<00:10, 3455.84 examples/s]Running tokenizer on d  | 146514/186688 [01:05<00:12, 3253.47 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147431/186688 [01:06<00:19, 2027.69 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148348/186688 [01:07<00:25, 1529.15 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149265/186688 [01:07<00:20, 1790.80 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151182/186688 [01:07<00:12, 2839.53 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████▏ | 152099/186688 [01:08<00:12, 2708.62 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153099/186688 [01:09<00:17, 1956.67 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 154016/186688 [01:09<00:17, 1875.80 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 156016/1866 [01:06<00:11, 3728.01 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145597/186688 [01:06<00:09, 4133.24 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▊  | 146597/186688 [01:07<00:19, 2005.21 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 148514/186688 [01:08<00:16, 2264.02 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 150348/186688 [01:09<00:18, 1962.67 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 152182/186688 [01:09<00:13, 2548.92 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153099/186688 [01:10<00:15, 2150.54 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 154016/186688 [01:10<00:13, 2452.71 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154933/186688 [01:11<00:16, 1  | 143514/186688 [01:08<00:21, 2049.90 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145348/186688 [01:08<00:18, 2186.25 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148182/186688 [01:09<00:09, 3966.96 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150016/186688 [01:09<00:09, 4018.49 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151016/186688 [01:09<00:09, 3589.62 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████▏ | 152016/186688 [01:10<00:12, 2729.47 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153016/186688 [01:11<00:12, 2595.26 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153933/186688 [01:11<00:13, 2454.02 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154933/18668, 2623.76 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 148514/186688 [01:09<00:07, 5133.23 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 150431/186688 [01:09<00:06, 5475.40 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151348/186688 [01:09<00:08, 4120.93 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 152265/186688 [01:10<00:12, 2725.17 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153265/186688 [01:10<00:11, 3030.27 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154182/186688 [01:11<00:10, 3104.64 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 155099/186688 [01:11<00:10, 3053.56 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 156016/186688 [01:13<00:29, 1030.52 exam████  | 151348/186688 [01:09<00:09, 3677.74 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 152265/186688 [01:10<00:12, 2822.11 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153182/186688 [01:10<00:11, 2985.22 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154099/186688 [01:11<00:14, 2215.36 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 155099/186688 [01:11<00:14, 2170.67 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 157016/186688 [01:12<00:14, 2004.23 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157933/186688 [01:12<00:12, 2250.26 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:13<00:13, 2028.69 examples/s]Running tokenizer on dataset (num_proc=64):  86%|███████dataset (num_proc=64):  78%|███████▊  | 146348/186688 [01:07<00:12, 3342.57 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148265/186688 [01:08<00:09, 4267.04 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149182/186688 [01:09<00:23, 1618.91 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150099/186688 [01:10<00:24, 1494.22 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151016/186688 [01:10<00:18, 1900.36 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████▏ | 152016/186688 [01:11<00:21, 1632.36 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153016/186688 [01:12<00:21, 1547.82 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 154016/186688 [01:12<00:18, 1762.86 examples/s]Running tokenizer on dataset (num_proc=/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148348/186688 [01:09<00:25, 1500.89 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150182/186688 [01:10<00:15, 2312.18 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151099/186688 [01:10<00:17, 2031.69 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 152933/186688 [01:12<00:20, 1627.46 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153933/186688 [01:13<00:25, 1271.08 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156850/186688 [01:14<00:16, 1837.76 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:14<00:13, 2110.52 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:16<00:19, 1401.45 examples/s]Running toataset (num_proc=64):  81%|████████▏ | 152016/186688 [01:09<00:10, 3424.15 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153016/186688 [01:10<00:19, 1770.45 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153933/186688 [01:11<00:16, 1955.84 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154933/186688 [01:12<00:21, 1478.15 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 155933/186688 [01:12<00:16, 1905.12 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156933/186688 [01:14<00:27, 1079.85 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:14<00:20, 1411.31 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:16<00:27, 997.37 examples/s] Running tokenizer on dataset (88 [01:09<00:09, 3070.05 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 157016/186688 [01:09<00:09, 3264.10 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157933/186688 [01:10<00:12, 2383.03 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:11<00:18, 1539.27 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:13<00:21, 1247.96 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:13<00:18, 1403.65 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:20<01:01, 402.89 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:22<00:57, 415.77 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:278 [01:12<00:21, 1470.31 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 155933/186688 [01:13<00:19, 1572.19 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156933/186688 [01:13<00:16, 1789.72 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:14<00:15, 1873.81 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:14<00:11, 2373.66 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:15<00:20, 1309.81 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:20<00:46, 560.14 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:26<01:19, 312.39 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:27<871.32 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 155933/186688 [01:12<00:18, 1700.77 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:12<00:10, 2733.88 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:15<00:27, 1008.98 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:15<00:21, 1271.27 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:17<00:26, 985.14 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:26<01:26, 288.81 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:28<00:53, 427.85 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:30<00:48, 454.97 exam64):  83%|████████▎ | 154933/186688 [01:14<00:27, 1164.39 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 155933/186688 [01:15<00:27, 1124.24 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:15<00:19, 1501.84 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:16<00:15, 1814.38 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:18<00:23, 1078.36 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:26<01:00, 412.52 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162850/186688 [01:30<01:05, 363.49 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:31<00:59, 386.44 examples/s]Running tokenizer on dataset (num_proc=64):  88%|ples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157933/186688 [01:14<00:19, 1470.88 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:15<00:20, 1386.50 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:15<00:15, 1786.81 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:17<00:28, 891.40 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161767/186688 [01:30<01:51, 223.05 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:31<01:21, 293.69 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:31<00:56, 404.48 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:33<00:50, 436.04 examples/s]Runn▌ | 159850/186688 [01:13<00:12, 2193.45 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:16<00:27, 932.81 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:19<00:42, 587.91 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162850/186688 [01:25<01:09, 342.24 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:27<01:02, 364.06 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:29<00:53, 409.40 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:34<01:09, 302.07 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:36<00:57, 346.26 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767kenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:17<00:22, 1193.79 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:19<00:28, 920.95 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161767/186688 [01:23<00:51, 484.37 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:26<00:51, 461.71 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:29<00:57, 399.99 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:32<00:55, 395.20 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:34<00:50, 417.49 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:35<00:41, 483.29 examples/s]Running tokenizer on dat<01:11, 321.17 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:28<00:57, 383.72 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:29<00:42, 490.32 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:31<00:39, 506.78 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:32<00:31, 598.68 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:32<00:24, 738.30 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:34<00:23, 717.61 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:35<00:22, 711.62 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:39<00:30, 484.num_proc=64):  86%|████████▌ | 160850/186688 [01:18<00:30, 845.77 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161767/186688 [01:20<00:34, 720.92 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:25<00:51, 462.41 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:31<01:18, 290.66 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:36<01:21, 268.77 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:37<00:59, 353.21 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:38<00:51, 389.98 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:39<00:38, 496.65 examples/s]Running tokenizer on dataset (num_proc=64):  9ples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:33<00:52, 397.31 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:36<00:50, 393.83 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:37<00:38, 487.77 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:37<00:27, 648.48 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:38<00:22, 758.27 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:39<00:19, 809.13 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:43<00:31, 479.73 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:45<00:27, 503.91 examples/s]Ru01:00, 398.50 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:31<01:05, 348.36 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:32<00:56, 388.92 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:36<00:58, 357.50 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:38<00:53, 375.12 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:41<00:36, 495.30 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:41<00:28, 600.16 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:42<00:22, 712.08 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:45<00:29, 510.2████████▊ | 164767/186688 [01:34<00:59, 370.15 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:36<00:50, 414.81 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:37<00:39, 498.21 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:37<00:28, 675.27 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:39<00:25, 696.72 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:41<00:30, 555.25 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:44<00:24, 598.97 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:47<00:28, 496.85 examples/s]Running tokenizer on dataset (num_proc=64):  93%|████/186688 [01:37<00:41, 458.24 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:37<00:29, 611.33 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:38<00:22, 741.27 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:38<00:15, 1025.77 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:42<00:28, 522.02 examples/s] Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173684/186688 [01:44<00:19, 661.01 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:44<00:14, 805.48 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:46<00:15, 696.14 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/1ing tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:35<00:49, 424.05 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:36<00:26, 714.96 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:37<00:22, 780.92 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:39<00:24, 704.52 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:39<00:18, 870.41 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:41<00:18, 788.74 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:43<00:22, 625.41 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:48<00:30, 418.34 examples/s]Running tokeaset (num_proc=64):  90%|████████▉ | 167767/186688 [01:37<00:37, 498.67 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:38<00:33, 530.97 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:42<00:39, 424.18 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:44<00:24, 603.19 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:44<00:20, 691.92 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173684/186688 [01:48<00:27, 480.19 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:48<00:18, 636.89 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:49<00:15, 696.89 examples/s]Running tokenizer on dataset (66 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:40<00:26, 522.99 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:41<00:19, 673.44 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:41<00:13, 863.09 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:43<00:14, 765.23 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:49<00:28, 359.55 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:52<00:28, 323.36 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:53<00:20, 411.10 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [01:53<00:12, 570.0%|█████████ | 168767/186688 [01:40<00:29, 602.33 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:42<00:27, 609.34 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:42<00:19, 821.42 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:43<00:16, 878.35 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:46<00:22, 607.05 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:46<00:17, 731.70 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:47<00:16, 746.16 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:49<00:16, 652.99 examples/s]Running tokenizer on dataset (num_proc=64):  95%|nning tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:45<00:19, 649.37 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:46<00:14, 811.99 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:51<00:26, 409.28 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:54<00:28, 349.21 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:55<00:20, 455.36 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:02<00:32, 251.38 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:03<00:22, 332.66 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:03<00:14, 455.78 examples/s]Ru47 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [01:57<00:15, 415.28 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [01:59<00:13, 410.58 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [01:59<00:08, 567.49 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:01<00:06, 565.09 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:03<00:05, 531.79 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:05<00:03, 464.32 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:06<00:01, 583.58 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:07<00:00, 1469.32 examples/s]
█████▎| 173767/186688 [01:48<00:21, 602.11 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:50<00:22, 541.52 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:52<00:21, 514.87 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:58<00:30, 336.22 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:59<00:21, 419.99 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:03<00:24, 332.60 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:06<00:23, 309.06 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:07<00:15, 412.61 examples/s]Running tokenizer on dataset (num_proc=64):  97%|████8 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:46<00:22, 617.28 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:46<00:15, 827.40 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:49<00:19, 616.32 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:49<00:13, 806.89 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:59<00:39, 254.00 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [02:01<00:30, 303.09 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:02<00:23, 350.13 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:08<00:18, 345.5nizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:50<00:29, 404.75 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:54<00:31, 349.65 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:58<00:32, 306.12 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:58<00:21, 419.66 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:01<00:21, 377.26 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:03<00:19, 381.49 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:04<00:13, 489.86 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:07<00:13, 401.17 examples/s]Running toke86688 [01:49<00:18, 546.07 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:58<00:36, 252.69 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:01<00:30, 269.90 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:03<00:24, 304.72 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:04<00:17, 373.20 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:04<00:10, 513.94 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:06<00:09, 468.76 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:07<00:06, 553.43 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/1num_proc=64):  95%|█████████▍| 176601/186688 [01:53<00:20, 486.79 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [02:01<00:37, 243.95 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:03<00:27, 295.33 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:03<00:18, 406.31 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:06<00:17, 365.50 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:07<00:11, 478.68 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:08<00:08, 548.79 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:09<00:06, 600.68 examples/s]Running tokenizer on dataset (████████▍| 176601/186688 [02:00<00:45, 222.36 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [02:01<00:32, 284.15 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:02<00:22, 362.43 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:06<00:24, 304.57 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:07<00:16, 381.09 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:08<00:11, 488.38 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:10<00:10, 444.23 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:11<00:06, 596.22 examples/s]Running tokenizer on dataset (num_proc=64):  99%|nning tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:06<00:12, 431.31 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:06<00:08, 566.70 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:08<00:07, 513.56 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:09<00:04, 631.79 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:12<00:04, 447.51 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:13<00:01, 617.47 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:13<00:00, 769.63 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:14<00:00, 1389.73 examples/s]
86688 [02:10<00:05, 459.65 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:10<00:02, 619.50 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:12<00:01, 673.83 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:14<00:00, 586.29 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:14<00:00, 1384.17 examples/s]
████████▉| 184854/186688 [02:14<00:02, 611.98 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:14<00:01, 738.72 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:15<00:00, 807.24 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:16<00:00, 1370.03 examples/s]
█████▋| 181186/186688 [02:07<00:10, 512.82 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:08<00:07, 621.76 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:10<00:06, 541.68 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:12<00:04, 586.16 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:14<00:03, 511.06 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:15<00:01, 532.01 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:16<00:00, 686.50 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:17<00:00, 1360.55 examples/s]
7 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:08<00:12, 444.87 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:09<00:09, 480.65 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:10<00:06, 606.32 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:12<00:05, 518.18 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:14<00:03, 543.78 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:14<00:01, 719.65 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:16<00:00, 585.42 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:17<00:00, 1357.60 examples/s]
num_proc=64):  99%|█████████▊| 183937/186688 [02:11<00:04, 553.56 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:11<00:02, 744.81 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:16<00:02, 420.15 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:16<00:00, 517.44 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:17<00:00, 1355.50 examples/s]
nizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:08<00:08, 511.32 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:08<00:05, 701.06 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:08<00:02, 939.53 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:13<00:04, 429.13 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:15<00:02, 414.74 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:17<00:00, 443.79 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:18<00:00, 1349.29 examples/s]
[INFO|configuration_utils.py:763] 2025-10-04 13:10:47,911 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 13:10:47,910 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 13:10:47,910 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 13:10:47,911 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 13:10:47,911 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-04 13:10:47,911 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 13:10:47,911 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-04 13:10:47,911 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 13:10:47,911 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
[INFO|configuration_utils.py:839] 2025-10-04 13:10:47,911 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
[INFO|configuration_utils.py:839] 2025-10-04 13:10:47,911 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-04 13:10:47,911 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-04 13:10:47,912 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-04 13:10:47,912 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-04 13:10:47,917 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-04 13:10:47,918 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "num_local_experts": 128,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|modeling_utils.py:1277] 2025-10-04 13:10:48,495 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 13:10:48,495 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 13:10:48,495 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 13:10:48,495 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 13:10:48,495 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-04 13:10:48,495 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-04 13:10:48,496 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 13:10:48,496 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 13:10:48,496 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 13:10:48,496 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 13:10:48,496 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-04 13:10:48,496 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-04 13:10:48,497 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-04 13:10:48,498 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-04 13:10:48,506 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:10:48,506 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:10:48,506 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:10:48,507 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:10:48,507 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:10:48,508 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:10:48,521 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|modeling_utils.py:1277] 2025-10-04 13:10:48,569 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-04 13:10:48,569 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-04 13:10:48,593 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:33,  1.30s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:33,  1.30s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:36,  1.34s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:25,  2.90s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:25,  2.90s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:29,  2.94s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:35,  3.08s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:35,  3.08s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:28,  1.23s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:28,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:28,  1.23s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:28,  1.22s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.22s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:33,  1.29s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:33,  1.29s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:33,  1.29s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:33,  1.29s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:26,  2.91s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:26,  2.91s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:26,  2.91s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:26,  2.91s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.07s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.07s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:28,  1.23s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:28,  1.23s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:28,  1.23s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:28,  1.23s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.06s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.06s/it]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:25,  1.19s/it]Loading checkpoint shards:   1%|▏         | 1/73 [00:01<01:27,  1.21s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:23,  2.86s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.87s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.88s/it]Loading checkpoint shards:   3%|▎         | 2/73 [00:05<03:24,  2.87s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.07s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:34,  3.07s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:59,  3.47s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:59,  3.47s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:59,  3.47s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:59,  3.47s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:21,  2.96s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:21,  2.96s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:21,  2.96s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:21,  2.96s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.31s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.31s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.31s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.06s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.06s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.96s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:21,  2.96s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.96s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.96s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.06s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.96s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.96s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.96s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.96s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.45s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:33,  3.05s/it]Loading checkpoint shards:   4%|▍         | 3/73 [00:08<03:35,  3.08s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:58,  3.46s/it]Loading checkpoint shards:   5%|▌         | 4/73 [00:12<03:59,  3.47s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:21,  2.97s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:21,  2.97s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:20,  2.95s/it]Loading checkpoint shards:   7%|▋         | 5/73 [00:14<03:21,  2.96s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:   8%|▊         | 6/73 [00:18<03:41,  3.31s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.35s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.35s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03         | 6/73 [00:18<03:41,  3.31s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.55s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03         | 6/73 [00:18<03:41,  3.30s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  10%|▉         | 7/73 [00:22<03:40,  3.34s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  11%|█         | 8/73 [00:26<03:50,  3.54s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  12%|█▏        | 9/73 [00:29<03:39,  3.43s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.62s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.62s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:15,  3.15s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:15,  3.15s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.38s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.38s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:59,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  14%|█▎        | 10/73 [00:33<03:48,  3.63s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  15%|█▌        | 11/73 [00:35<03:14,  3.14s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  16%|█▋        | 12/73 [00:39<03:26,  3.39s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:59,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:12,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:12,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.91s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.91s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:02,  3.21s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:02,  3.21s/it]Loading checkpoint shards:  22%|██▏       | 16/<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  18%|█▊        | 13/73 [00:41<02:58,  2.98s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  19%|█▉        | 14/73 [00:45<03:13,  3.27s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  21%|██        | 15/73 [00:47<02:48,  2.90s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shard73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shard73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shard73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shard73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shard73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shard73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shard73 [00:51<03:03,  3.22s/it]Loading checkpoint shards:  22%|██▏       | 16/73 [00:51<03:03,  3.21s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.87s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.87s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  23%|██▎       | 17/73 [00:53<02:40,  2.86s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  25%|██▍       | 18/73 [00:57<02:55,  3.19s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34ss:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34ss:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34ss:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34ss:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34ss:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34ss:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34ss:  26%|██▌       | 19/73 [01:00<02:51,  3.18s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:00,  3.41s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:00,  3.41s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  27%|██▋       | 20/73 [01:04<03:01,  3.42s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.05s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.05s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  29%|██▉       | 21/73 [01:06<02:38,  3.04s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.33s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  30%|███       | 22/73 [01:10<02:50,  3.34s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.29s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.29s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shar/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  32%|███▏      | 23/73 [01:13<02:44,  3.28s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  33%|███▎      | 24/73 [01:17<02:50,  3.48s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  34%|███▍      | 25/73 [01:19<02:26,  3.05s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.31s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.31s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.28s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.28s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:31<02:38,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:31<02:38,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:31<02:39,  3.53s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:31<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:31<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:31<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:31<02:39,  3.53s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  40%|███▉      |ds:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  36%|███▌      | 26/73 [01:23<02:35,  3.32s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  37%|███▋      | 27/73 [01:26<02:30,  3.27s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  38%|███▊      | 28/73 [01:30<02:39,  3.53s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:31,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:31,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:33,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:33,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.46s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.46s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.46s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:2 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  40%|███▉      | 29/73 [01:34<02:30,  3.43s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  41%|████      | 30/73 [01:38<02:34,  3.58s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  42%|████▏     | 31/73 [01:41<02:25,  3.45s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.14s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.14s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.37s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.37s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  7,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  7,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  7,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  7,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  7,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  7,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  7,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  44%|████▍     | 32/73 [01:45<02:27,  3.60s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  45%|████▌     | 33/73 [01:47<02:05,  3.13s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  47%|████▋     | 34/73 [01:51<02:11,  3.38s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  48%|████▊     | 35/73 [01:53<01:53,  2.98s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  49%|████▉     | 36/73 [01:57<02:03,  3.33s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  51%|█████     | 37/73 [01:59<01:46,  2.95s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.24s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.24s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  56%|█████▌    | 41/25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  56%|█████▌    | 41/25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  56%|█████▌    | 41/25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  56%|█████▌    | 41/25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  56%|█████▌    | 41/25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  56%|█████▌    | 41/25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  56%|█████▌    | 41/25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  52%|█████▏    | 38/73 [02:03<01:53,  3.25s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  53%|█████▎    | 39/73 [02:06<01:49,  3.21s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  55%|█████▍    | 40/73 [02:10<01:53,  3.43s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.02s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.02s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:41,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:41,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.26s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.26s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.26s/it]Loading checkpoint shards:  60%|██73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  60%|██73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  60%|██73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  60%|██73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  60%|██73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  60%|██73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  60%|██73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  56%|█████▌    | 41/73 [02:12<01:36,  3.01s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  58%|█████▊    | 42/73 [02:16<01:42,  3.29s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  59%|█████▉    | 43/73 [02:19<01:37,  3.25s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.50s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.50s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:27,  3.11s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:27,  3.11s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.11s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.35s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.35s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loa███    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loa███    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loa███    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loa███    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loa███    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loa███    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loa███    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  60%|██████    | 44/73 [02:23<01:41,  3.51s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  62%|██████▏   | 45/73 [02:25<01:26,  3.10s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  63%|██████▎   | 46/73 [02:29<01:30,  3.36s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:33<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:33<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:33<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:33<01:25,  3.31s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:37<01:27,  3.50s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:39<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:39<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:39<01:13,  3.06s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|█████ding checkpoint shards:  64%|██████▍   | 47/73 [02:33<01:26,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:33<01:26,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:32<01:25,  3.31s/it]Loading checkpoint shards:  64%|██████▍   | 47/73 [02:33<01:25,  3.31s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.49s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.49s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:36<01:27,  3.50s/it]Loading checkpoint shards:  66%|██████▌   | 48/73 [02:37<01:27,  3.50s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:39<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:39<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  67%|██████▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Load▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Load▋   | 49/73 [02:39<01:13,  3.06s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:45<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:45<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:45<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:45<01:04,  2.94s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Load▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Load▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Load▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Load▋   | 49/73 [02:38<01:13,  3.06s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.33s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Load▋   | 49/73 [02:39<01:13,  3.06s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:42<01:16,  3.32s/it]Loading checkpoint shards:  68%|██████▊   | 50/73 [02:43<01:16,  3.32s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:45<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:45<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:44<01:04,  2.94s/it]Loading checkpoint shards:  70%|██████▉   | 51/73 [02:45<01:04,  2.94s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.27s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.27s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.24s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.24s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:49<01:08,  3.28s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  7ing checkpoint shards:  71%|███████   | 52/73 [02:48<01:08,  3.28s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  73%|███████▎  | 53/73 [02:51<00:58,  2.91s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  74%|███████▍  | 54/73 [02:55<01:01,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.37s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.37s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:52,  3.53s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|██████5%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 55/73 [02:58<00:58,  3.23s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  77%|███████▋  | 56/73 [03:02<00:58,  3.45s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  78%|███████▊  | 57/73 [03:05<00:53,  3.36s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:52,  3.53s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.41s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.41s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  79%|███████▉  | 58/73 [03:09<00:53,  3.54s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  81%|████████  | 59/73 [03:12<00:47,  3.40s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  82%|████████▏ | 60/73 [03:16<00:46,  3.56s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73  [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  84%|████████▎ | 61/73 [03:19<00:41,  3.46s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  85%|████████▍ | 62/73 [03:23<00:39,  3.60s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  86%|████████▋ | 63/73 [03:25<00:31,  3.13s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.25s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.25s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:25<00:31,  3.13s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:25<00:31,  3.13s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:25<00:31,  3.13s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:25<00:31,  3.13s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:25<00:31,  3.13s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:25<00:31,  3.13s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [[03:25<00:31,  3.13s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  88%|████████▊ | 64/73 [03:29<00:30,  3.37s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  89%|████████▉ | 65/73 [03:31<00:23,  2.97s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.51s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.51s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.08s/it]Loading checkpoint shards:  95%|██████03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|██████03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|██████03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|██████03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|██████03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|██████03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|██████03:35<00:22,  3.26s/it]Loading checkpoint shards:  90%|█████████ | 66/73 [03:35<00:22,  3.26s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  92%|█████████▏| 67/73 [03:38<00:19,  3.27s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  93%|█████████▎| 68/73 [03:42<00:17,  3.52s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.08s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:09,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  99%██▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  95%|█████████▍| 69/73 [03:44<00:12,  3.07s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  96%|█████████▌| 70/73 [03:48<00:10,  3.33s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  97%|█████████▋| 71/73 [03:52<00:06,  3.30s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:56<00:03,  3.48s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:56<00:03,  3.48s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:56<00:03,  3.49s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
|█████████▊| 72/73 [03:56<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:56<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:56<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:56<00:03,  3.49s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]

Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
[INFO|modeling_utils.py:5724] 2025-10-04 13:14:54,128 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
[INFO|modeling_utils.py:5732] 2025-10-04 13:14:54,128 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
[INFO|modeling_utils.py:5724] 2025-10-04 13:14:54,128 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 13:14:54,128 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1008] 2025-10-04 13:14:54,130 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
[INFO|configuration_utils.py:1008] 2025-10-04 13:14:54,130 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
[INFO|configuration_utils.py:1055] 2025-10-04 13:14:54,130 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:14:54,130 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|modeling_utils.py:5724] 2025-10-04 13:14:54,130 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 13:14:54,131 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it][INFO|configuration_utils.py:1008] 2025-10-04 13:14:54,133 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
[INFO|configuration_utils.py:1055] 2025-10-04 13:14:54,133 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
[INFO|modeling_utils.py:5724] 2025-10-04 13:14:54,137 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 13:14:54,137 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
[INFO|configuration_utils.py:1008] 2025-10-04 13:14:54,140 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
[INFO|configuration_utils.py:1055] 2025-10-04 13:14:54,140 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards:  99%|█████████▊| 72/73 [03:55<00:03,  3.49s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
[INFO|modeling_utils.py:5724] 2025-10-04 13:14:54,141 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5724] 2025-10-04 13:14:54,140 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 13:14:54,141 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|modeling_utils.py:5732] 2025-10-04 13:14:54,141 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|modeling_utils.py:5724] 2025-10-04 13:14:54,141 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 13:14:54,141 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1008] 2025-10-04 13:14:54,143 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
[INFO|configuration_utils.py:1008] 2025-10-04 13:14:54,143 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
[INFO|configuration_utils.py:1008] 2025-10-04 13:14:54,144 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-04 13:14:54,144 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:14:54,144 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1055] 2025-10-04 13:14:54,144 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|trainer.py:757] 2025-10-04 13:14:54,155 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 13:14:54,156 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 13:14:54,158 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 13:14:54,159 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 13:14:54,161 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 13:14:54,162 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 13:14:54,164 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 13:14:54,165 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 13:14:54,166 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-04 13:14:54,166 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 13:14:54,167 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 73/73 [03:59<00:00,  3.28s/it]
[WARNING|trainer.py:985] 2025-10-04 13:14:54,168 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|modeling_utils.py:5724] 2025-10-04 13:14:54,168 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-04 13:14:54,168 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1008] 2025-10-04 13:14:54,170 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-120b-bf16/generation_config.json
[INFO|trainer.py:757] 2025-10-04 13:14:54,170 >> Using auto half precision backend
[INFO|configuration_utils.py:1055] 2025-10-04 13:14:54,171 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[WARNING|trainer.py:985] 2025-10-04 13:14:54,171 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-04 13:14:54,191 >> Using auto half precision backend
[WARNING|trainer.py:985] 2025-10-04 13:14:54,192 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 13:14:54,460 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 13:14:54,461 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 13:14:54,467 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 13:14:54,474 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[INFO|deepspeed.py:380] 2025-10-04 13:14:54,475 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-04 13:14:54,477 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[INFO|deepspeed.py:380] 2025-10-04 13:14:54,479 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 4. Using DeepSpeed's value.
[INFO|deepspeed.py:380] 2025-10-04 13:14:54,487 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[rank12]:W1004 13:14:58.097000 197337 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank12]:W1004 13:14:58.097000 197337 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank14]:W1004 13:14:58.205000 197339 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank14]:W1004 13:14:58.205000 197339 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank11]:W1004 13:14:58.247000 20620 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank11]:W1004 13:14:58.247000 20620 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank18]:W1004 13:14:58.264000 2484416 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank18]:W1004 13:14:58.264000 2484416 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank7]:W1004 13:14:58.328000 3540822 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank7]:W1004 13:14:58.328000 3540822 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank23]:W1004 13:14:58.598000 1659494 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank23]:W1004 13:14:58.598000 1659494 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank20]:W1004 13:14:58.641000 1659491 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank20]:W1004 13:14:58.641000 1659491 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[INFO|trainer.py:2523] 2025-10-04 13:15:45,666 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-04 13:15:45,667 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-04 13:15:45,667 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-04 13:15:45,667 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-04 13:15:45,667 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-04 13:15:45,666 >>   Num examples = 186,688
[INFO|trainer.py:2523] 2025-10-04 13:15:45,667 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-04 13:15:45,666 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-04 13:15:45,666 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-04 13:15:45,666 >>   Num Epochs = 1
[INFO|trainer.py:2524] 2025-10-04 13:15:45,667 >>   Num examples = 186,688
[INFO|trainer.py:2524] 2025-10-04 13:15:45,667 >>   Num examples = 186,688
[INFO|trainer.py:2524] 2025-10-04 13:15:45,667 >>   Num examples = 186,688
[INFO|trainer.py:2524] 2025-10-04 13:15:45,667 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-04 13:15:45,666 >>   Num Epochs = 1
[INFO|trainer.py:2524] 2025-10-04 13:15:45,667 >>   Num examples = 186,688
[INFO|trainer.py:2526] 2025-10-04 13:15:45,666 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-04 13:15:45,666 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-04 13:15:45,666 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2525] 2025-10-04 13:15:45,667 >>   Num Epochs = 1
[INFO|trainer.py:2525] 2025-10-04 13:15:45,667 >>   Num Epochs = 1
[INFO|trainer.py:2525] 2025-10-04 13:15:45,667 >>   Num Epochs = 1
[INFO|trainer.py:2525] 2025-10-04 13:15:45,667 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-04 13:15:45,666 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2525] 2025-10-04 13:15:45,667 >>   Num Epochs = 1
[INFO|trainer.py:2531] 2025-10-04 13:15:45,667 >>   Total optimization steps = 1,459
[INFO|trainer.py:2526] 2025-10-04 13:15:45,667 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2526] 2025-10-04 13:15:45,667 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2526] 2025-10-04 13:15:45,667 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2526] 2025-10-04 13:15:45,667 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-04 13:15:45,667 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2526] 2025-10-04 13:15:45,667 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2532] 2025-10-04 13:15:45,668 >>   Number of trainable parameters = 116,829,156,672
[INFO|trainer.py:2529] 2025-10-04 13:15:45,667 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2529] 2025-10-04 13:15:45,667 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2529] 2025-10-04 13:15:45,667 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2529] 2025-10-04 13:15:45,667 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-04 13:15:45,667 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2529] 2025-10-04 13:15:45,667 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2530] 2025-10-04 13:15:45,667 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2530] 2025-10-04 13:15:45,667 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2530] 2025-10-04 13:15:45,667 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2530] 2025-10-04 13:15:45,667 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-04 13:15:45,667 >>   Total optimization steps = 1,459
[INFO|trainer.py:2530] 2025-10-04 13:15:45,667 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-04 13:15:45,667 >>   Total optimization steps = 1,459
[INFO|trainer.py:2531] 2025-10-04 13:15:45,667 >>   Total optimization steps = 1,459
[INFO|trainer.py:2531] 2025-10-04 13:15:45,667 >>   Total optimization steps = 1,459
[INFO|trainer.py:2531] 2025-10-04 13:15:45,667 >>   Total optimization steps = 1,459
[INFO|trainer.py:2532] 2025-10-04 13:15:45,668 >>   Number of trainable parameters = 116,829,156,672
[INFO|trainer.py:2531] 2025-10-04 13:15:45,667 >>   Total optimization steps = 1,459
[INFO|trainer.py:2532] 2025-10-04 13:15:45,668 >>   Number of trainable parameters = 116,829,156,672
[INFO|trainer.py:2532] 2025-10-04 13:15:45,668 >>   Number of trainable parameters = 116,829,156,672
[INFO|trainer.py:2532] 2025-10-04 13:15:45,668 >>   Number of trainable parameters = 116,829,156,672
[INFO|trainer.py:2532] 2025-10-04 13:15:45,668 >>   Number of trainable parameters = 116,829,156,672
[INFO|trainer.py:2532] 2025-10-04 13:15:45,668 >>   Number of trainable parameters = 116,829,156,672
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2523] 2025-10-04 13:15:45,921 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-04 13:15:45,921 >>   Num examples = 186,688
[INFO|trainer.py:2525] 2025-10-04 13:15:45,921 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-04 13:15:45,921 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-04 13:15:45,921 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-04 13:15:45,921 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-04 13:15:45,921 >>   Total optimization steps = 1,459
[INFO|trainer.py:2532] 2025-10-04 13:15:45,922 >>   Number of trainable parameters = 116,829,156,672
srun: Job step aborted: Waiting up to 47 seconds for job step to finish.
W1004 13:18:12.289000 197289 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:18:12.290000 197289 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 197337 closing signal SIGTERM
W1004 13:18:12.290000 2484366 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:18:12.290000 3540769 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:18:12.290000 20568 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:18:12.290000 2484366 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2484414 closing signal SIGTERM
W1004 13:18:12.290000 3459238 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:18:12.290000 3438480 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:18:12.290000 3540769 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3540819 closing signal SIGTERM
W1004 13:18:12.290000 1659439 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:18:12.290000 20568 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 20617 closing signal SIGTERM
W1004 13:18:12.290000 3459238 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3459286 closing signal SIGTERM
W1004 13:18:12.290000 3438480 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3438528 closing signal SIGTERM
slurmstepd: error: *** JOB 1178433 ON della-j16g1 CANCELLED AT 2025-10-04T13:18:12 ***
W1004 13:18:12.290000 1659439 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1659491 closing signal SIGTERM
  0%|          | 0/1459 [00:00<?, ?it/s]slurmstepd: error: *** STEP 1178433.0 ON della-j16g1 CANCELLED AT 2025-10-04T13:18:12 ***
W1004 13:18:12.292000 3302548 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1004 13:18:12.295000 3302548 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3302597 closing signal SIGTERM
W1004 13:18:12.294000 3438480 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3438529 closing signal SIGTERM
W1004 13:18:12.292000 1659439 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1659492 closing signal SIGTERM
W1004 13:18:12.295000 3540769 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3540820 closing signal SIGTERM
W1004 13:18:12.295000 3459238 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3459287 closing signal SIGTERM
W1004 13:18:12.294000 2484366 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2484415 closing signal SIGTERM
W1004 13:18:12.299000 3459238 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3459288 closing signal SIGTERM
W1004 13:18:12.309000 2484366 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2484416 closing signal SIGTERM
W1004 13:18:12.295000 197289 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 197338 closing signal SIGTERM
W1004 13:18:12.312000 2484366 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2484417 closing signal SIGTERM
W1004 13:18:12.300000 3302548 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3302598 closing signal SIGTERM
W1004 13:18:12.319000 1659439 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1659493 closing signal SIGTERM
W1004 13:18:12.329000 20568 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 20618 closing signal SIGTERM
W1004 13:18:12.334000 3540769 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3540821 closing signal SIGTERM
W1004 13:18:12.349000 197289 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 197339 closing signal SIGTERM
W1004 13:18:12.358000 20568 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 20619 closing signal SIGTERM
