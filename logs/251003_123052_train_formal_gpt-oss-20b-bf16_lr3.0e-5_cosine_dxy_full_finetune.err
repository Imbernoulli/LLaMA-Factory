+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g[1-2],della-j15g[1-3],della-j16g[1,3],della-j17g1'
++ head -n 1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g[1-2],della-j15g[1-3],della-j16g[1,3],della-j17g1'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=0 --master_addr=della-j12g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j12g[1-2],della-j15g[1-3],della-j16g[1,3],della-j17g1'
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=2 --master_addr=della-j12g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j12g[1-2],della-j15g[1-3],della-j16g[1,3],della-j17g1'
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=3 --master_addr=della-j12g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j12g[1-2],della-j15g[1-3],della-j16g[1,3],della-j17g1'
++ head -n 1
++ head -n 1
++ scontrol show hostnames 'della-j12g[1-2],della-j15g[1-3],della-j16g[1,3],della-j17g1'
++ scontrol show hostnames 'della-j12g[1-2],della-j15g[1-3],della-j16g[1,3],della-j17g1'
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=1 --master_addr=della-j12g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j12g[1-2],della-j15g[1-3],della-j16g[1,3],della-j17g1'
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=5 --master_addr=della-j12g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=6 --master_addr=della-j12g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 -+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=7 --master_addr=della-j12g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
-enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=4 --master_addr=della-j12g1 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
W1003 12:31:11.958000 1198023 site-packages/torch/distributed/run.py:774] 
W1003 12:31:11.958000 1198023 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:11.958000 1198023 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 12:31:11.958000 1198023 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:11.959000 1612067 site-packages/torch/distributed/run.py:774] 
W1003 12:31:11.959000 1612067 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:11.959000 1612067 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 12:31:11.959000 1612067 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:12.227000 3141086 site-packages/torch/distributed/run.py:774] 
W1003 12:31:12.227000 3141086 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:12.227000 3141086 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 12:31:12.227000 3141086 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:12.227000 2414669 site-packages/torch/distributed/run.py:774] 
W1003 12:31:12.227000 2414669 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:12.227000 2414669 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 12:31:12.227000 2414669 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:12.228000 2896856 site-packages/torch/distributed/run.py:774] 
W1003 12:31:12.228000 2896856 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:12.228000 2896856 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 12:31:12.228000 2896856 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:12.228000 3246996 site-packages/torch/distributed/run.py:774] 
W1003 12:31:12.228000 3246996 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:12.228000 3246996 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 12:31:12.228000 3246996 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:12.765000 1615146 site-packages/torch/distributed/run.py:774] 
W1003 12:31:12.765000 1615146 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:12.765000 1615146 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 12:31:12.765000 1615146 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:12.869000 2348438 site-packages/torch/distributed/run.py:774] 
W1003 12:31:12.869000 2348438 site-packages/torch/distributed/run.py:774] *****************************************
W1003 12:31:12.869000 2348438 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 12:31:12.869000 2348438 site-packages/torch/distributed/run.py:774] *****************************************
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,298 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,298 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,298 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,298 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,298 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,298 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,299 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,299 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,299 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,299 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,299 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,299 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,300 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,300 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,300 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,300 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,300 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,300 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,302 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,302 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,302 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,302 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,302 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,302 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,305 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,305 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,305 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,305 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,305 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,305 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,312 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,312 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,312 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,312 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,312 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,312 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,371 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,371 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,371 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,371 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,371 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,371 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,407 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,407 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,407 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,407 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,407 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,407 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:42,965 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 12:31:42,965 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:42,969 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 12:31:42,970 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:42,970 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 12:31:42,971 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 12:31:42,971 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,972 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,972 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,972 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,972 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,972 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,972 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:42,975 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 12:31:42,976 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 12:31:42,976 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,976 >> loading file tokenizer.json
[INFO|configuration_utils.py:839] 2025-10-03 12:31:42,976 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,976 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,976 >> loading file added_tokens.json
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,976 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,976 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,976 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,977 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,977 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,977 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,977 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,977 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,977 >> loading file chat_template.jinja
[INFO|configuration_utils.py:839] 2025-10-03 12:31:42,978 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,979 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,979 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,979 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,979 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,979 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,979 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:42,980 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 12:31:42,981 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 12:31:42,983 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,983 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,983 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,983 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,983 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,984 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,984 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:42,989 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 12:31:42,989 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 12:31:42,992 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,992 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,992 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,992 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,992 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,992 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:42,992 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:43,042 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 12:31:43,043 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 12:31:43,050 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:43,050 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:43,050 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:43,050 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:43,050 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:43,050 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:43,050 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:43,074 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 12:31:43,075 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 12:31:43,077 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:43,077 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:43,077 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:43,078 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:43,078 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:43,078 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 12:31:43,078 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:43,674 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:43,686 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:43,690 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:43,690 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:43,691 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:43,693 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:43,756 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 12:31:43,771 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 186963 examples [00:01, 219.00 examples/s]            Converting format of dataset (num_proc=64): 208323 examples [00:01, 21993.06 examples/s]Converting format of dataset (num_proc=64): 230048 examples [00:01, 46789.70 examples/s]Converting format of dataset (num_proc=64): 254089 examples [00:01, 76424.61 examples/s]Converting format of dataset (num_proc=64): 274535 examples [00:01, 98749.19 examples/s]Converting format of dataset (num_proc=64): 296674 examples [00:01, 123588.04 examples/s]Converting format of dataset (num_proc=64): 320241 examples [00:01, 148645.09 examples/s]Converting format of dataset (num_proc=64): 341932 examples [00:02, 134463.10 examples/s]Converting format of dataset (num_proc=64): 360306 examples [00:02, 141384.17 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 50836.86 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187164 examples [00:01, 371.85 examples/s]            Converting format of dataset (num_proc=64): 210635 examples [00:01, 23881.14 examples/s]Converting format of dataset (num_proc=64): 236131 examples [00:01, 52836.85 examples/s]Converting format of dataset (num_proc=64): 261366 examples [00:01, 82603.80 examples/s]Converting format of dataset (num_proc=64): 284802 examples [00:01, 109494.80 examples/s]Converting format of dataset (num_proc=64): 310280 examples [00:01, 138769.39 examples/s]Converting format of dataset (num_proc=64): 333697 examples [00:01, 151584.30 examples/s]Converting format of dataset (num_proc=64): 355586 examples [00:02, 151413.23 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 49740.47 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187237 examples [00:01, 420.67 examples/s]            Converting format of dataset (num_proc=64): 200337 examples [00:01, 13250.09 examples/s]Converting format of dataset (num_proc=64): 223282 examples [00:01, 39842.55 examples/s]Converting format of dataset (num_proc=64): 250613 examples [00:01, 74672.56 examples/s]Converting format of dataset (num_proc=64): 276782 examples [00:01, 107265.66 examples/s]Converting format of dataset (num_proc=64): 300642 examples [00:01, 133567.01 examples/s]Converting format of dataset (num_proc=64): 323336 examples [00:01, 147239.50 examples/s]Converting format of dataset (num_proc=64): 345364 examples [00:02, 163809.72 examples/s]Converting format of dataset (num_proc=64): 367138 examples [00:02, 125927.32 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 49290.63 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187221 examples [00:01, 384.99 examples/s]            Converting format of dataset (num_proc=64): 209054 examples [00:01, 20750.85 examples/s]Converting format of dataset (num_proc=64): 232419 examples [00:01, 45580.02 examples/s]Converting format of dataset (num_proc=64): 255055 examples [00:01, 70889.21 examples/s]Converting format of dataset (num_proc=64): 283422 examples [00:01, 106679.83 examples/s]Converting format of dataset (num_proc=64): 305992 examples [00:01, 127710.82 examples/s]Converting format of dataset (num_proc=64): 328166 examples [00:02, 147827.20 examples/s]Converting format of dataset (num_proc=64): 350213 examples [00:02, 153961.62 examples/s]Converting format of dataset (num_proc=64): 370802 examples [00:02, 106868.22 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 49493.62 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187167 examples [00:01, 356.62 examples/s]            Converting format of dataset (num_proc=64): 200511 examples [00:01, 13149.33 examples/s]Converting format of dataset (num_proc=64): 226379 examples [00:01, 42744.31 examples/s]Converting format of dataset (num_proc=64): 252248 examples [00:01, 74466.21 examples/s]Converting format of dataset (num_proc=64): 276747 examples [00:01, 104043.68 examples/s]Converting format of dataset (num_proc=64): 304039 examples [00:01, 137330.51 examples/s]Converting format of dataset (num_proc=64): 327656 examples [00:01, 145380.90 examples/s]Converting format of dataset (num_proc=64): 348922 examples [00:02, 152150.39 examples/s]Converting format of dataset (num_proc=64): 369058 examples [00:02, 104989.51 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 49403.91 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187233 examples [00:01, 413.57 examples/s]            Converting format of dataset (num_proc=64): 205081 examples [00:01, 17727.66 examples/s]Converting format of dataset (num_proc=64): 232968 examples [00:01, 49094.35 examples/s]Converting format of dataset (num_proc=64): 259134 examples [00:01, 80155.25 examples/s]Converting format of dataset (num_proc=64): 281301 examples [00:01, 104567.55 examples/s]Converting format of dataset (num_proc=64): 303588 examples [00:01, 128008.39 examples/s]Converting format of dataset (num_proc=64): 325472 examples [00:01, 146530.00 examples/s]Converting format of dataset (num_proc=64): 346985 examples [00:02, 156309.81 examples/s]Converting format of dataset (num_proc=64): 367628 examples [00:02, 118106.03 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 49211.79 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187205 examples [00:01, 382.72 examples/s]            Converting format of dataset (num_proc=64): 197041 examples [00:01, 9749.06 examples/s]Converting format of dataset (num_proc=64): 227009 examples [00:01, 44215.76 examples/s]Converting format of dataset (num_proc=64): 250663 examples [00:01, 72113.17 examples/s]Converting format of dataset (num_proc=64): 277352 examples [00:01, 104630.43 examples/s]Converting format of dataset (num_proc=64): 298856 examples [00:01, 124919.42 examples/s]Converting format of dataset (num_proc=64): 322827 examples [00:01, 149652.39 examples/s]Converting format of dataset (num_proc=64): 344859 examples [00:02, 158764.19 examples/s]Converting format of dataset (num_proc=64): 365787 examples [00:02, 125936.96 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 48948.85 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187182 examples [00:01, 335.90 examples/s]            Converting format of dataset (num_proc=64): 209622 examples [00:01, 20103.91 examples/s]Converting format of dataset (num_proc=64): 233922 examples [00:01, 45045.44 examples/s]Converting format of dataset (num_proc=64): 254891 examples [00:01, 67657.95 examples/s]Converting format of dataset (num_proc=64): 273986 examples [00:02, 65668.48 examples/s]Converting format of dataset (num_proc=64): 288449 examples [00:03, 26015.69 examples/s]Converting format of dataset (num_proc=64): 298345 examples [00:04, 19348.37 examples/s]Converting format of dataset (num_proc=64): 305332 examples [00:05, 16719.59 examples/s]Converting format of dataset (num_proc=64): 310424 examples [00:05, 15257.80 examples/s]Converting format of dataset (num_proc=64): 314245 examples [00:06, 14333.90 examples/s]Converting format of dataset (num_proc=64): 317212 examples [00:06, 13587.73 examples/s]Converting format of dataset (num_proc=64): 319593 examples [00:06, 12942.52 examples/s]Converting format of dataset (num_proc=64): 321555 examples [00:06, 12515.41 examples/s]Converting format of dataset (num_proc=64): 323234 examples [00:06, 12063.84 examples/s]Converting format of dataset (num_proc=64): 324709 examples [00:07, 11771.42 examples/s]Converting format of dataset (num_proc=64): 326049 examples [00:07, 11391.34 examples/s]Converting format of dataset (num_proc=64): 327281 examples [00:07, 10976.37 examples/s]Converting format of dataset (num_proc=64): 328464 examples [00:07, 11131.08 examples/s]Converting format of dataset (num_proc=64): 329628 examples [00:07, 10626.11 examples/s]Converting format of dataset (num_proc=64): 330806 examples [00:07, 10779.95 examples/s]Converting format of dataset (num_proc=64): 331911 examples [00:07, 10109.97 examples/s]Converting format of dataset (num_proc=64): 333185 examples [00:07, 10742.27 examples/s]Converting format of dataset (num_proc=64): 334293 examples [00:07, 10694.50 examples/s]Converting format of dataset (num_proc=64): 335381 examples [00:08, 10251.97 examples/s]Converting format of dataset (num_proc=64): 336735 examples [00:08, 11101.48 examples/s]Converting format of dataset (num_proc=64): 337874 examples [00:08, 10716.23 examples/s]Converting format of dataset (num_proc=64): 338963 examples [00:08, 10689.26 examples/s]Converting format of dataset (num_proc=64): 340049 examples [00:08, 9784.89 examples/s] Converting format of dataset (num_proc=64): 341062 examples [00:08, 9865.66 examples/s]Converting format of dataset (num_proc=64): 342067 examples [00:08, 9383.59 examples/s]Converting format of dataset (num_proc=64): 343018 examples [00:08, 8882.34 examples/s]Converting format of dataset (num_proc=64): 343920 examples [00:09, 8456.54 examples/s]Converting format of dataset (num_proc=64): 344781 examples [00:09, 8300.61 examples/s]Converting format of dataset (num_proc=64): 345619 examples [00:09, 8152.29 examples/s]Converting format of dataset (num_proc=64): 346440 examples [00:09, 7891.11 examples/s]Converting format of dataset (num_proc=64): 347238 examples [00:09, 7750.74 examples/s]Converting format of dataset (num_proc=64): 348016 examples [00:09, 7573.47 examples/s]Converting format of dataset (num_proc=64): 348776 examples [00:09, 7440.24 examples/s]Converting format of dataset (num_proc=64): 349520 examples [00:09, 7295.74 examples/s]Converting format of dataset (num_proc=64): 350260 examples [00:09, 6978.27 examples/s]Converting format of dataset (num_proc=64): 350964 examples [00:10, 6614.03 examples/s]Converting format of dataset (num_proc=64): 351652 examples [00:10, 6180.43 examples/s]Converting format of dataset (num_proc=64): 353809 examples [00:10, 10202.46 examples/s]Converting format of dataset (num_proc=64): 356149 examples [00:10, 13765.44 examples/s]Converting format of dataset (num_proc=64): 358461 examples [00:10, 16361.12 examples/s]Converting format of dataset (num_proc=64): 360440 examples [00:10, 17324.31 examples/s]Converting format of dataset (num_proc=64): 362267 examples [00:10, 17109.67 examples/s]Converting format of dataset (num_proc=64): 364039 examples [00:10, 16606.62 examples/s]Converting format of dataset (num_proc=64): 365967 examples [00:10, 17210.32 examples/s]Converting format of dataset (num_proc=64): 367990 examples [00:10, 18044.46 examples/s]Converting format of dataset (num_proc=64): 369873 examples [00:11, 18098.34 examples/s]Converting format of dataset (num_proc=64): 371767 examples [00:11, 16732.76 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:11, 15990.71 examples/s]
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:13, 75.36 examples/s]             Running tokenizer on dataset (num_proc=64): 188688 examples [00:13, 177.18 examples/s]Running tokenizer on dataset (num_proc=64): 189688 examples [00:14, 283.80 examples/s]Running tokenizer on dataset (num_proc=64): 190688 examples [00:15, 427.86 examples/s]Running tokenizer on dataset (num_proc=64): 191688 examples [00:15, 597.67 examples/s]Running tokenizer on dataset (num_proc=64): 192688 examples [00:16, 770.78 examples/s]Running tokenizer on dataset (num_proc=64): 195688 examples [00:16, 1584.88 examples/s]Running tokenizer on dataset (num_proc=64): 198688 examples [00:17, 2362.98 examples/s]Running tokenizer on dataset (num_proc=64): 202688 examples [00:17, 3520.07 examples/s]Running tokenizer on dataset (num_proc=64): 204688 examples [00:18, 3671.11 examples/s]Running tokenizer Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:14, 70.33 examples/s]             Running tokenizer on dataset (num_proc=64): 188688 examples [00:14, 162.71 examples/s]Running tokenizer on dataset (num_proc=64): 189688 examples [00:15, 278.92 examples/s]Running tokenizer on dataset (num_proc=64): 191688 examples [00:15, 590.77 examples/s]Running tokenizer on dataset (num_proc=64): 193688 examples [00:16, 951.04 examples/s]Running tokenizer on dataset (num_proc=64): 195688 examples [00:16, 1430.20 examples/s]Running tokenizer on dataset (num_proc=64): 196688 examples [00:17, 1548.54 examples/s]Running tokenizer on dataset (num_proc=64): 199688 examples [00:17, 2346.60 examples/s]Running tokenizer on dataset (num_proc=64): 200688 examples [00:18, 2327.23 examples/s]Running tokenizer on dataset (num_proc=64): 202688 examples [00:18, 2644.21 examples/s]Running tokenizerRunning tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:14, 66.92 examples/s]             Running tokenizer on dataset (num_proc=64): 190688 examples [00:15, 337.22 examples/s]Running tokenizer on dataset (num_proc=64): 192688 examples [00:15, 558.82 examples/s]Running tokenizer on dataset (num_proc=64): 194688 examples [00:16, 867.42 examples/s]Running tokenizer on dataset (num_proc=64): 196688 examples [00:16, 1207.46 examples/s]Running tokenizer on dataset (num_proc=64): 198688 examples [00:17, 1577.46 examples/s]Running tokenizer on dataset (num_proc=64): 200688 examples [00:17, 1960.67 examples/s]Running tokenizer on dataset (num_proc=64): 202688 examples [00:18, 2349.69 examples/s]Running tokenizer on dataset (num_proc=64): 205688 examples [00:18, 2800.33 examples/s]Running tokenizer on dataset (num_proc=64): 208688 examples [00:19, 3437.42 examples/s]Running tokenizeRunning tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:12, 78.27 examples/s]             Running tokenizer on dataset (num_proc=64): 188688 examples [00:13, 174.10 examples/s]Running tokenizer on dataset (num_proc=64): 189688 examples [00:15, 265.15 examples/s]Running tokenizer on dataset (num_proc=64): 190688 examples [00:15, 417.93 examples/s]Running tokenizer on dataset (num_proc=64): 191688 examples [00:15, 612.01 examples/s]Running tokenizer on dataset (num_proc=64): 192688 examples [00:16, 787.44 examples/s]Running tokenizer on dataset (num_proc=64): 197688 examples [00:16, 2205.87 examples/s]Running tokenizer on dataset (num_proc=64): 198688 examples [00:17, 2134.68 examples/s]Running tokenizer on dataset (num_proc=64): 200688 examples [00:17, 2722.98 examples/s]Running tokenizer on dataset (num_proc=64): 201688 examples [00:19, 1624.47 examples/s]Running tokenizer Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:14, 70.45 examples/s]             Running tokenizer on dataset (num_proc=64): 188688 examples [00:14, 165.41 examples/s]Running tokenizer on dataset (num_proc=64): 189688 examples [00:15, 283.12 examples/s]Running tokenizer on dataset (num_proc=64): 190688 examples [00:15, 428.56 examples/s]Running tokenizer on dataset (num_proc=64): 191688 examples [00:16, 596.76 examples/s]Running tokenizer on dataset (num_proc=64): 193688 examples [00:17, 922.99 examples/s]Running tokenizer on dataset (num_proc=64): 197688 examples [00:17, 1930.38 examples/s]Running tokenizer on dataset (num_proc=64): 201688 examples [00:18, 2953.80 examples/s]Running tokenizer on dataset (num_proc=64): 204688 examples [00:18, 3493.11 examples/s]Running tokenizer on dataset (num_proc=64): 207688 examples [00:19, 3289.54 examples/s]Running tokenizer Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:15, 66.47 examples/s]             Running tokenizer on dataset (num_proc=64): 189688 examples [00:15, 235.17 examples/s]Running tokenizer on dataset (num_proc=64): 191688 examples [00:16, 447.72 examples/s]Running tokenizer on dataset (num_proc=64): 193688 examples [00:17, 681.75 examples/s]Running tokenizer on dataset (num_proc=64): 194688 examples [00:18, 774.08 examples/s]Running tokenizer on dataset (num_proc=64): 196688 examples [00:18, 1115.08 examples/s]Running tokenizer on dataset (num_proc=64): 198688 examples [00:19, 1601.50 examples/s]Running tokenizer on dataset (num_proc=64): 200688 examples [00:19, 2144.26 examples/s]Running tokenizer on dataset (num_proc=64): 201688 examples [00:19, 2288.99 examples/s]Running tokenizer on dataset (num_proc=64): 202688 examples [00:20, 2323.87 examples/s]Running tokenizerRunning tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:13, 72.40 examples/s]             Running tokenizer on dataset (num_proc=64): 188688 examples [00:14, 157.19 examples/s]Running tokenizer on dataset (num_proc=64): 191688 examples [00:15, 517.07 examples/s]Running tokenizer on dataset (num_proc=64): 193688 examples [00:16, 797.83 examples/s]Running tokenizer on dataset (num_proc=64): 196688 examples [00:16, 1323.40 examples/s]Running tokenizer on dataset (num_proc=64): 198688 examples [00:17, 1672.66 examples/s]Running tokenizer on dataset (num_proc=64): 201688 examples [00:17, 2336.14 examples/s]Running tokenizer on dataset (num_proc=64): 202688 examples [00:18, 2285.23 examples/s]Running tokenizer on dataset (num_proc=64): 204688 examples [00:18, 2330.73 examples/s]Running tokenizer on dataset (num_proc=64): 205688 examples [00:19, 1864.04 examples/s]Running tokenizeRunning tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 187688 examples [00:14, 67.05 examples/s]             Running tokenizer on dataset (num_proc=64): 190688 examples [00:15, 337.23 examples/s]Running tokenizer on dataset (num_proc=64): 192688 examples [00:15, 559.63 examples/s]Running tokenizer on dataset (num_proc=64): 196688 examples [00:16, 1152.90 examples/s]Running tokenizer on dataset (num_proc=64): 198688 examples [00:17, 1438.18 examples/s]Running tokenizer on dataset (num_proc=64): 203688 examples [00:17, 2509.87 examples/s]Running tokenizer on dataset (num_proc=64): 204688 examples [00:19, 1819.57 examples/s]Running tokenizer on dataset (num_proc=64): 206688 examples [00:19, 2079.52 examples/s]Running tokenizer on dataset (num_proc=64): 208688 examples [00:20, 2368.93 examples/s]Running tokenizer on dataset (num_proc=64): 210688 examples [00:20, 2668.95 examples/s]Running tokeniz on dataset (num_proc=64): 204688 examples [00:19, 2993.63 examples/s]Running tokenizer on dataset (num_proc=64): 206688 examples [00:19, 3272.50 examples/s]Running tokenizer on dataset (num_proc=64): 208688 examples [00:20, 3493.48 examples/s]Running tokenizer on dataset (num_proc=64): 209688 examples [00:20, 3065.31 examples/s]Running tokenizer on dataset (num_proc=64): 210688 examples [00:21, 2739.76 examples/s]Running tokenizer on dataset (num_proc=64): 212688 examples [00:21, 3580.49 examples/s]Running tokenizer on dataset (num_proc=64): 214688 examples [00:21, 3792.13 examples/s]Running tokenizer on dataset (num_proc=64): 215688 examples [00:22, 3829.99 examples/s]Running tokenizer on dataset (num_proc=64): 216688 examples [00:22, 3906.24 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:23, 3351.45 examples/s]Running tokenizer on dataset (num_proc=64): 219688 examples [00:23, 3834.02 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:23, 2886. on dataset (num_proc=64): 203688 examples [00:20, 2197.36 examples/s]Running tokenizer on dataset (num_proc=64): 204688 examples [00:21, 2284.38 examples/s]Running tokenizer on dataset (num_proc=64): 205688 examples [00:22, 1723.48 examples/s]Running tokenizer on dataset (num_proc=64): 206688 examples [00:22, 2168.61 examples/s]Running tokenizer on dataset (num_proc=64): 207688 examples [00:22, 2160.75 examples/s]Running tokenizer on dataset (num_proc=64): 208688 examples [00:23, 2311.20 examples/s]Running tokenizer on dataset (num_proc=64): 209688 examples [00:23, 2924.86 examples/s]Running tokenizer on dataset (num_proc=64): 210688 examples [00:23, 2648.88 examples/s]Running tokenizer on dataset (num_proc=64): 211688 examples [00:24, 2450.35 examples/s]Running tokenizer on dataset (num_proc=64): 212688 examples [00:24, 2836.36 examples/s]Running tokenizer on dataset (num_proc=64): 213688 examples [00:25, 2090.25 examples/s]Running tokenizer on dataset (num_proc=64): 214688 examples [00:25, 1795.on dataset (num_proc=64): 204688 examples [00:20, 2224.68 examples/s]Running tokenizer on dataset (num_proc=64): 206688 examples [00:20, 2434.18 examples/s]Running tokenizer on dataset (num_proc=64): 207688 examples [00:21, 2192.62 examples/s]Running tokenizer on dataset (num_proc=64): 209688 examples [00:21, 3040.86 examples/s]Running tokenizer on dataset (num_proc=64): 210688 examples [00:22, 2040.89 examples/s]Running tokenizer on dataset (num_proc=64): 212688 examples [00:23, 2479.35 examples/s]Running tokenizer on dataset (num_proc=64): 213688 examples [00:23, 2319.78 examples/s]Running tokenizer on dataset (num_proc=64): 215688 examples [00:24, 2627.10 examples/s]Running tokenizer on dataset (num_proc=64): 216688 examples [00:24, 2229.74 examples/s]Running tokenizer on dataset (num_proc=64): 217688 examples [00:26, 1668.91 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:28, 1063.98 examples/s]Running tokenizer on dataset (num_proc=64): 219688 examples [00:28, 1292.5r on dataset (num_proc=64): 207688 examples [00:20, 1875.86 examples/s]Running tokenizer on dataset (num_proc=64): 209688 examples [00:21, 2243.57 examples/s]Running tokenizer on dataset (num_proc=64): 211688 examples [00:21, 2594.45 examples/s]Running tokenizer on dataset (num_proc=64): 213688 examples [00:22, 2882.55 examples/s]Running tokenizer on dataset (num_proc=64): 215688 examples [00:23, 2776.03 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:23, 3913.50 examples/s]Running tokenizer on dataset (num_proc=64): 219688 examples [00:24, 2520.61 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:25, 1905.32 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:26, 1798.07 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:28, 1271.04 examples/s]Running tokenizer on dataset (num_proc=64): 223688 examples [00:28, 1611.89 examples/s]Running tokenizer on dataset (num_proc=64): 224688 examples [00:28, 1598r on dataset (num_proc=64): 209688 examples [00:20, 3096.14 examples/s]Running tokenizer on dataset (num_proc=64): 210688 examples [00:20, 3208.08 examples/s]Running tokenizer on dataset (num_proc=64): 212688 examples [00:20, 3260.30 examples/s]Running tokenizer on dataset (num_proc=64): 214688 examples [00:22, 2137.50 examples/s]Running tokenizer on dataset (num_proc=64): 215688 examples [00:22, 2469.41 examples/s]Running tokenizer on dataset (num_proc=64): 216688 examples [00:23, 2107.92 examples/s]Running tokenizer on dataset (num_proc=64): 217688 examples [00:23, 2217.88 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:25, 1326.67 examples/s]Running tokenizer on dataset (num_proc=64): 219688 examples [00:26, 1307.48 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:26, 1362.77 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:28, 1058.75 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:29, 978.on dataset (num_proc=64): 210688 examples [00:20, 3789.63 examples/s]Running tokenizer on dataset (num_proc=64): 211688 examples [00:20, 3357.32 examples/s]Running tokenizer on dataset (num_proc=64): 214688 examples [00:21, 3981.74 examples/s]Running tokenizer on dataset (num_proc=64): 215688 examples [00:21, 3452.39 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:22, 4137.38 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:23, 2499.42 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:25, 2106.25 examples/s]Running tokenizer on dataset (num_proc=64): 223688 examples [00:26, 1656.81 examples/s]Running tokenizer on dataset (num_proc=64): 224688 examples [00:27, 1341.91 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:29, 1177.69 examples/s]Running tokenizer on dataset (num_proc=64): 226688 examples [00:29, 1348.69 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:29, 1573.5er on dataset (num_proc=64): 212688 examples [00:21, 2931.20 examples/s]Running tokenizer on dataset (num_proc=64): 217688 examples [00:21, 4534.91 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:23, 2555.66 examples/s]Running tokenizer on dataset (num_proc=64): 219688 examples [00:24, 1764.70 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:25, 2074.88 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:25, 2379.53 examples/s]Running tokenizer on dataset (num_proc=64): 223688 examples [00:26, 2105.12 examples/s]Running tokenizer on dataset (num_proc=64): 224688 examples [00:28, 1250.10 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:28, 1523.58 examples/s]Running tokenizer on dataset (num_proc=64): 226688 examples [00:29, 1570.61 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:29, 1589.52 examples/s]Running tokenizer on dataset (num_proc=64): 229688 examples [00:30, 213on dataset (num_proc=64): 205688 examples [00:18, 3217.71 examples/s]Running tokenizer on dataset (num_proc=64): 207688 examples [00:19, 2736.17 examples/s]Running tokenizer on dataset (num_proc=64): 211688 examples [00:20, 3888.18 examples/s]Running tokenizer on dataset (num_proc=64): 214688 examples [00:20, 4373.59 examples/s]Running tokenizer on dataset (num_proc=64): 217688 examples [00:21, 5123.02 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:21, 4308.29 examples/s]Running tokenizer on dataset (num_proc=64): 219688 examples [00:22, 3493.35 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:22, 2632.43 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:23, 2231.03 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:28, 731.42 examples/s] Running tokenizer on dataset (num_proc=64): 224688 examples [00:28, 1048.91 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:32, 673.9306 examples/s]Running tokenizer on dataset (num_proc=64): 216688 examples [00:26, 1797.38 examples/s]Running tokenizer on dataset (num_proc=64): 218688 examples [00:27, 2804.02 examples/s]Running tokenizer on dataset (num_proc=64): 219688 examples [00:27, 2901.64 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:28, 2244.09 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:28, 2667.89 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:29, 1926.48 examples/s]Running tokenizer on dataset (num_proc=64): 223688 examples [00:29, 1784.96 examples/s]Running tokenizer on dataset (num_proc=64): 224688 examples [00:29, 2262.08 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:30, 2000.44 examples/s]Running tokenizer on dataset (num_proc=64): 226688 examples [00:31, 1652.69 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:31, 1851.23 examples/s]Running tokenizer on dataset (num_proc=648 examples/s]Running tokenizer on dataset (num_proc=64): 220688 examples [00:28, 1613.93 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:28, 1826.00 examples/s]Running tokenizer on dataset (num_proc=64): 223688 examples [00:29, 2769.68 examples/s]Running tokenizer on dataset (num_proc=64): 224688 examples [00:30, 2049.31 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:30, 2459.27 examples/s]Running tokenizer on dataset (num_proc=64): 226688 examples [00:30, 2034.27 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:31, 2062.25 examples/s]Running tokenizer on dataset (num_proc=64): 228688 examples [00:32, 1565.28 examples/s]Running tokenizer on dataset (num_proc=64): 229688 examples [00:32, 1985.75 examples/s]Running tokenizer on dataset (num_proc=64): 230688 examples [00:32, 2352.79 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:33, 2113.60 examples/s]Running tokenizer on dataset (num_proc=64)42 examples/s]Running tokenizer on dataset (num_proc=64): 221688 examples [00:24, 2928.12 examples/s]Running tokenizer on dataset (num_proc=64): 222688 examples [00:25, 1986.77 examples/s]Running tokenizer on dataset (num_proc=64): 223688 examples [00:25, 2266.62 examples/s]Running tokenizer on dataset (num_proc=64): 224688 examples [00:26, 1755.80 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:29, 743.37 examples/s] Running tokenizer on dataset (num_proc=64): 226688 examples [00:30, 760.66 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:31, 864.33 examples/s]Running tokenizer on dataset (num_proc=64): 228688 examples [00:33, 785.23 examples/s]Running tokenizer on dataset (num_proc=64): 229688 examples [00:33, 1063.57 examples/s]Running tokenizer on dataset (num_proc=64): 230688 examples [00:33, 1373.48 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:33, 1487.97 examples/s]Running tokenizer on dataset (num_proc=64): 98 examples/s] Running tokenizer on dataset (num_proc=64): 223688 examples [00:30, 951.71 examples/s]Running tokenizer on dataset (num_proc=64): 224688 examples [00:30, 1200.68 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:31, 1590.35 examples/s]Running tokenizer on dataset (num_proc=64): 226688 examples [00:31, 1783.57 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:32, 1656.50 examples/s]Running tokenizer on dataset (num_proc=64): 228688 examples [00:32, 1874.96 examples/s]Running tokenizer on dataset (num_proc=64): 229688 examples [00:33, 1434.86 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:33, 2503.50 examples/s]Running tokenizer on dataset (num_proc=64): 232688 examples [00:33, 2977.02 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:34, 1934.33 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:35, 2301.60 examples/s]Running tokenizer on dataset (num_proc=64 examples/s] Running tokenizer on dataset (num_proc=64): 226688 examples [00:32, 830.81 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:32, 1062.82 examples/s]Running tokenizer on dataset (num_proc=64): 228688 examples [00:33, 1197.37 examples/s]Running tokenizer on dataset (num_proc=64): 229688 examples [00:33, 1531.70 examples/s]Running tokenizer on dataset (num_proc=64): 230688 examples [00:33, 1780.64 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:33, 2130.14 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:34, 3015.92 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:34, 3228.05 examples/s]Running tokenizer on dataset (num_proc=64): 236688 examples [00:34, 4606.80 examples/s]Running tokenizer on dataset (num_proc=64): 237688 examples [00:34, 4065.00 examples/s]Running tokenizer on dataset (num_proc=64): 238688 examples [00:35, 3364.60 examples/s]Running tokenizer on dataset (num_proc=64):.49 examples/s]Running tokenizer on dataset (num_proc=64): 225688 examples [00:28, 2061.25 examples/s]Running tokenizer on dataset (num_proc=64): 226688 examples [00:29, 1848.55 examples/s]Running tokenizer on dataset (num_proc=64): 227688 examples [00:29, 2397.73 examples/s]Running tokenizer on dataset (num_proc=64): 228688 examples [00:29, 2984.40 examples/s]Running tokenizer on dataset (num_proc=64): 229688 examples [00:30, 2741.62 examples/s]Running tokenizer on dataset (num_proc=64): 230688 examples [00:30, 3281.39 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:32, 1226.19 examples/s]Running tokenizer on dataset (num_proc=64): 232688 examples [00:32, 1548.94 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:32, 2627.36 examples/s]Running tokenizer on dataset (num_proc=64): 235688 examples [00:34, 1477.66 examples/s]Running tokenizer on dataset (num_proc=64): 236688 examples [00:35, 1101.34 examples/s]Running tokenizer on dataset (num_proc=62.63 examples/s]Running tokenizer on dataset (num_proc=64): 230688 examples [00:30, 2411.79 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:31, 2007.29 examples/s]Running tokenizer on dataset (num_proc=64): 232688 examples [00:31, 2527.01 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:32, 1648.47 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:33, 1521.53 examples/s]Running tokenizer on dataset (num_proc=64): 235688 examples [00:33, 2000.84 examples/s]Running tokenizer on dataset (num_proc=64): 236688 examples [00:33, 2194.74 examples/s]Running tokenizer on dataset (num_proc=64): 237688 examples [00:36, 908.35 examples/s] Running tokenizer on dataset (num_proc=64): 238688 examples [00:36, 1198.29 examples/s]Running tokenizer on dataset (num_proc=64): 239688 examples [00:36, 1609.29 examples/s]Running tokenizer on dataset (num_proc=64): 240688 examples [00:36, 2042.83 examples/s]Running tokenizer on dataset (num_proc=2 examples/s]Running tokenizer on dataset (num_proc=64): 228688 examples [00:30, 1795.84 examples/s]Running tokenizer on dataset (num_proc=64): 229688 examples [00:30, 2131.02 examples/s]Running tokenizer on dataset (num_proc=64): 230688 examples [00:30, 2425.27 examples/s]Running tokenizer on dataset (num_proc=64): 232688 examples [00:31, 2034.47 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:32, 1881.38 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:33, 1383.73 examples/s]Running tokenizer on dataset (num_proc=64): 235688 examples [00:34, 1421.18 examples/s]Running tokenizer on dataset (num_proc=64): 236688 examples [00:35, 1255.48 examples/s]Running tokenizer on dataset (num_proc=64): 238688 examples [00:35, 1761.24 examples/s]Running tokenizer on dataset (num_proc=64): 239688 examples [00:36, 2151.29 examples/s]Running tokenizer on dataset (num_proc=64): 240688 examples [00:36, 2139.17 examples/s]Running tokenizer on dataset (num_proc=64)): 229688 examples [00:32, 2630.94 examples/s]Running tokenizer on dataset (num_proc=64): 230688 examples [00:32, 2502.70 examples/s]Running tokenizer on dataset (num_proc=64): 231688 examples [00:33, 2146.17 examples/s]Running tokenizer on dataset (num_proc=64): 232688 examples [00:33, 2532.23 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:35, 1408.68 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:35, 1581.14 examples/s]Running tokenizer on dataset (num_proc=64): 235688 examples [00:35, 1914.22 examples/s]Running tokenizer on dataset (num_proc=64): 237688 examples [00:36, 2952.19 examples/s]Running tokenizer on dataset (num_proc=64): 238688 examples [00:36, 2942.57 examples/s]Running tokenizer on dataset (num_proc=64): 239688 examples [00:36, 3045.28 examples/s]Running tokenizer on dataset (num_proc=64): 240688 examples [00:37, 3001.57 examples/s]Running tokenizer on dataset (num_proc=64): 241688 examples [00:37, 3486.10 examples/s]Running t: 232688 examples [00:33, 2576.07 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:35, 1152.64 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:35, 1459.38 examples/s]Running tokenizer on dataset (num_proc=64): 235688 examples [00:36, 1869.06 examples/s]Running tokenizer on dataset (num_proc=64): 236688 examples [00:36, 2299.94 examples/s]Running tokenizer on dataset (num_proc=64): 237688 examples [00:36, 2874.12 examples/s]Running tokenizer on dataset (num_proc=64): 238688 examples [00:36, 2397.51 examples/s]Running tokenizer on dataset (num_proc=64): 239688 examples [00:37, 2916.86 examples/s]Running tokenizer on dataset (num_proc=64): 240688 examples [00:37, 3159.57 examples/s]Running tokenizer on dataset (num_proc=64): 241688 examples [00:37, 3582.38 examples/s]Running tokenizer on dataset (num_proc=64): 242688 examples [00:38, 2831.94 examples/s]Running tokenizer on dataset (num_proc=64): 244688 examples [00:38, 4178.93 examples/s]Running to232688 examples [00:34, 1883.67 examples/s]Running tokenizer on dataset (num_proc=64): 233688 examples [00:34, 2174.09 examples/s]Running tokenizer on dataset (num_proc=64): 234688 examples [00:34, 2576.08 examples/s]Running tokenizer on dataset (num_proc=64): 235688 examples [00:35, 2129.54 examples/s]Running tokenizer on dataset (num_proc=64): 236688 examples [00:35, 2345.67 examples/s]Running tokenizer on dataset (num_proc=64): 238688 examples [00:36, 2993.45 examples/s]Running tokenizer on dataset (num_proc=64): 239688 examples [00:36, 2633.94 examples/s]Running tokenizer on dataset (num_proc=64): 240688 examples [00:36, 3136.78 examples/s]Running tokenizer on dataset (num_proc=64): 241688 examples [00:37, 3454.02 examples/s]Running tokenizer on dataset (num_proc=64): 243688 examples [00:37, 4700.40 examples/s]Running tokenizer on dataset (num_proc=64): 244688 examples [00:38, 2861.29 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:38, 4376.08 examples/s]Running toke): 235688 examples [00:35, 2323.21 examples/s]Running tokenizer on dataset (num_proc=64): 236688 examples [00:36, 1734.85 examples/s]Running tokenizer on dataset (num_proc=64): 238688 examples [00:36, 2886.14 examples/s]Running tokenizer on dataset (num_proc=64): 239688 examples [00:36, 3174.99 examples/s]Running tokenizer on dataset (num_proc=64): 241688 examples [00:36, 4833.49 examples/s]Running tokenizer on dataset (num_proc=64): 243688 examples [00:38, 3085.11 examples/s]Running tokenizer on dataset (num_proc=64): 245688 examples [00:38, 3567.53 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:38, 3147.27 examples/s]Running tokenizer on dataset (num_proc=64): 247688 examples [00:39, 3427.03 examples/s]Running tokenizer on dataset (num_proc=64): 248688 examples [00:39, 2676.14 examples/s]Running tokenizer on dataset (num_proc=64): 249688 examples [00:39, 3169.34 examples/s]Running tokenizer on dataset (num_proc=64): 250688 examples [00:40, 3533.30 examples/s]Running t 240688 examples [00:35, 4216.64 examples/s]Running tokenizer on dataset (num_proc=64): 242688 examples [00:36, 4050.85 examples/s]Running tokenizer on dataset (num_proc=64): 243688 examples [00:36, 4583.74 examples/s]Running tokenizer on dataset (num_proc=64): 244688 examples [00:36, 3257.40 examples/s]Running tokenizer on dataset (num_proc=64): 245688 examples [00:37, 3581.09 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:37, 3695.43 examples/s]Running tokenizer on dataset (num_proc=64): 247688 examples [00:37, 3790.68 examples/s]Running tokenizer on dataset (num_proc=64): 248688 examples [00:38, 2187.63 examples/s]Running tokenizer on dataset (num_proc=64): 249688 examples [00:38, 2447.36 examples/s]Running tokenizer on dataset (num_proc=64): 251688 examples [00:39, 3523.62 examples/s]Running tokenizer on dataset (num_proc=64): 252688 examples [00:39, 2920.67 examples/s]Running tokenizer on dataset (num_proc=64): 254688 examples [00:40, 3140.25 examples/s]Running tok4): 237688 examples [00:36, 1137.40 examples/s]Running tokenizer on dataset (num_proc=64): 238688 examples [00:37, 1341.45 examples/s]Running tokenizer on dataset (num_proc=64): 240688 examples [00:37, 2066.90 examples/s]Running tokenizer on dataset (num_proc=64): 241688 examples [00:37, 2477.37 examples/s]Running tokenizer on dataset (num_proc=64): 242688 examples [00:37, 2690.96 examples/s]Running tokenizer on dataset (num_proc=64): 243688 examples [00:38, 2232.49 examples/s]Running tokenizer on dataset (num_proc=64): 244688 examples [00:38, 2433.59 examples/s]Running tokenizer on dataset (num_proc=64): 245688 examples [00:39, 2715.58 examples/s]Running tokenizer on dataset (num_proc=64): 247688 examples [00:39, 3511.21 examples/s]Running tokenizer on dataset (num_proc=64): 248605 examples [00:39, 3572.33 examples/s]Running tokenizer on dataset (num_proc=64): 249605 examples [00:39, 3908.16 examples/s]Running tokenizer on dataset (num_proc=64): 250605 examples [00:40, 3665.29 examples/s]Running 64): 241688 examples [00:36, 2573.38 examples/s]Running tokenizer on dataset (num_proc=64): 242688 examples [00:37, 2431.17 examples/s]Running tokenizer on dataset (num_proc=64): 243688 examples [00:37, 2942.10 examples/s]Running tokenizer on dataset (num_proc=64): 245688 examples [00:37, 4648.78 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:38, 3571.25 examples/s]Running tokenizer on dataset (num_proc=64): 247688 examples [00:38, 3385.07 examples/s]Running tokenizer on dataset (num_proc=64): 248688 examples [00:38, 3438.23 examples/s]Running tokenizer on dataset (num_proc=64): 250688 examples [00:40, 2198.58 examples/s]Running tokenizer on dataset (num_proc=64): 251688 examples [00:40, 2356.98 examples/s]Running tokenizer on dataset (num_proc=64): 252688 examples [00:41, 1930.21 examples/s]Running tokenizer on dataset (num_proc=64): 253688 examples [00:41, 2315.78 examples/s]Running tokenizer on dataset (num_proc=64): 254688 examples [00:41, 2785.77 examples/s]Running: 241688 examples [00:37, 2130.10 examples/s]Running tokenizer on dataset (num_proc=64): 242688 examples [00:37, 2289.69 examples/s]Running tokenizer on dataset (num_proc=64): 244688 examples [00:37, 3657.86 examples/s]Running tokenizer on dataset (num_proc=64): 245688 examples [00:38, 3024.68 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:38, 2224.79 examples/s]Running tokenizer on dataset (num_proc=64): 247688 examples [00:39, 1998.44 examples/s]Running tokenizer on dataset (num_proc=64): 249688 examples [00:40, 2086.59 examples/s]Running tokenizer on dataset (num_proc=64): 250688 examples [00:40, 2125.66 examples/s]Running tokenizer on dataset (num_proc=64): 251688 examples [00:41, 2521.03 examples/s]Running tokenizer on dataset (num_proc=64): 253688 examples [00:41, 3065.24 examples/s]Running tokenizer on dataset (num_proc=64): 254688 examples [00:41, 3534.96 examples/s]Running tokenizer on dataset (num_proc=64): 256688 examples [00:42, 3481.03 examples/s]Running tookenizer on dataset (num_proc=64): 242688 examples [00:37, 4229.07 examples/s]Running tokenizer on dataset (num_proc=64): 243688 examples [00:38, 2394.33 examples/s]Running tokenizer on dataset (num_proc=64): 245688 examples [00:38, 2692.83 examples/s]Running tokenizer on dataset (num_proc=64): 246688 examples [00:39, 2163.76 examples/s]Running tokenizer on dataset (num_proc=64): 248688 examples [00:41, 1632.64 examples/s]Running tokenizer on dataset (num_proc=64): 249688 examples [00:41, 2004.53 examples/s]Running tokenizer on dataset (num_proc=64): 250688 examples [00:41, 2389.91 examples/s]Running tokenizer on dataset (num_proc=64): 252688 examples [00:41, 3579.91 examples/s]Running tokenizer on dataset (num_proc=64): 253688 examples [00:42, 2218.26 examples/s]Running tokenizer on dataset (num_proc=64): 254688 examples [00:43, 2214.58 examples/s]Running tokenizer on dataset (num_proc=64): 255688 examples [00:43, 2721.62 examples/s]Running tokenizer on dataset (num_proc=64): 256688 examples [00:4kenizer on dataset (num_proc=64): 245688 examples [00:38, 4510.72 examples/s]Running tokenizer on dataset (num_proc=64): 247688 examples [00:38, 4752.73 examples/s]Running tokenizer on dataset (num_proc=64): 249688 examples [00:39, 4388.28 examples/s]Running tokenizer on dataset (num_proc=64): 250688 examples [00:40, 2304.59 examples/s]Running tokenizer on dataset (num_proc=64): 252688 examples [00:41, 2806.84 examples/s]Running tokenizer on dataset (num_proc=64): 253688 examples [00:41, 2954.91 examples/s]Running tokenizer on dataset (num_proc=64): 254688 examples [00:41, 2754.72 examples/s]Running tokenizer on dataset (num_proc=64): 255688 examples [00:42, 2255.29 examples/s]Running tokenizer on dataset (num_proc=64): 256605 examples [00:42, 2485.78 examples/s]Running tokenizer on dataset (num_proc=64): 257605 examples [00:43, 2294.34 examples/s]Running tokenizer on dataset (num_proc=64): 259605 examples [00:43, 3347.81 examples/s]Running tokenizer on dataset (num_proc=64): 260605 examples [00:43nizer on dataset (num_proc=64): 247688 examples [00:38, 2778.86 examples/s]Running tokenizer on dataset (num_proc=64): 249688 examples [00:40, 2169.16 examples/s]Running tokenizer on dataset (num_proc=64): 250688 examples [00:40, 2594.79 examples/s]Running tokenizer on dataset (num_proc=64): 251688 examples [00:40, 2669.06 examples/s]Running tokenizer on dataset (num_proc=64): 252688 examples [00:41, 2517.88 examples/s]Running tokenizer on dataset (num_proc=64): 255688 examples [00:41, 4723.54 examples/s]Running tokenizer on dataset (num_proc=64): 258688 examples [00:41, 6753.65 examples/s]Running tokenizer on dataset (num_proc=64): 260688 examples [00:42, 4537.40 examples/s]Running tokenizer on dataset (num_proc=64): 261688 examples [00:42, 4965.29 examples/s]Running tokenizer on dataset (num_proc=64): 263605 examples [00:42, 4423.42 examples/s]Running tokenizer on dataset (num_proc=64): 264605 examples [00:43, 4391.65 examples/s]Running tokenizer on dataset (num_proc=64): 265605 examples [00:44, okenizer on dataset (num_proc=64): 251688 examples [00:40, 3189.53 examples/s]Running tokenizer on dataset (num_proc=64): 252688 examples [00:41, 2321.36 examples/s]Running tokenizer on dataset (num_proc=64): 253688 examples [00:41, 2770.30 examples/s]Running tokenizer on dataset (num_proc=64): 255688 examples [00:42, 2408.89 examples/s]Running tokenizer on dataset (num_proc=64): 257688 examples [00:42, 3442.68 examples/s]Running tokenizer on dataset (num_proc=64): 258688 examples [00:43, 2874.70 examples/s]Running tokenizer on dataset (num_proc=64): 259688 examples [00:43, 3028.97 examples/s]Running tokenizer on dataset (num_proc=64): 260688 examples [00:43, 2641.78 examples/s]Running tokenizer on dataset (num_proc=64): 261688 examples [00:44, 2930.40 examples/s]Running tokenizer on dataset (num_proc=64): 262688 examples [00:44, 3568.16 examples/s]Running tokenizer on dataset (num_proc=64): 263688 examples [00:44, 4250.46 examples/s]Running tokenizer on dataset (num_proc=64): 264688 examples [00:4enizer on dataset (num_proc=64): 255688 examples [00:40, 3462.66 examples/s]Running tokenizer on dataset (num_proc=64): 256688 examples [00:40, 4078.12 examples/s]Running tokenizer on dataset (num_proc=64): 258688 examples [00:40, 4302.55 examples/s]Running tokenizer on dataset (num_proc=64): 259688 examples [00:41, 4307.12 examples/s]Running tokenizer on dataset (num_proc=64): 260688 examples [00:41, 2882.64 examples/s]Running tokenizer on dataset (num_proc=64): 261688 examples [00:42, 2806.88 examples/s]Running tokenizer on dataset (num_proc=64): 263688 examples [00:42, 3235.74 examples/s]Running tokenizer on dataset (num_proc=64): 265605 examples [00:43, 4184.37 examples/s]Running tokenizer on dataset (num_proc=64): 266605 examples [00:43, 3666.05 examples/s]Running tokenizer on dataset (num_proc=64): 267605 examples [00:43, 3965.30 examples/s]Running tokenizer on dataset (num_proc=64): 268605 examples [00:44, 2141.51 examples/s]Running tokenizer on dataset (num_proc=64): 269605 examples [00:44, tokenizer on dataset (num_proc=64): 255688 examples [00:41, 3442.23 examples/s]Running tokenizer on dataset (num_proc=64): 258688 examples [00:41, 5863.64 examples/s]Running tokenizer on dataset (num_proc=64): 260688 examples [00:42, 5545.52 examples/s]Running tokenizer on dataset (num_proc=64): 261688 examples [00:42, 4587.94 examples/s]Running tokenizer on dataset (num_proc=64): 264688 examples [00:42, 6988.13 examples/s]Running tokenizer on dataset (num_proc=64): 265688 examples [00:43, 4640.16 examples/s]Running tokenizer on dataset (num_proc=64): 266688 examples [00:43, 4884.39 examples/s]Running tokenizer on dataset (num_proc=64): 267688 examples [00:43, 4425.76 examples/s]Running tokenizer on dataset (num_proc=64): 268605 examples [00:44, 3707.63 examples/s]Running tokenizer on dataset (num_proc=64): 269522 examples [00:44, 3621.09 examples/s]Running tokenizer on dataset (num_proc=64): 270522 examples [00:45, 3075.61 examples/s]Running tokenizer on dataset (num_proc=64): 271522 examples [00tokenizer on dataset (num_proc=64): 251605 examples [00:40, 2469.53 examples/s]Running tokenizer on dataset (num_proc=64): 252605 examples [00:41, 2598.98 examples/s]Running tokenizer on dataset (num_proc=64): 253522 examples [00:41, 2514.74 examples/s]Running tokenizer on dataset (num_proc=64): 254522 examples [00:42, 2088.34 examples/s]Running tokenizer on dataset (num_proc=64): 255522 examples [00:42, 2431.19 examples/s]Running tokenizer on dataset (num_proc=64): 256522 examples [00:42, 2860.63 examples/s]Running tokenizer on dataset (num_proc=64): 259522 examples [00:43, 5247.96 examples/s]Running tokenizer on dataset (num_proc=64): 260522 examples [00:43, 3651.80 examples/s]Running tokenizer on dataset (num_proc=64): 262439 examples [00:44, 3845.28 examples/s]Running tokenizer on dataset (num_proc=64): 264356 examples [00:44, 3212.02 examples/s]Running tokenizer on dataset (num_proc=64): 265356 examples [00:45, 2367.20 examples/s]Running tokenizer on dataset (num_proc=64): 266356 examples [00:kenizer on dataset (num_proc=64): 257688 examples [00:42, 3614.97 examples/s]Running tokenizer on dataset (num_proc=64): 260605 examples [00:43, 3297.48 examples/s]Running tokenizer on dataset (num_proc=64): 261605 examples [00:43, 3398.68 examples/s]Running tokenizer on dataset (num_proc=64): 262605 examples [00:44, 3283.05 examples/s]Running tokenizer on dataset (num_proc=64): 263605 examples [00:44, 3739.27 examples/s]Running tokenizer on dataset (num_proc=64): 265605 examples [00:45, 2406.91 examples/s]Running tokenizer on dataset (num_proc=64): 266605 examples [00:45, 2653.75 examples/s]Running tokenizer on dataset (num_proc=64): 267605 examples [00:45, 2811.82 examples/s]Running tokenizer on dataset (num_proc=64): 269605 examples [00:46, 4299.85 examples/s]Running tokenizer on dataset (num_proc=64): 270605 examples [00:46, 4834.34 examples/s]Running tokenizer on dataset (num_proc=64): 271605 examples [00:46, 5354.78 examples/s]Running tokenizer on dataset (num_proc=64): 272522 examples [00:46, 3923.10 examples/s]Running tokenizer on dataset (num_proc=64): 262605 examples [00:43, 5796.98 examples/s]Running tokenizer on dataset (num_proc=64): 264605 examples [00:43, 6582.42 examples/s]Running tokenizer on dataset (num_proc=64): 266605 examples [00:44, 4536.82 examples/s]Running tokenizer on dataset (num_proc=64): 267605 examples [00:45, 3498.84 examples/s]Running tokenizer on dataset (num_proc=64): 268605 examples [00:45, 3102.05 examples/s]Running tokenizer on dataset (num_proc=64): 271605 examples [00:46, 2815.93 examples/s]Running tokenizer on dataset (num_proc=64): 272522 examples [00:47, 2349.59 examples/s]Running tokenizer on dataset (num_proc=64): 273439 examples [00:48, 2030.60 examples/s]Running tokenizer on dataset (num_proc=64): 274356 examples [00:48, 2039.32 examples/s]Running tokenizer on dataset (num_proc=64): 275356 examples [00:49, 1630.18 examples/s]Running tokenizer on dataset (num_proc=64): 276273 examples [00:49, 2050.91 examples/s]Running tokenizer on dataset (num_2623.39 examples/s]Running tokenizer on dataset (num_proc=64): 267605 examples [00:44, 3814.53 examples/s]Running tokenizer on dataset (num_proc=64): 268605 examples [00:44, 3423.77 examples/s]Running tokenizer on dataset (num_proc=64): 269605 examples [00:44, 3541.13 examples/s]Running tokenizer on dataset (num_proc=64): 271605 examples [00:44, 5158.79 examples/s]Running tokenizer on dataset (num_proc=64): 272605 examples [00:45, 3253.84 examples/s]Running tokenizer on dataset (num_proc=64): 274522 examples [00:46, 3739.12 examples/s]Running tokenizer on dataset (num_proc=64): 276439 examples [00:46, 4650.03 examples/s]Running tokenizer on dataset (num_proc=64): 277439 examples [00:46, 4665.76 examples/s]Running tokenizer on dataset (num_proc=64): 278439 examples [00:46, 4302.59 examples/s]Running tokenizer on dataset (num_proc=64): 279439 examples [00:48, 1828.45 examples/s]Running tokenizer on dataset (num_proc=64): 280439 examples [00:49, 1663.74 examples/s]Running tokenizer on dataset (num_pr3, 3266.71 examples/s]Running tokenizer on dataset (num_proc=64): 257688 examples [00:44, 2509.48 examples/s]Running tokenizer on dataset (num_proc=64): 258688 examples [00:44, 2365.98 examples/s]Running tokenizer on dataset (num_proc=64): 259688 examples [00:44, 2821.38 examples/s]Running tokenizer on dataset (num_proc=64): 261688 examples [00:45, 2394.28 examples/s]Running tokenizer on dataset (num_proc=64): 262688 examples [00:46, 1862.94 examples/s]Running tokenizer on dataset (num_proc=64): 264688 examples [00:46, 2923.65 examples/s]Running tokenizer on dataset (num_proc=64): 265688 examples [00:46, 3332.23 examples/s]Running tokenizer on dataset (num_proc=64): 266688 examples [00:47, 3541.20 examples/s]Running tokenizer on dataset (num_proc=64): 267688 examples [00:47, 3190.78 examples/s]Running tokenizer on dataset (num_proc=64): 270688 examples [00:47, 4318.55 examples/s]Running tokenizer on dataset (num_proc=64): 271688 examples [00:49, 2255.30 examples/s]Running tokenizer on dataset (num4, 3130.00 examples/s]Running tokenizer on dataset (num_proc=64): 265688 examples [00:45, 1981.18 examples/s]Running tokenizer on dataset (num_proc=64): 267688 examples [00:46, 2899.87 examples/s]Running tokenizer on dataset (num_proc=64): 268688 examples [00:46, 2524.11 examples/s]Running tokenizer on dataset (num_proc=64): 270688 examples [00:46, 3892.73 examples/s]Running tokenizer on dataset (num_proc=64): 271605 examples [00:47, 2934.96 examples/s]Running tokenizer on dataset (num_proc=64): 272605 examples [00:48, 2305.66 examples/s]Running tokenizer on dataset (num_proc=64): 274439 examples [00:49, 1822.96 examples/s]Running tokenizer on dataset (num_proc=64): 275356 examples [00:49, 1852.88 examples/s]Running tokenizer on dataset (num_proc=64): 276273 examples [00:50, 1915.53 examples/s]Running tokenizer on dataset (num_proc=64): 277273 examples [00:50, 1967.61 examples/s]Running tokenizer on dataset (num_proc=64): 278273 examples [00:51, 2157.72 examples/s]Running tokenizer on dataset (num:45, 3263.55 examples/s]Running tokenizer on dataset (num_proc=64): 272522 examples [00:46, 2135.16 examples/s]Running tokenizer on dataset (num_proc=64): 273522 examples [00:47, 1683.35 examples/s]Running tokenizer on dataset (num_proc=64): 275522 examples [00:47, 2796.96 examples/s]Running tokenizer on dataset (num_proc=64): 276439 examples [00:48, 1451.51 examples/s]Running tokenizer on dataset (num_proc=64): 277356 examples [00:49, 1345.26 examples/s]Running tokenizer on dataset (num_proc=64): 278356 examples [00:49, 1599.08 examples/s]Running tokenizer on dataset (num_proc=64): 279356 examples [00:50, 2091.57 examples/s]Running tokenizer on dataset (num_proc=64): 280356 examples [00:50, 1921.44 examples/s]Running tokenizer on dataset (num_proc=64): 281273 examples [00:51, 1878.68 examples/s]Running tokenizer on dataset (num_proc=64): 282190 examples [00:51, 2261.55 examples/s]Running tokenizer on dataset (num_proc=64): 283107 examples [00:51, 2223.46 examples/s]Running tokenizer on dataset (n46, 1725.11 examples/s]Running tokenizer on dataset (num_proc=64): 267356 examples [00:46, 2114.65 examples/s]Running tokenizer on dataset (num_proc=64): 268356 examples [00:48, 1592.41 examples/s]Running tokenizer on dataset (num_proc=64): 269356 examples [00:48, 1869.88 examples/s]Running tokenizer on dataset (num_proc=64): 270356 examples [00:48, 2214.94 examples/s]Running tokenizer on dataset (num_proc=64): 272356 examples [00:48, 3392.49 examples/s]Running tokenizer on dataset (num_proc=64): 273356 examples [00:49, 3421.02 examples/s]Running tokenizer on dataset (num_proc=64): 274356 examples [00:49, 4065.65 examples/s]Running tokenizer on dataset (num_proc=64): 275356 examples [00:50, 2385.28 examples/s]Running tokenizer on dataset (num_proc=64): 276273 examples [00:50, 2344.46 examples/s]Running tokenizer on dataset (num_proc=64): 278273 examples [00:51, 2769.06 examples/s]Running tokenizer on dataset (num_proc=64): 280190 examples [00:52, 2401.72 examples/s]Running tokenizer on dataset (nu 2567.44 examples/s]Running tokenizer on dataset (num_proc=64): 270605 examples [00:45, 3007.90 examples/s]Running tokenizer on dataset (num_proc=64): 272605 examples [00:46, 2467.92 examples/s]Running tokenizer on dataset (num_proc=64): 273605 examples [00:46, 2401.79 examples/s]Running tokenizer on dataset (num_proc=64): 276439 examples [00:48, 2049.14 examples/s]Running tokenizer on dataset (num_proc=64): 277439 examples [00:50, 1125.04 examples/s]Running tokenizer on dataset (num_proc=64): 278356 examples [00:50, 1338.86 examples/s]Running tokenizer on dataset (num_proc=64): 279273 examples [00:51, 1559.07 examples/s]Running tokenizer on dataset (num_proc=64): 280273 examples [00:51, 1623.76 examples/s]Running tokenizer on dataset (num_proc=64): 281273 examples [00:51, 1889.92 examples/s]Running tokenizer on dataset (num_proc=64): 283107 examples [00:52, 2473.24 examples/s]Running tokenizer on dataset (num_proc=64): 284024 examples [00:53, 1857.88 examples/s]Running tokenizer on dataset (num_p, 3504.48 examples/s]Running tokenizer on dataset (num_proc=64): 274522 examples [00:47, 4099.70 examples/s]Running tokenizer on dataset (num_proc=64): 275522 examples [00:47, 3904.86 examples/s]Running tokenizer on dataset (num_proc=64): 276439 examples [00:47, 4359.96 examples/s]Running tokenizer on dataset (num_proc=64): 277356 examples [00:49, 1521.60 examples/s]Running tokenizer on dataset (num_proc=64): 279273 examples [00:50, 1967.44 examples/s]Running tokenizer on dataset (num_proc=64): 280273 examples [00:50, 1778.87 examples/s]Running tokenizer on dataset (num_proc=64): 281190 examples [00:51, 1849.84 examples/s]Running tokenizer on dataset (num_proc=64): 282107 examples [00:52, 1252.44 examples/s]Running tokenizer on dataset (num_proc=64): 283024 examples [00:52, 1554.49 examples/s]Running tokenizer on dataset (num_proc=64): 283941 examples [00:53, 1716.09 examples/s]Running tokenizer on dataset (num_proc=64): 285858 examples [00:53, 2465.63 examples/s]Running tokenizer on dataset (num_proc=64): 277190 examples [00:49, 2513.95 examples/s]Running tokenizer on dataset (num_proc=64): 278107 examples [00:50, 2136.63 examples/s]Running tokenizer on dataset (num_proc=64): 279024 examples [00:51, 1646.51 examples/s]Running tokenizer on dataset (num_proc=64): 281858 examples [00:52, 2409.79 examples/s]Running tokenizer on dataset (num_proc=64): 282775 examples [00:52, 2718.41 examples/s]Running tokenizer on dataset (num_proc=64): 284692 examples [00:53, 2351.87 examples/s]Running tokenizer on dataset (num_proc=64): 285609 examples [00:53, 2708.06 examples/s]Running tokenizer on dataset (num_proc=64): 287526 examples [00:53, 3470.81 examples/s]Running tokenizer on dataset (num_proc=64): 288526 examples [00:54, 2628.19 examples/s]Running tokenizer on dataset (num_proc=64): 289443 examples [00:54, 3119.12 examples/s]Running tokenizer on dataset (num_proc=64): 290443 examples [00:54, 2827.28 examples/s]Running tokenizer on dataset (num_proc=64): 291360 examples [00:55, 2515.88 examples/s]Ru_proc=64): 279190 examples [00:51, 1789.05 examples/s]Running tokenizer on dataset (num_proc=64): 280107 examples [00:53, 1276.74 examples/s]Running tokenizer on dataset (num_proc=64): 281107 examples [00:53, 1677.86 examples/s]Running tokenizer on dataset (num_proc=64): 282107 examples [00:53, 1789.93 examples/s]Running tokenizer on dataset (num_proc=64): 283107 examples [00:53, 2245.32 examples/s]Running tokenizer on dataset (num_proc=64): 284107 examples [00:54, 1699.76 examples/s]Running tokenizer on dataset (num_proc=64): 285107 examples [00:54, 2263.98 examples/s]Running tokenizer on dataset (num_proc=64): 286107 examples [00:55, 2936.09 examples/s]Running tokenizer on dataset (num_proc=64): 288107 examples [00:55, 3380.87 examples/s]Running tokenizer on dataset (num_proc=64): 289107 examples [00:55, 3795.08 examples/s]Running tokenizer on dataset (num_proc=64): 290024 examples [00:55, 3951.29 examples/s]Running tokenizer on dataset (num_proc=64): 290941 examples [00:56, 4477.90 examples/s]R_proc=64): 272688 examples [00:51, 1258.46 examples/s]Running tokenizer on dataset (num_proc=64): 274688 examples [00:51, 1833.00 examples/s]Running tokenizer on dataset (num_proc=64): 275688 examples [00:52, 1869.05 examples/s]Running tokenizer on dataset (num_proc=64): 277688 examples [00:52, 2418.10 examples/s]Running tokenizer on dataset (num_proc=64): 279605 examples [00:52, 2938.62 examples/s]Running tokenizer on dataset (num_proc=64): 280605 examples [00:53, 2930.19 examples/s]Running tokenizer on dataset (num_proc=64): 281605 examples [00:53, 3215.58 examples/s]Running tokenizer on dataset (num_proc=64): 282605 examples [00:54, 2639.68 examples/s]Running tokenizer on dataset (num_proc=64): 283522 examples [00:55, 1742.29 examples/s]Running tokenizer on dataset (num_proc=64): 284439 examples [00:55, 1969.45 examples/s]Running tokenizer on dataset (num_proc=64): 285439 examples [00:56, 1889.54 examples/s]Running tokenizer on dataset (num_proc=64): 286356 examples [00:56, 2367.58 examples/s]Rroc=64): 284941 examples [00:53, 2072.18 examples/s]Running tokenizer on dataset (num_proc=64): 285858 examples [00:53, 2381.54 examples/s]Running tokenizer on dataset (num_proc=64): 286858 examples [00:54, 2324.59 examples/s]Running tokenizer on dataset (num_proc=64): 287775 examples [00:54, 2676.52 examples/s]Running tokenizer on dataset (num_proc=64): 288692 examples [00:54, 3194.08 examples/s]Running tokenizer on dataset (num_proc=64): 290609 examples [00:54, 4465.25 examples/s]Running tokenizer on dataset (num_proc=64): 291526 examples [00:55, 3788.52 examples/s]Running tokenizer on dataset (num_proc=64): 292526 examples [00:55, 3570.64 examples/s]Running tokenizer on dataset (num_proc=64): 293526 examples [00:55, 3486.65 examples/s]Running tokenizer on dataset (num_proc=64): 294443 examples [00:56, 3058.05 examples/s]Running tokenizer on dataset (num_proc=64): 299028 examples [00:56, 7715.12 examples/s]Running tokenizer on dataset (num_proc=64): 300862 examples [00:56, 6389.99 examples/s]Runum_proc=64): 284024 examples [00:52, 2489.35 examples/s]Running tokenizer on dataset (num_proc=64): 284941 examples [00:52, 2476.82 examples/s]Running tokenizer on dataset (num_proc=64): 286858 examples [00:53, 2431.04 examples/s]Running tokenizer on dataset (num_proc=64): 287775 examples [00:53, 2296.63 examples/s]Running tokenizer on dataset (num_proc=64): 288692 examples [00:54, 1862.02 examples/s]Running tokenizer on dataset (num_proc=64): 289609 examples [00:54, 1913.01 examples/s]Running tokenizer on dataset (num_proc=64): 292443 examples [00:55, 3731.52 examples/s]Running tokenizer on dataset (num_proc=64): 295277 examples [00:56, 3467.56 examples/s]Running tokenizer on dataset (num_proc=64): 297111 examples [00:56, 3949.98 examples/s]Running tokenizer on dataset (num_proc=64): 298111 examples [00:56, 3745.13 examples/s]Running tokenizer on dataset (num_proc=64): 299111 examples [00:57, 2883.86 examples/s]Running tokenizer on dataset (num_proc=64): 300945 examples [00:57, 3206.81 examples/s]m_proc=64): 281190 examples [00:52, 1961.76 examples/s]Running tokenizer on dataset (num_proc=64): 282190 examples [00:53, 1626.17 examples/s]Running tokenizer on dataset (num_proc=64): 283190 examples [00:54, 1908.61 examples/s]Running tokenizer on dataset (num_proc=64): 284107 examples [00:54, 2319.00 examples/s]Running tokenizer on dataset (num_proc=64): 285107 examples [00:54, 2153.66 examples/s]Running tokenizer on dataset (num_proc=64): 286107 examples [00:54, 2590.26 examples/s]Running tokenizer on dataset (num_proc=64): 288941 examples [00:55, 4476.95 examples/s]Running tokenizer on dataset (num_proc=64): 289858 examples [00:55, 3893.03 examples/s]Running tokenizer on dataset (num_proc=64): 290858 examples [00:55, 4346.77 examples/s]Running tokenizer on dataset (num_proc=64): 291775 examples [00:56, 2695.10 examples/s]Running tokenizer on dataset (num_proc=64): 292692 examples [00:56, 2960.75 examples/s]Running tokenizer on dataset (num_proc=64): 293609 examples [00:57, 2625.46 examples/s]oc=64): 281356 examples [00:50, 1008.90 examples/s]Running tokenizer on dataset (num_proc=64): 283190 examples [00:51, 1483.08 examples/s]Running tokenizer on dataset (num_proc=64): 284107 examples [00:51, 1750.92 examples/s]Running tokenizer on dataset (num_proc=64): 286024 examples [00:53, 1339.64 examples/s]Running tokenizer on dataset (num_proc=64): 287024 examples [00:53, 1629.31 examples/s]Running tokenizer on dataset (num_proc=64): 287941 examples [00:54, 1445.62 examples/s]Running tokenizer on dataset (num_proc=64): 289775 examples [00:55, 1802.96 examples/s]Running tokenizer on dataset (num_proc=64): 290692 examples [00:55, 1769.89 examples/s]Running tokenizer on dataset (num_proc=64): 292526 examples [00:56, 2322.53 examples/s]Running tokenizer on dataset (num_proc=64): 293443 examples [00:56, 2595.62 examples/s]Running tokenizer on dataset (num_proc=64): 294360 examples [00:56, 2268.39 examples/s]Running tokenizer on dataset (num_proc=64): 295360 examples [00:57, 1938.51 examples/s]Runnproc=64): 286858 examples [00:53, 2746.47 examples/s]Running tokenizer on dataset (num_proc=64): 287775 examples [00:54, 2385.26 examples/s]Running tokenizer on dataset (num_proc=64): 288775 examples [00:54, 2701.88 examples/s]Running tokenizer on dataset (num_proc=64): 290775 examples [00:55, 2859.27 examples/s]Running tokenizer on dataset (num_proc=64): 292609 examples [00:56, 2209.35 examples/s]Running tokenizer on dataset (num_proc=64): 293526 examples [00:56, 2289.75 examples/s]Running tokenizer on dataset (num_proc=64): 294526 examples [00:57, 2103.86 examples/s]Running tokenizer on dataset (num_proc=64): 296360 examples [00:57, 2468.20 examples/s]Running tokenizer on dataset (num_proc=64): 298194 examples [00:58, 3239.26 examples/s]Running tokenizer on dataset (num_proc=64): 300194 examples [00:58, 4413.16 examples/s]Running tokenizer on dataset (num_proc=64): 301111 examples [00:58, 4644.04 examples/s]Running tokenizer on dataset (num_proc=64): 303028 examples [00:58, 6273.43 examples/s]Running tokenizer on dataset (num_proc=64): 292360 examples [00:55, 2729.01 examples/s]Running tokenizer on dataset (num_proc=64): 293360 examples [00:57, 1559.27 examples/s]Running tokenizer on dataset (num_proc=64): 294360 examples [00:57, 1814.72 examples/s]Running tokenizer on dataset (num_proc=64): 296277 examples [00:57, 2992.81 examples/s]Running tokenizer on dataset (num_proc=64): 298111 examples [00:57, 3947.98 examples/s]Running tokenizer on dataset (num_proc=64): 300111 examples [00:57, 5215.14 examples/s]Running tokenizer on dataset (num_proc=64): 301111 examples [00:58, 4117.97 examples/s]Running tokenizer on dataset (num_proc=64): 302028 examples [00:58, 4461.20 examples/s]Running tokenizer on dataset (num_proc=64): 302945 examples [00:58, 4885.42 examples/s]Running tokenizer on dataset (num_proc=64): 304862 examples [00:58, 6782.02 examples/s]Running tokenizer on dataset (num_proc=64): 305862 examples [00:58, 6450.39 examples/s]Running tokenizer on dataset (num_proc=64): 306862 exampleunning tokenizer on dataset (num_proc=64): 288273 examples [00:56, 2690.90 examples/s]Running tokenizer on dataset (num_proc=64): 289190 examples [00:57, 2526.06 examples/s]Running tokenizer on dataset (num_proc=64): 290190 examples [00:58, 1731.99 examples/s]Running tokenizer on dataset (num_proc=64): 292024 examples [00:58, 2631.14 examples/s]Running tokenizer on dataset (num_proc=64): 293941 examples [00:58, 3422.24 examples/s]Running tokenizer on dataset (num_proc=64): 294858 examples [00:58, 3767.57 examples/s]Running tokenizer on dataset (num_proc=64): 295775 examples [00:59, 3908.30 examples/s]Running tokenizer on dataset (num_proc=64): 296692 examples [00:59, 3932.34 examples/s]Running tokenizer on dataset (num_proc=64): 297692 examples [00:59, 3822.11 examples/s]Running tokenizer on dataset (num_proc=64): 298609 examples [00:59, 3553.60 examples/s]Running tokenizer on dataset (num_proc=64): 300526 examples [01:00, 4243.97 examples/s]Running tokenizer on dataset (num_proc=64): 302360 exampling tokenizer on dataset (num_proc=64): 296277 examples [00:57, 2339.70 examples/s]Running tokenizer on dataset (num_proc=64): 297277 examples [00:57, 2996.06 examples/s]Running tokenizer on dataset (num_proc=64): 300111 examples [00:58, 5536.81 examples/s]Running tokenizer on dataset (num_proc=64): 301111 examples [00:58, 3233.23 examples/s]Running tokenizer on dataset (num_proc=64): 302111 examples [00:59, 3654.77 examples/s]Running tokenizer on dataset (num_proc=64): 304945 examples [00:59, 4704.85 examples/s]Running tokenizer on dataset (num_proc=64): 305862 examples [00:59, 5036.84 examples/s]Running tokenizer on dataset (num_proc=64): 307779 examples [00:59, 5710.11 examples/s]Running tokenizer on dataset (num_proc=64): 308696 examples [00:59, 6144.29 examples/s]Running tokenizer on dataset (num_proc=64): 310530 examples [01:00, 6998.69 examples/s]Running tokenizer on dataset (num_proc=64): 312447 examples [01:00, 7266.85 examples/s]Running tokenizer on dataset (num_proc=64): 313447 examples ning tokenizer on dataset (num_proc=64): 302862 examples [00:57, 5180.75 examples/s]Running tokenizer on dataset (num_proc=64): 303862 examples [00:57, 5122.96 examples/s]Running tokenizer on dataset (num_proc=64): 304862 examples [00:57, 4296.24 examples/s]Running tokenizer on dataset (num_proc=64): 305779 examples [00:57, 4642.28 examples/s]Running tokenizer on dataset (num_proc=64): 306696 examples [00:58, 4133.53 examples/s]Running tokenizer on dataset (num_proc=64): 308530 examples [00:58, 4548.59 examples/s]Running tokenizer on dataset (num_proc=64): 310530 examples [00:59, 4556.11 examples/s]Running tokenizer on dataset (num_proc=64): 312364 examples [00:59, 5182.83 examples/s]Running tokenizer on dataset (num_proc=64): 314198 examples [00:59, 6335.57 examples/s]Running tokenizer on dataset (num_proc=64): 316115 examples [01:00, 3890.65 examples/s]Running tokenizer on dataset (num_proc=64): 317115 examples [01:00, 4191.10 examples/s]Running tokenizer on dataset (num_proc=64): 318949 examplesunning tokenizer on dataset (num_proc=64): 292775 examples [00:56, 5295.75 examples/s]Running tokenizer on dataset (num_proc=64): 293775 examples [00:56, 3816.07 examples/s]Running tokenizer on dataset (num_proc=64): 294775 examples [00:57, 3752.64 examples/s]Running tokenizer on dataset (num_proc=64): 295692 examples [00:57, 3171.40 examples/s]Running tokenizer on dataset (num_proc=64): 297526 examples [00:58, 2732.29 examples/s]Running tokenizer on dataset (num_proc=64): 299443 examples [00:58, 3808.49 examples/s]Running tokenizer on dataset (num_proc=64): 300360 examples [00:59, 2414.19 examples/s]Running tokenizer on dataset (num_proc=64): 302277 examples [00:59, 3537.44 examples/s]Running tokenizer on dataset (num_proc=64): 303194 examples [01:00, 2989.24 examples/s]Running tokenizer on dataset (num_proc=64): 304194 examples [01:00, 2566.02 examples/s]Running tokenizer on dataset (num_proc=64): 305194 examples [01:00, 2688.20 examples/s]Running tokenizer on dataset (num_proc=64): 308862 examplRunning tokenizer on dataset (num_proc=64): 301862 examples [00:58, 3054.15 examples/s]Running tokenizer on dataset (num_proc=64): 302779 examples [00:58, 2374.46 examples/s]Running tokenizer on dataset (num_proc=64): 303779 examples [00:59, 2403.50 examples/s]Running tokenizer on dataset (num_proc=64): 304779 examples [00:59, 2762.82 examples/s]Running tokenizer on dataset (num_proc=64): 305696 examples [00:59, 3107.65 examples/s]Running tokenizer on dataset (num_proc=64): 306613 examples [00:59, 3691.45 examples/s]Running tokenizer on dataset (num_proc=64): 307613 examples [01:00, 3249.42 examples/s]Running tokenizer on dataset (num_proc=64): 309613 examples [01:00, 3497.33 examples/s]Running tokenizer on dataset (num_proc=64): 310613 examples [01:00, 3790.59 examples/s]Running tokenizer on dataset (num_proc=64): 311530 examples [01:00, 4415.94 examples/s]Running tokenizer on dataset (num_proc=64): 312447 examples [01:01, 5029.75 examples/s]Running tokenizer on dataset (num_proc=64): 314364 examnning tokenizer on dataset (num_proc=64): 304028 examples [00:58, 6263.08 examples/s]Running tokenizer on dataset (num_proc=64): 305945 examples [00:59, 6509.67 examples/s]Running tokenizer on dataset (num_proc=64): 306862 examples [00:59, 5153.63 examples/s]Running tokenizer on dataset (num_proc=64): 307779 examples [00:59, 4521.34 examples/s]Running tokenizer on dataset (num_proc=64): 308696 examples [01:00, 3191.11 examples/s]Running tokenizer on dataset (num_proc=64): 310613 examples [01:00, 4873.96 examples/s]Running tokenizer on dataset (num_proc=64): 312530 examples [01:00, 5142.25 examples/s]Running tokenizer on dataset (num_proc=64): 313530 examples [01:01, 4184.92 examples/s]Running tokenizer on dataset (num_proc=64): 314530 examples [01:01, 4706.42 examples/s]Running tokenizer on dataset (num_proc=64): 316364 examples [01:01, 5079.54 examples/s]Running tokenizer on dataset (num_proc=64): 318198 examples [01:02, 2676.11 examples/s]Running tokenizer on dataset (num_proc=64): 319115 exampleRunning tokenizer on dataset (num_proc=64): 294609 examples [00:57, 2141.30 examples/s]Running tokenizer on dataset (num_proc=64): 295526 examples [00:58, 2363.59 examples/s]Running tokenizer on dataset (num_proc=64): 296443 examples [00:58, 2498.79 examples/s]Running tokenizer on dataset (num_proc=64): 297443 examples [00:58, 2392.56 examples/s]Running tokenizer on dataset (num_proc=64): 298360 examples [00:58, 2922.65 examples/s]Running tokenizer on dataset (num_proc=64): 299277 examples [01:00, 1638.69 examples/s]Running tokenizer on dataset (num_proc=64): 301277 examples [01:00, 2110.09 examples/s]Running tokenizer on dataset (num_proc=64): 303194 examples [01:01, 2762.39 examples/s]Running tokenizer on dataset (num_proc=64): 304194 examples [01:01, 3110.33 examples/s]Running tokenizer on dataset (num_proc=64): 307945 examples [01:01, 4167.48 examples/s]Running tokenizer on dataset (num_proc=64): 309779 examples [01:02, 4032.25 examples/s]Running tokenizer on dataset (num_proc=64): 310779 exampes [01:00, 4973.19 examples/s]Running tokenizer on dataset (num_proc=64): 304194 examples [01:00, 5123.49 examples/s]Running tokenizer on dataset (num_proc=64): 305194 examples [01:01, 4297.88 examples/s]Running tokenizer on dataset (num_proc=64): 306194 examples [01:01, 4188.57 examples/s]Running tokenizer on dataset (num_proc=64): 307111 examples [01:01, 4440.28 examples/s]Running tokenizer on dataset (num_proc=64): 308028 examples [01:01, 4442.76 examples/s]Running tokenizer on dataset (num_proc=64): 308945 examples [01:02, 3733.95 examples/s]Running tokenizer on dataset (num_proc=64): 309862 examples [01:02, 4254.49 examples/s]Running tokenizer on dataset (num_proc=64): 310862 examples [01:02, 4704.63 examples/s]Running tokenizer on dataset (num_proc=64): 311779 examples [01:02, 4739.78 examples/s]Running tokenizer on dataset (num_proc=64): 314613 examples [01:02, 7739.16 examples/s]Running tokenizer on dataset (num_proc=64): 315530 examples [01:03, 4642.39 examples/s]Running tokenizer on datas [00:59, 3881.26 examples/s]Running tokenizer on dataset (num_proc=64): 310696 examples [00:59, 7765.68 examples/s]Running tokenizer on dataset (num_proc=64): 312613 examples [00:59, 7170.79 examples/s]Running tokenizer on dataset (num_proc=64): 314530 examples [01:01, 3389.81 examples/s]Running tokenizer on dataset (num_proc=64): 316447 examples [01:01, 4268.86 examples/s]Running tokenizer on dataset (num_proc=64): 317364 examples [01:01, 4151.05 examples/s]Running tokenizer on dataset (num_proc=64): 318364 examples [01:02, 3001.02 examples/s]Running tokenizer on dataset (num_proc=64): 319281 examples [01:03, 2077.88 examples/s]Running tokenizer on dataset (num_proc=64): 321281 examples [01:03, 3125.07 examples/s]Running tokenizer on dataset (num_proc=64): 323115 examples [01:03, 3393.22 examples/s]Running tokenizer on dataset (num_proc=64): 324115 examples [01:04, 3833.83 examples/s]Running tokenizer on dataset (num_proc=64): 325032 examples [01:04, 4043.49 examples/s]Running tokenizer on datas[01:00, 6015.27 examples/s]Running tokenizer on dataset (num_proc=64): 314447 examples [01:00, 6370.39 examples/s]Running tokenizer on dataset (num_proc=64): 316364 examples [01:00, 8495.54 examples/s]Running tokenizer on dataset (num_proc=64): 318198 examples [01:01, 5422.19 examples/s]Running tokenizer on dataset (num_proc=64): 319115 examples [01:01, 4150.38 examples/s]Running tokenizer on dataset (num_proc=64): 320032 examples [01:02, 4547.99 examples/s]Running tokenizer on dataset (num_proc=64): 320949 examples [01:03, 2220.80 examples/s]Running tokenizer on dataset (num_proc=64): 321866 examples [01:03, 2077.94 examples/s]Running tokenizer on dataset (num_proc=64): 322783 examples [01:03, 2486.52 examples/s]Running tokenizer on dataset (num_proc=64): 324617 examples [01:04, 3109.55 examples/s]Running tokenizer on dataset (num_proc=64): 325534 examples [01:04, 3406.94 examples/s]Running tokenizer on dataset (num_proc=64): 326451 examples [01:04, 3483.81 examples/s]Running tokenizer on datasetes [01:01, 5630.10 examples/s]Running tokenizer on dataset (num_proc=64): 310779 examples [01:01, 4395.87 examples/s]Running tokenizer on dataset (num_proc=64): 312613 examples [01:02, 3047.25 examples/s]Running tokenizer on dataset (num_proc=64): 313530 examples [01:02, 3174.42 examples/s]Running tokenizer on dataset (num_proc=64): 315447 examples [01:03, 3105.41 examples/s]Running tokenizer on dataset (num_proc=64): 316364 examples [01:03, 3311.68 examples/s]Running tokenizer on dataset (num_proc=64): 319281 examples [01:04, 5117.04 examples/s]Running tokenizer on dataset (num_proc=64): 320198 examples [01:04, 4135.67 examples/s]Running tokenizer on dataset (num_proc=64): 321115 examples [01:05, 3220.01 examples/s]Running tokenizer on dataset (num_proc=64): 322032 examples [01:05, 3748.58 examples/s]Running tokenizer on dataset (num_proc=64): 323032 examples [01:05, 3178.27 examples/s]Running tokenizer on dataset (num_proc=64): 323949 examples [01:05, 3460.84 examples/s]Running tokenizer on data [01:00, 4659.80 examples/s]Running tokenizer on dataset (num_proc=64): 319866 examples [01:00, 4756.89 examples/s]Running tokenizer on dataset (num_proc=64): 320866 examples [01:01, 3897.38 examples/s]Running tokenizer on dataset (num_proc=64): 321783 examples [01:01, 3539.61 examples/s]Running tokenizer on dataset (num_proc=64): 323700 examples [01:01, 4571.48 examples/s]Running tokenizer on dataset (num_proc=64): 324617 examples [01:02, 4894.01 examples/s]Running tokenizer on dataset (num_proc=64): 325617 examples [01:02, 2680.78 examples/s]Running tokenizer on dataset (num_proc=64): 326534 examples [01:04, 1590.26 examples/s]Running tokenizer on dataset (num_proc=64): 327451 examples [01:04, 1939.53 examples/s]Running tokenizer on dataset (num_proc=64): 328368 examples [01:05, 1828.64 examples/s]Running tokenizer on dataset (num_proc=64): 329285 examples [01:06, 1340.32 examples/s]Running tokenizer on dataset (num_proc=64): 330202 examples [01:06, 1727.29 examples/s]Running tokenizer on dataseples [01:01, 5795.01 examples/s]Running tokenizer on dataset (num_proc=64): 315281 examples [01:01, 5272.23 examples/s]Running tokenizer on dataset (num_proc=64): 317115 examples [01:02, 2951.49 examples/s]Running tokenizer on dataset (num_proc=64): 318949 examples [01:02, 3672.48 examples/s]Running tokenizer on dataset (num_proc=64): 319866 examples [01:02, 4065.30 examples/s]Running tokenizer on dataset (num_proc=64): 320783 examples [01:03, 4285.35 examples/s]Running tokenizer on dataset (num_proc=64): 321700 examples [01:03, 4848.60 examples/s]Running tokenizer on dataset (num_proc=64): 322700 examples [01:03, 4423.00 examples/s]Running tokenizer on dataset (num_proc=64): 323617 examples [01:04, 3345.95 examples/s]Running tokenizer on dataset (num_proc=64): 325451 examples [01:04, 4644.72 examples/s]Running tokenizer on dataset (num_proc=64): 326368 examples [01:05, 2373.03 examples/s]Running tokenizer on dataset (num_proc=64): 327368 examples [01:05, 2629.76 examples/s]Running tokenizer on daset (num_proc=64): 316447 examples [01:03, 4042.10 examples/s]Running tokenizer on dataset (num_proc=64): 317447 examples [01:03, 3795.35 examples/s]Running tokenizer on dataset (num_proc=64): 318364 examples [01:04, 2917.87 examples/s]Running tokenizer on dataset (num_proc=64): 319281 examples [01:04, 3248.83 examples/s]Running tokenizer on dataset (num_proc=64): 320198 examples [01:05, 2398.41 examples/s]Running tokenizer on dataset (num_proc=64): 321115 examples [01:05, 3003.44 examples/s]Running tokenizer on dataset (num_proc=64): 322032 examples [01:05, 2841.88 examples/s]Running tokenizer on dataset (num_proc=64): 324866 examples [01:06, 4677.31 examples/s]Running tokenizer on dataset (num_proc=64): 326700 examples [01:06, 5216.27 examples/s]Running tokenizer on dataset (num_proc=64): 327617 examples [01:06, 4702.07 examples/s]Running tokenizer on dataset (num_proc=64): 328534 examples [01:07, 3844.46 examples/s]Running tokenizer on dataset (num_proc=64): 331368 examples [01:07, 5457.48 examples [01:03, 2909.46 examples/s]Running tokenizer on dataset (num_proc=64): 311696 examples [01:04, 2102.23 examples/s]Running tokenizer on dataset (num_proc=64): 312613 examples [01:04, 2297.44 examples/s]Running tokenizer on dataset (num_proc=64): 314447 examples [01:04, 2815.86 examples/s]Running tokenizer on dataset (num_proc=64): 316364 examples [01:05, 3020.18 examples/s]Running tokenizer on dataset (num_proc=64): 317281 examples [01:05, 2735.29 examples/s]Running tokenizer on dataset (num_proc=64): 318198 examples [01:06, 3166.26 examples/s]Running tokenizer on dataset (num_proc=64): 319115 examples [01:06, 3195.38 examples/s]Running tokenizer on dataset (num_proc=64): 320032 examples [01:06, 2727.31 examples/s]Running tokenizer on dataset (num_proc=64): 321866 examples [01:07, 3461.24 examples/s]Running tokenizer on dataset (num_proc=64): 323700 examples [01:07, 4066.60 examples/s]Running tokenizer on dataset (num_proc=64): 326534 examples [01:07, 5525.65 examples/s]Running tokenizer on datet (num_proc=64): 325949 examples [01:04, 4614.13 examples/s]Running tokenizer on dataset (num_proc=64): 326866 examples [01:04, 5217.82 examples/s]Running tokenizer on dataset (num_proc=64): 327866 examples [01:05, 3203.09 examples/s]Running tokenizer on dataset (num_proc=64): 328866 examples [01:05, 2022.63 examples/s]Running tokenizer on dataset (num_proc=64): 329783 examples [01:06, 1949.05 examples/s]Running tokenizer on dataset (num_proc=64): 331700 examples [01:06, 2600.72 examples/s]Running tokenizer on dataset (num_proc=64): 332617 examples [01:07, 2358.99 examples/s]Running tokenizer on dataset (num_proc=64): 333617 examples [01:07, 2383.34 examples/s]Running tokenizer on dataset (num_proc=64): 334534 examples [01:08, 2175.91 examples/s]Running tokenizer on dataset (num_proc=64): 335451 examples [01:08, 2589.47 examples/s]Running tokenizer on dataset (num_proc=64): 336368 examples [01:08, 3086.97 examples/s]Running tokenizer on dataset (num_proc=64): 337285 examples [01:08, 3617.14 exampls [01:02, 2962.03 examples/s]Running tokenizer on dataset (num_proc=64): 320949 examples [01:03, 3387.14 examples/s]Running tokenizer on dataset (num_proc=64): 322866 examples [01:04, 2719.20 examples/s]Running tokenizer on dataset (num_proc=64): 323783 examples [01:04, 3033.78 examples/s]Running tokenizer on dataset (num_proc=64): 324700 examples [01:04, 3123.97 examples/s]Running tokenizer on dataset (num_proc=64): 325617 examples [01:05, 2861.16 examples/s]Running tokenizer on dataset (num_proc=64): 326534 examples [01:05, 3195.71 examples/s]Running tokenizer on dataset (num_proc=64): 327451 examples [01:05, 2489.23 examples/s]Running tokenizer on dataset (num_proc=64): 328368 examples [01:07, 1518.48 examples/s]Running tokenizer on dataset (num_proc=64): 329285 examples [01:07, 1623.77 examples/s]Running tokenizer on dataset (num_proc=64): 331119 examples [01:07, 2258.72 examples/s]Running tokenizer on dataset (num_proc=64): 332119 examples [01:08, 2580.89 examples/s]Running tokenizer on datastaset (num_proc=64): 328285 examples [01:06, 1636.15 examples/s]Running tokenizer on dataset (num_proc=64): 329202 examples [01:06, 1820.77 examples/s]Running tokenizer on dataset (num_proc=64): 330202 examples [01:07, 2233.02 examples/s]Running tokenizer on dataset (num_proc=64): 331202 examples [01:08, 1783.80 examples/s]Running tokenizer on dataset (num_proc=64): 332202 examples [01:08, 1878.69 examples/s]Running tokenizer on dataset (num_proc=64): 333202 examples [01:08, 2093.88 examples/s]Running tokenizer on dataset (num_proc=64): 334119 examples [01:09, 1950.48 examples/s]Running tokenizer on dataset (num_proc=64): 335119 examples [01:09, 2449.67 examples/s]Running tokenizer on dataset (num_proc=64): 336036 examples [01:09, 2807.99 examples/s]Running tokenizer on dataset (num_proc=64): 336953 examples [01:10, 2483.57 examples/s]Running tokenizer on dataset (num_proc=64): 337870 examples [01:10, 2066.43 examples/s]Running tokenizer on dataset (num_proc=64): 338787 examples [01:11, 2200.75 exales/s]Running tokenizer on dataset (num_proc=64): 333202 examples [01:07, 4282.37 examples/s]Running tokenizer on dataset (num_proc=64): 334202 examples [01:08, 4283.31 examples/s]Running tokenizer on dataset (num_proc=64): 336119 examples [01:08, 5535.93 examples/s]Running tokenizer on dataset (num_proc=64): 337036 examples [01:08, 3745.52 examples/s]Running tokenizer on dataset (num_proc=64): 338953 examples [01:09, 4848.83 examples/s]Running tokenizer on dataset (num_proc=64): 339870 examples [01:09, 5134.02 examples/s]Running tokenizer on dataset (num_proc=64): 340787 examples [01:09, 3615.02 examples/s]Running tokenizer on dataset (num_proc=64): 342787 examples [01:09, 5133.52 examples/s]Running tokenizer on dataset (num_proc=64): 343787 examples [01:10, 4712.42 examples/s]Running tokenizer on dataset (num_proc=64): 344787 examples [01:10, 5297.62 examples/s]Running tokenizer on dataset (num_proc=64): 345704 examples [01:10, 3963.82 examples/s]Running tokenizer on dataset (num_proc=64): 34662aset (num_proc=64): 327534 examples [01:07, 5908.29 examples/s]Running tokenizer on dataset (num_proc=64): 328534 examples [01:08, 5324.49 examples/s]Running tokenizer on dataset (num_proc=64): 329451 examples [01:08, 4576.67 examples/s]Running tokenizer on dataset (num_proc=64): 331285 examples [01:09, 3643.90 examples/s]Running tokenizer on dataset (num_proc=64): 333202 examples [01:09, 3976.94 examples/s]Running tokenizer on dataset (num_proc=64): 334119 examples [01:09, 4232.89 examples/s]Running tokenizer on dataset (num_proc=64): 335036 examples [01:10, 3696.27 examples/s]Running tokenizer on dataset (num_proc=64): 335953 examples [01:11, 1963.15 examples/s]Running tokenizer on dataset (num_proc=64): 336870 examples [01:11, 2436.47 examples/s]Running tokenizer on dataset (num_proc=64): 337787 examples [01:11, 2771.22 examples/s]Running tokenizer on dataset (num_proc=64): 338787 examples [01:11, 2754.55 examples/s]Running tokenizer on dataset (num_proc=64): 339787 examples [01:12, 2972.03 exam (num_proc=64): 328368 examples [01:05, 2233.96 examples/s]Running tokenizer on dataset (num_proc=64): 330285 examples [01:08, 1359.62 examples/s]Running tokenizer on dataset (num_proc=64): 332202 examples [01:08, 1741.12 examples/s]Running tokenizer on dataset (num_proc=64): 333119 examples [01:09, 1888.75 examples/s]Running tokenizer on dataset (num_proc=64): 334036 examples [01:09, 2228.54 examples/s]Running tokenizer on dataset (num_proc=64): 336036 examples [01:09, 2562.61 examples/s]Running tokenizer on dataset (num_proc=64): 337870 examples [01:10, 3049.78 examples/s]Running tokenizer on dataset (num_proc=64): 338787 examples [01:10, 3370.13 examples/s]Running tokenizer on dataset (num_proc=64): 339704 examples [01:10, 2947.21 examples/s]Running tokenizer on dataset (num_proc=64): 340621 examples [01:10, 3505.25 examples/s]Running tokenizer on dataset (num_proc=64): 342538 examples [01:12, 2052.71 examples/s]Running tokenizer on dataset (num_proc=64): 344538 examples [01:12, 2619.62 examplesset (num_proc=64): 325783 examples [01:06, 3487.19 examples/s]Running tokenizer on dataset (num_proc=64): 326700 examples [01:06, 3426.98 examples/s]Running tokenizer on dataset (num_proc=64): 327617 examples [01:06, 3720.06 examples/s]Running tokenizer on dataset (num_proc=64): 328534 examples [01:06, 3695.74 examples/s]Running tokenizer on dataset (num_proc=64): 332202 examples [01:09, 2065.99 examples/s]Running tokenizer on dataset (num_proc=64): 333119 examples [01:09, 2302.71 examples/s]Running tokenizer on dataset (num_proc=64): 335036 examples [01:09, 3123.59 examples/s]Running tokenizer on dataset (num_proc=64): 337036 examples [01:09, 4113.19 examples/s]Running tokenizer on dataset (num_proc=64): 337953 examples [01:10, 2836.03 examples/s]Running tokenizer on dataset (num_proc=64): 338870 examples [01:11, 1956.37 examples/s]Running tokenizer on dataset (num_proc=64): 339787 examples [01:12, 1882.56 examples/s]Running tokenizer on dataset (num_proc=64): 340704 examples [01:13, 1586.24 exampet (num_proc=64): 333036 examples [01:09, 1945.58 examples/s]Running tokenizer on dataset (num_proc=64): 334036 examples [01:09, 2346.57 examples/s]Running tokenizer on dataset (num_proc=64): 335953 examples [01:09, 3181.71 examples/s]Running tokenizer on dataset (num_proc=64): 336870 examples [01:09, 3170.58 examples/s]Running tokenizer on dataset (num_proc=64): 338787 examples [01:10, 4557.53 examples/s]Running tokenizer on dataset (num_proc=64): 339704 examples [01:10, 3170.44 examples/s]Running tokenizer on dataset (num_proc=64): 340621 examples [01:10, 3101.27 examples/s]Running tokenizer on dataset (num_proc=64): 341538 examples [01:11, 2692.24 examples/s]Running tokenizer on dataset (num_proc=64): 342538 examples [01:11, 2537.06 examples/s]Running tokenizer on dataset (num_proc=64): 343538 examples [01:12, 2674.29 examples/s]Running tokenizer on dataset (num_proc=64): 344538 examples [01:12, 2664.90 examples/s]Running tokenizer on dataset (num_proc=64): 345538 examples [01:13, 2405.18 examplt (num_proc=64): 331119 examples [01:06, 2005.18 examples/s]Running tokenizer on dataset (num_proc=64): 332036 examples [01:08, 1203.95 examples/s]Running tokenizer on dataset (num_proc=64): 334953 examples [01:08, 2377.31 examples/s]Running tokenizer on dataset (num_proc=64): 335953 examples [01:08, 2410.25 examples/s]Running tokenizer on dataset (num_proc=64): 336870 examples [01:09, 2271.09 examples/s]Running tokenizer on dataset (num_proc=64): 337787 examples [01:09, 2172.14 examples/s]Running tokenizer on dataset (num_proc=64): 338787 examples [01:10, 2121.58 examples/s]Running tokenizer on dataset (num_proc=64): 339704 examples [01:10, 2129.36 examples/s]Running tokenizer on dataset (num_proc=64): 340621 examples [01:11, 1491.50 examples/s]Running tokenizer on dataset (num_proc=64): 342538 examples [01:12, 2061.81 examples/s]Running tokenizer on dataset (num_proc=64): 343538 examples [01:13, 1857.46 examples/s]Running tokenizer on dataset (num_proc=64): 344538 examples [01:13, 1744.82 examplees/s]Running tokenizer on dataset (num_proc=64): 338202 examples [01:09, 3197.49 examples/s]Running tokenizer on dataset (num_proc=64): 339119 examples [01:09, 3321.93 examples/s]Running tokenizer on dataset (num_proc=64): 340036 examples [01:10, 2227.14 examples/s]Running tokenizer on dataset (num_proc=64): 340953 examples [01:10, 2565.03 examples/s]Running tokenizer on dataset (num_proc=64): 342870 examples [01:10, 3875.22 examples/s]Running tokenizer on dataset (num_proc=64): 344704 examples [01:11, 3656.81 examples/s]Running tokenizer on dataset (num_proc=64): 345704 examples [01:11, 3554.63 examples/s]Running tokenizer on dataset (num_proc=64): 346621 examples [01:13, 1536.35 examples/s]Running tokenizer on dataset (num_proc=64): 347538 examples [01:14, 1235.34 examples/s]Running tokenizer on dataset (num_proc=64): 348538 examples [01:16, 874.55 examples/s] Running tokenizer on dataset (num_proc=64): 349538 examples [01:21, 440.16 examples/s]Running tokenizer on dataset (num_proc=64): 350538 mples/s]Running tokenizer on dataset (num_proc=64): 339704 examples [01:11, 2714.19 examples/s]Running tokenizer on dataset (num_proc=64): 340704 examples [01:11, 3365.30 examples/s]Running tokenizer on dataset (num_proc=64): 341621 examples [01:11, 3280.07 examples/s]Running tokenizer on dataset (num_proc=64): 343621 examples [01:12, 2359.48 examples/s]Running tokenizer on dataset (num_proc=64): 344538 examples [01:13, 2155.35 examples/s]Running tokenizer on dataset (num_proc=64): 345538 examples [01:15, 1123.10 examples/s]Running tokenizer on dataset (num_proc=64): 346538 examples [01:15, 1395.31 examples/s]Running tokenizer on dataset (num_proc=64): 347538 examples [01:20, 565.69 examples/s] Running tokenizer on dataset (num_proc=64): 348538 examples [01:25, 370.99 examples/s]Running tokenizer on dataset (num_proc=64): 349455 examples [01:26, 424.21 examples/s]Running tokenizer on dataset (num_proc=64): 350455 examples [01:31, 309.40 examples/s]Running tokenizer on dataset (num_proc=64): 351455les/s]Running tokenizer on dataset (num_proc=64): 341704 examples [01:13, 1505.62 examples/s]Running tokenizer on dataset (num_proc=64): 343538 examples [01:14, 1899.57 examples/s]Running tokenizer on dataset (num_proc=64): 344538 examples [01:15, 1424.89 examples/s]Running tokenizer on dataset (num_proc=64): 345538 examples [01:16, 1180.04 examples/s]Running tokenizer on dataset (num_proc=64): 346538 examples [01:17, 1235.03 examples/s]Running tokenizer on dataset (num_proc=64): 347538 examples [01:20, 780.00 examples/s] Running tokenizer on dataset (num_proc=64): 348455 examples [01:21, 800.97 examples/s]Running tokenizer on dataset (num_proc=64): 349455 examples [01:22, 758.57 examples/s]Running tokenizer on dataset (num_proc=64): 350455 examples [01:30, 305.42 examples/s]Running tokenizer on dataset (num_proc=64): 351455 examples [01:32, 362.10 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:32, 492.85 examples/s]Running tokenizer on dataset (num_proc=64): 353455 exaples/s]Running tokenizer on dataset (num_proc=64): 340704 examples [01:12, 2435.31 examples/s]Running tokenizer on dataset (num_proc=64): 341621 examples [01:14, 994.40 examples/s] Running tokenizer on dataset (num_proc=64): 342538 examples [01:15, 1045.62 examples/s]Running tokenizer on dataset (num_proc=64): 343538 examples [01:16, 1111.80 examples/s]Running tokenizer on dataset (num_proc=64): 344538 examples [01:17, 968.64 examples/s] Running tokenizer on dataset (num_proc=64): 345538 examples [01:19, 872.63 examples/s]Running tokenizer on dataset (num_proc=64): 346538 examples [01:19, 988.30 examples/s]Running tokenizer on dataset (num_proc=64): 347538 examples [01:21, 817.79 examples/s]Running tokenizer on dataset (num_proc=64): 348455 examples [01:24, 614.43 examples/s]Running tokenizer on dataset (num_proc=64): 349455 examples [01:27, 433.53 examples/s]Running tokenizer on dataset (num_proc=64): 350455 examples [01:35, 265.39 examples/s]Running tokenizer on dataset (num_proc=64): 351455 exa1 examples [01:11, 2243.12 examples/s]Running tokenizer on dataset (num_proc=64): 347538 examples [01:11, 2768.33 examples/s]Running tokenizer on dataset (num_proc=64): 348538 examples [01:16, 560.11 examples/s] Running tokenizer on dataset (num_proc=64): 349455 examples [01:21, 393.79 examples/s]Running tokenizer on dataset (num_proc=64): 350455 examples [01:26, 300.12 examples/s]Running tokenizer on dataset (num_proc=64): 351455 examples [01:27, 383.63 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:27, 497.55 examples/s]Running tokenizer on dataset (num_proc=64): 353455 examples [01:31, 389.17 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:31, 546.37 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:33, 517.95 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:36, 469.67 examples/s]Running tokenizer on dataset (num_proc=64): 357455 examples [01:37, 611.25 examples/s]Running tokenizer on datass/s]Running tokenizer on dataset (num_proc=64): 345538 examples [01:14, 1643.55 examples/s]Running tokenizer on dataset (num_proc=64): 346538 examples [01:15, 1268.47 examples/s]Running tokenizer on dataset (num_proc=64): 347538 examples [01:18, 726.55 examples/s] Running tokenizer on dataset (num_proc=64): 348538 examples [01:25, 323.59 examples/s]Running tokenizer on dataset (num_proc=64): 349538 examples [01:27, 383.19 examples/s]Running tokenizer on dataset (num_proc=64): 350455 examples [01:29, 387.05 examples/s]Running tokenizer on dataset (num_proc=64): 351455 examples [01:29, 512.85 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:31, 568.82 examples/s]Running tokenizer on dataset (num_proc=64): 353455 examples [01:37, 336.74 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:38, 378.70 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:40, 446.71 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples/s]Running tokenizer on dataset (num_proc=64): 345538 examples [01:13, 2181.50 examples/s]Running tokenizer on dataset (num_proc=64): 346538 examples [01:14, 1646.43 examples/s]Running tokenizer on dataset (num_proc=64): 347538 examples [01:16, 1217.36 examples/s]Running tokenizer on dataset (num_proc=64): 348455 examples [01:17, 1089.84 examples/s]Running tokenizer on dataset (num_proc=64): 349455 examples [01:26, 307.27 examples/s] Running tokenizer on dataset (num_proc=64): 350455 examples [01:30, 297.68 examples/s]Running tokenizer on dataset (num_proc=64): 351455 examples [01:32, 314.04 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:34, 368.88 examples/s]Running tokenizer on dataset (num_proc=64): 353455 examples [01:37, 366.94 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:39, 379.40 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:40, 454.81 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examplees/s]Running tokenizer on dataset (num_proc=64): 346538 examples [01:16, 788.91 examples/s] Running tokenizer on dataset (num_proc=64): 347538 examples [01:17, 900.27 examples/s]Running tokenizer on dataset (num_proc=64): 348538 examples [01:23, 368.06 examples/s]Running tokenizer on dataset (num_proc=64): 349455 examples [01:28, 278.36 examples/s]Running tokenizer on dataset (num_proc=64): 350455 examples [01:30, 341.07 examples/s]Running tokenizer on dataset (num_proc=64): 351455 examples [01:30, 460.39 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:34, 360.38 examples/s]Running tokenizer on dataset (num_proc=64): 353455 examples [01:35, 488.92 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:37, 516.65 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:40, 439.68 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:41, 476.96 examples/s]Running tokenizer on dataset (num_proc=64): 357455 examples examples [01:23, 456.29 examples/s]Running tokenizer on dataset (num_proc=64): 351538 examples [01:24, 573.32 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:24, 772.59 examples/s]Running tokenizer on dataset (num_proc=64): 353455 examples [01:29, 383.24 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:31, 435.70 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:32, 562.80 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:34, 521.11 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:35, 749.05 examples/s]Running tokenizer on dataset (num_proc=64): 359455 examples [01:35, 934.69 examples/s]Running tokenizer on dataset (num_proc=64): 360455 examples [01:38, 730.96 examples/s]Running tokenizer on dataset (num_proc=64): 361372 examples [01:38, 906.10 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:39, 931.60 examples/s]Running tokenizer on dataset (n examples [01:34, 319.69 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:36, 373.25 examples/s]Running tokenizer on dataset (num_proc=64): 353455 examples [01:38, 401.69 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:40, 530.31 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:41, 660.81 examples/s]Running tokenizer on dataset (num_proc=64): 357455 examples [01:41, 773.92 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:43, 704.94 examples/s]Running tokenizer on dataset (num_proc=64): 359455 examples [01:45, 674.71 examples/s]Running tokenizer on dataset (num_proc=64): 360455 examples [01:46, 655.33 examples/s]Running tokenizer on dataset (num_proc=64): 361372 examples [01:47, 682.18 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:51, 470.88 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:57, 312.78 examples/s]Running tokenizer on dataset (mples [01:35, 357.74 examples/s]Running tokenizer on dataset (num_proc=64): 352455 examples [01:36, 443.78 examples/s]Running tokenizer on dataset (num_proc=64): 353455 examples [01:37, 509.15 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:39, 570.60 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:40, 646.32 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:41, 654.51 examples/s]Running tokenizer on dataset (num_proc=64): 357455 examples [01:45, 473.32 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:48, 385.77 examples/s]Running tokenizer on dataset (num_proc=64): 359455 examples [01:50, 433.48 examples/s]Running tokenizer on dataset (num_proc=64): 360372 examples [01:50, 570.59 examples/s]Running tokenizer on dataset (num_proc=64): 361372 examples [01:52, 578.92 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:53, 673.04 examples/s]Running tokenizer on dataset (num_et (num_proc=64): 358372 examples [01:37, 753.95 examples/s]Running tokenizer on dataset (num_proc=64): 359372 examples [01:38, 891.07 examples/s]Running tokenizer on dataset (num_proc=64): 360372 examples [01:39, 965.41 examples/s]Running tokenizer on dataset (num_proc=64): 361372 examples [01:39, 1258.55 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:40, 1270.23 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:46, 369.91 examples/s] Running tokenizer on dataset (num_proc=64): 364206 examples [01:49, 359.38 examples/s]Running tokenizer on dataset (num_proc=64): 365123 examples [01:51, 391.34 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [01:51, 537.05 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [01:58, 279.30 examples/s]Running tokenizer on dataset (num_proc=64): 367874 examples [02:02, 275.52 examples/s]Running tokenizer on dataset (num_proc=64): 368791 examples [02:02, 381.99 examples/s]Runmples [01:35, 424.61 examples/s]Running tokenizer on dataset (num_proc=64): 354455 examples [01:38, 392.34 examples/s]Running tokenizer on dataset (num_proc=64): 355455 examples [01:38, 548.03 examples/s]Running tokenizer on dataset (num_proc=64): 356455 examples [01:40, 539.67 examples/s]Running tokenizer on dataset (num_proc=64): 357455 examples [01:43, 483.42 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:43, 642.54 examples/s]Running tokenizer on dataset (num_proc=64): 359372 examples [01:45, 606.96 examples/s]Running tokenizer on dataset (num_proc=64): 360372 examples [01:48, 473.76 examples/s]Running tokenizer on dataset (num_proc=64): 361372 examples [01:48, 666.03 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:50, 670.92 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:54, 429.43 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [01:59, 298.75 examples/s]Running tokenizer on dataset (num_ [01:40, 569.26 examples/s]Running tokenizer on dataset (num_proc=64): 357455 examples [01:41, 704.86 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:44, 549.64 examples/s]Running tokenizer on dataset (num_proc=64): 359455 examples [01:46, 490.40 examples/s]Running tokenizer on dataset (num_proc=64): 360455 examples [01:47, 650.95 examples/s]Running tokenizer on dataset (num_proc=64): 361455 examples [01:48, 734.06 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:49, 693.54 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:49, 895.31 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [02:01, 230.22 examples/s]Running tokenizer on dataset (num_proc=64): 365123 examples [02:01, 320.41 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [02:03, 338.38 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [02:06, 349.57 examples/s]Running tokenizer on dataset (num_proc=ning tokenizer on dataset (num_proc=64): 369708 examples [02:03, 478.17 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:03, 634.82 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:04, 740.00 examples/s]Running tokenizer on dataset (num_proc=64): 372459 examples [02:05, 751.03 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:06, 779.53 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:07, 1466.58 examples/s]
um_proc=64): 363289 examples [01:44, 411.28 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [01:47, 414.33 examples/s]Running tokenizer on dataset (num_proc=64): 365123 examples [01:52, 298.19 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [01:54, 334.10 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [01:57, 329.19 examples/s]Running tokenizer on dataset (num_proc=64): 367874 examples [01:58, 378.56 examples/s]Running tokenizer on dataset (num_proc=64): 368791 examples [01:58, 526.52 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [01:59, 587.53 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:02, 477.49 examples/s]Running tokenizer on dataset (num_proc=64): 372459 examples [02:03, 747.64 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:07, 470.49 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:08, 1452.68 examples/s]
[01:41, 665.86 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:43, 629.20 examples/s]Running tokenizer on dataset (num_proc=64): 359455 examples [01:45, 556.50 examples/s]Running tokenizer on dataset (num_proc=64): 360372 examples [01:46, 672.74 examples/s]Running tokenizer on dataset (num_proc=64): 361372 examples [01:47, 710.63 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:50, 601.34 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:58, 255.28 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [01:59, 341.98 examples/s]Running tokenizer on dataset (num_proc=64): 365123 examples [02:01, 380.33 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [02:02, 453.50 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [02:05, 400.71 examples/s]Running tokenizer on dataset (num_proc=64): 367874 examples [02:08, 340.19 examples/s]Running tokenizer on dataset (num_proc=6s [01:41, 561.53 examples/s]Running tokenizer on dataset (num_proc=64): 357455 examples [01:42, 718.63 examples/s]Running tokenizer on dataset (num_proc=64): 358455 examples [01:44, 587.48 examples/s]Running tokenizer on dataset (num_proc=64): 359455 examples [01:45, 736.11 examples/s]Running tokenizer on dataset (num_proc=64): 360455 examples [01:45, 927.11 examples/s]Running tokenizer on dataset (num_proc=64): 361455 examples [01:47, 813.98 examples/s]Running tokenizer on dataset (num_proc=64): 362372 examples [01:50, 543.53 examples/s]Running tokenizer on dataset (num_proc=64): 363289 examples [01:52, 461.74 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [02:01, 227.81 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [02:04, 322.60 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [02:06, 352.42 examples/s]Running tokenizer on dataset (num_proc=64): 367874 examples [02:08, 398.77 examples/s]Running tokenizer on dataset (num_proc64): 367874 examples [02:07, 427.22 examples/s]Running tokenizer on dataset (num_proc=64): 368791 examples [02:07, 586.03 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [02:09, 492.62 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:12, 450.79 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:13, 550.89 examples/s]Running tokenizer on dataset (num_proc=64): 372459 examples [02:14, 589.23 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:15, 683.17 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:16, 1372.62 examples/s]
=64): 368791 examples [02:09, 460.25 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [02:11, 444.19 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:12, 537.58 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:12, 641.79 examples/s]Running tokenizer on dataset (num_proc=64): 372459 examples [02:13, 830.82 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:15, 611.61 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:16, 1367.15 examples/s]
4): 368791 examples [02:08, 468.36 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [02:10, 537.05 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:11, 538.94 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:14, 489.63 examples/s]Running tokenizer on dataset (num_proc=64): 372459 examples [02:14, 659.92 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:17, 507.47 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:17, 1353.95 examples/s]
num_proc=64): 364206 examples [01:58, 368.89 examples/s]Running tokenizer on dataset (num_proc=64): 365123 examples [02:04, 262.84 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [02:07, 264.43 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [02:08, 340.24 examples/s]Running tokenizer on dataset (num_proc=64): 367874 examples [02:09, 413.38 examples/s]Running tokenizer on dataset (num_proc=64): 368791 examples [02:09, 567.77 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [02:10, 656.61 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:11, 852.54 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:13, 611.58 examples/s]Running tokenizer on dataset (num_proc=64): 372459 examples [02:14, 762.46 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:17, 530.31 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:17, 1353.45 examples/s]
proc=64): 365123 examples [02:03, 272.85 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [02:05, 320.68 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [02:06, 372.10 examples/s]Running tokenizer on dataset (num_proc=64): 368791 examples [02:07, 611.63 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [02:08, 639.09 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:11, 478.66 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:14, 444.16 examples/s]Running tokenizer on dataset (num_proc=64): 372459 examples [02:15, 504.67 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:17, 520.97 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:18, 1352.22 examples/s]
proc=64): 363289 examples [02:00, 320.25 examples/s]Running tokenizer on dataset (num_proc=64): 364206 examples [02:00, 443.09 examples/s]Running tokenizer on dataset (num_proc=64): 365123 examples [02:02, 402.88 examples/s]Running tokenizer on dataset (num_proc=64): 366040 examples [02:05, 373.43 examples/s]Running tokenizer on dataset (num_proc=64): 366957 examples [02:09, 341.34 examples/s]Running tokenizer on dataset (num_proc=64): 367874 examples [02:11, 354.18 examples/s]Running tokenizer on dataset (num_proc=64): 368791 examples [02:11, 475.51 examples/s]Running tokenizer on dataset (num_proc=64): 369708 examples [02:12, 565.24 examples/s]Running tokenizer on dataset (num_proc=64): 370625 examples [02:16, 403.94 examples/s]Running tokenizer on dataset (num_proc=64): 371542 examples [02:18, 446.36 examples/s]Running tokenizer on dataset (num_proc=64): 372459 examples [02:19, 525.05 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:19, 675.55 examples/s]Running tokenizer on dataset (num_proc=64): 373376 examples [02:20, 1329.85 examples/s]
[INFO|configuration_utils.py:763] 2025-10-03 12:34:16,980 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 12:34:16,979 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 12:34:16,980 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 12:34:16,979 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 12:34:16,980 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 12:34:16,982 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|configuration_utils.py:763] 2025-10-03 12:34:16,980 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-03 12:34:16,982 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-03 12:34:16,982 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-03 12:34:16,983 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|configuration_utils.py:839] 2025-10-03 12:34:16,983 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-03 12:34:16,983 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-03 12:34:16,984 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 12:34:16,987 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-03 12:34:16,988 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 12:34:16,991 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|modeling_utils.py:1277] 2025-10-03 12:34:17,664 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-03 12:34:17,666 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 12:34:17,666 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-03 12:34:17,666 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-03 12:34:17,666 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 12:34:17,667 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-03 12:34:17,669 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-03 12:34:17,669 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 12:34:17,669 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-03 12:34:17,669 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-03 12:34:17,671 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 12:34:17,672 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-03 12:34:17,683 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 12:34:17,684 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-03 12:34:17,684 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 12:34:17,685 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-03 12:34:17,687 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 12:34:17,691 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 12:34:17,693 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 12:34:17,693 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 12:34:17,695 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 12:34:17,696 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 12:34:17,709 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 12:34:17,719 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.10s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.10s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:57,  7.21s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.86s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.86s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:48,  6.89s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.10s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.10s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.86s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.86s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.09s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:47,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.86s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.86s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]L41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]L41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]L41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]L41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]L41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]L41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]L41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:20<00:41,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:27<00:34,  6.85s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:34<00:27,  6.84s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:41<00:20,  6.78s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 12:35:16,966 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 12:35:16,966 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
[INFO|configuration_utils.py:1008] 2025-10-03 12:35:16,968 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 12:35:16,968 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██oading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 12:35:16,971 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 12:35:16,971 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 12:35:16,972 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 12:35:16,972 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██oading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]

Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██[INFO|modeling_utils.py:5724] 2025-10-03 12:35:16,972 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5724] 2025-10-03 12:35:16,972 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 12:35:16,973 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]


[INFO|modeling_utils.py:5732] 2025-10-03 12:35:16,973 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
oading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:40<00:20,  6.78s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:47<00:13,  6.64s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:54<00:06,  6.69s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 12:35:16,973 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 12:35:16,973 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.66s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]

Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
[INFO|configuration_utils.py:1008] 2025-10-03 12:35:16,973 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|modeling_utils.py:5724] 2025-10-03 12:35:16,973 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 12:35:16,973 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1055] 2025-10-03 12:35:16,973 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-03 12:35:16,973 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 12:35:16,974 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-03 12:35:16,974 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1008] 2025-10-03 12:35:16,974 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1008] 2025-10-03 12:35:16,974 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 12:35:16,974 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1055] 2025-10-03 12:35:16,974 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1055] 2025-10-03 12:35:16,974 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-03 12:35:16,974 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 12:35:16,975 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|trainer.py:757] 2025-10-03 12:35:16,991 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 12:35:16,992 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 12:35:16,997 >> Using auto half precision backend
[INFO|trainer.py:757] 2025-10-03 12:35:16,997 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 12:35:16,998 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 12:35:16,998 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  5.65s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:57<00:00,  6.39s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 12:35:16,999 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 12:35:16,999 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|trainer.py:757] 2025-10-03 12:35:17,000 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 12:35:17,000 >> Using auto half precision backend
[INFO|configuration_utils.py:1008] 2025-10-03 12:35:17,000 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|configuration_utils.py:1055] 2025-10-03 12:35:17,000 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[WARNING|trainer.py:985] 2025-10-03 12:35:17,000 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 12:35:17,001 >> Using auto half precision backend
[WARNING|trainer.py:985] 2025-10-03 12:35:17,001 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 12:35:17,001 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 12:35:17,002 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 12:35:17,002 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 12:35:17,018 >> Using auto half precision backend
[WARNING|trainer.py:985] 2025-10-03 12:35:17,023 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 12:35:17,483 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 12:35:17,489 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[INFO|deepspeed.py:380] 2025-10-03 12:35:17,490 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[INFO|deepspeed.py:380] 2025-10-03 12:35:17,490 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 12:35:17,495 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 12:35:17,522 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 4. Using DeepSpeed's value.
[INFO|deepspeed.py:380] 2025-10-03 12:35:17,523 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 12:35:17,560 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[rank7]:W1003 12:35:22.813000 2414723 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank7]:W1003 12:35:22.813000 2414723 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank6]:W1003 12:35:22.857000 2414722 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank6]:W1003 12:35:22.857000 2414722 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank15]:W1003 12:35:22.874000 3247069 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank15]:W1003 12:35:22.874000 3247069 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank1]:W1003 12:35:22.896000 1615204 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank1]:W1003 12:35:22.896000 1615204 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank16]:W1003 12:35:23.194000 1612124 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank16]:W1003 12:35:23.194000 1612124 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank8]:W1003 12:35:23.265000 2348487 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank8]:W1003 12:35:23.265000 2348487 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[INFO|trainer.py:2523] 2025-10-03 12:35:31,439 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-03 12:35:31,439 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-03 12:35:31,439 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 12:35:31,439 >>   Num examples = 184,821
[INFO|trainer.py:2524] 2025-10-03 12:35:31,439 >>   Num examples = 184,821
[INFO|trainer.py:2524] 2025-10-03 12:35:31,439 >>   Num examples = 184,821
[INFO|trainer.py:2525] 2025-10-03 12:35:31,439 >>   Num Epochs = 1
[INFO|trainer.py:2525] 2025-10-03 12:35:31,439 >>   Num Epochs = 1
[INFO|trainer.py:2525] 2025-10-03 12:35:31,439 >>   Num Epochs = 1
[INFO|trainer.py:2523] 2025-10-03 12:35:31,439 >> ***** Running training *****
[INFO|trainer.py:2526] 2025-10-03 12:35:31,439 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2526] 2025-10-03 12:35:31,439 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2524] 2025-10-03 12:35:31,439 >>   Num examples = 184,821
[INFO|trainer.py:2526] 2025-10-03 12:35:31,439 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 12:35:31,439 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2529] 2025-10-03 12:35:31,439 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2525] 2025-10-03 12:35:31,439 >>   Num Epochs = 1
[INFO|trainer.py:2529] 2025-10-03 12:35:31,439 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-03 12:35:31,439 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2530] 2025-10-03 12:35:31,439 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2530] 2025-10-03 12:35:31,439 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-03 12:35:31,439 >>   Total optimization steps = 1,444
[INFO|trainer.py:2531] 2025-10-03 12:35:31,439 >>   Total optimization steps = 1,444
[INFO|trainer.py:2531] 2025-10-03 12:35:31,439 >>   Total optimization steps = 1,444
[INFO|trainer.py:2523] 2025-10-03 12:35:31,439 >> ***** Running training *****
[INFO|trainer.py:2523] 2025-10-03 12:35:31,439 >> ***** Running training *****
[INFO|trainer.py:2526] 2025-10-03 12:35:31,439 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2524] 2025-10-03 12:35:31,439 >>   Num examples = 184,821
[INFO|trainer.py:2524] 2025-10-03 12:35:31,439 >>   Num examples = 184,821
[INFO|trainer.py:2529] 2025-10-03 12:35:31,439 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2525] 2025-10-03 12:35:31,439 >>   Num Epochs = 1
[INFO|trainer.py:2525] 2025-10-03 12:35:31,439 >>   Num Epochs = 1
[INFO|trainer.py:2530] 2025-10-03 12:35:31,439 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2526] 2025-10-03 12:35:31,439 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2526] 2025-10-03 12:35:31,439 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2531] 2025-10-03 12:35:31,439 >>   Total optimization steps = 1,444
[INFO|trainer.py:2529] 2025-10-03 12:35:31,439 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2529] 2025-10-03 12:35:31,439 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-03 12:35:31,439 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2530] 2025-10-03 12:35:31,439 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-03 12:35:31,439 >>   Total optimization steps = 1,444
[INFO|trainer.py:2531] 2025-10-03 12:35:31,439 >>   Total optimization steps = 1,444
[INFO|trainer.py:2532] 2025-10-03 12:35:31,440 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2532] 2025-10-03 12:35:31,440 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2532] 2025-10-03 12:35:31,440 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2532] 2025-10-03 12:35:31,440 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2532] 2025-10-03 12:35:31,440 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2532] 2025-10-03 12:35:31,440 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2523] 2025-10-03 12:35:31,440 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 12:35:31,440 >>   Num examples = 184,821
[INFO|trainer.py:2525] 2025-10-03 12:35:31,440 >>   Num Epochs = 1
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2526] 2025-10-03 12:35:31,440 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 12:35:31,440 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-03 12:35:31,440 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-03 12:35:31,440 >>   Total optimization steps = 1,444
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2532] 2025-10-03 12:35:31,441 >>   Number of trainable parameters = 20,914,757,184
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2523] 2025-10-03 12:35:31,668 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 12:35:31,668 >>   Num examples = 184,821
[INFO|trainer.py:2525] 2025-10-03 12:35:31,668 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 12:35:31,668 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 12:35:31,668 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2530] 2025-10-03 12:35:31,668 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2531] 2025-10-03 12:35:31,668 >>   Total optimization steps = 1,444
[INFO|trainer.py:2532] 2025-10-03 12:35:31,669 >>   Number of trainable parameters = 20,914,757,184
  0%|          | 0/1444 [00:00<?, ?it/s]  0%|          | 1/1444 [00:50<20:25:37, 50.96s/it]  0%|          | 2/1444 [01:22<15:48:04, 39.45s/it]  0%|          | 3/1444 [01:55<14:43:36, 36.79s/it]  0%|          | 4/1444 [02:26<13:47:04, 34.46s/it]  0%|          | 5/1444 [02:58<13:22:50, 33.48s/it]  0%|          | 6/1444 [03:31<13:19:35, 33.36s/it]  0%|          | 7/1444 [04:07<13:38:59, 34.20s/it]  1%|          | 8/1444 [04:48<14:32:45, 36.47s/it]  1%|          | 9/1444 [05:21<14:04:26, 35.31s/it]  1%|          | 10/1444 [05:56<13:58:13, 35.07s/it]                                                      1%|          | 10/1444 [05:56<13:58:13, 35.07s/it]  1%|          | 11/1444 [06:27<13:30:21, 33.93s/it]  1%|          | 12/1444 [07:02<13:36:36, 34.22s/it]  1%|          | 13/1444 [07:41<14:08:11, 35.56s/it]  1%|          | 14/1444 [08:11<13:32:27, 34.09s/it]  1%|          | 15/1444 [08:43<13:17:31, 33.49s/it]  1%|          | 16/1444 [09:18<13:26:55, 33.90s/it]  1%|          | 17/1444 [09:55<13:44:04, 34.65s/it]  1%|          | 18/1444 [10:25<13:12:58, 33.36s/it]  1%|▏         | 19/1444 [10:56<12:55:30, 32.65s/it]  1%|▏         | 20/1444 [11:30<13:07:21, 33.18s/it]                                                      1%|▏         | 20/1444 [11:30<13:07:21, 33.18s/it]  1%|▏         | 21/1444 [12:03<13:00:46, 32.92s/it]  2%|▏         | 22/1444 [12:38<13:15:56, 33.58s/it]  2%|▏         | 23/1444 [13:10<13:02:04, 33.02s/it]  2%|▏         | 24/1444 [13:41<12:46:36, 32.39s/it]  2%|▏         | 25/1444 [14:12<12:36:38, 31.99s/it]  2%|▏         | 26/1444 [14:44<12:36:15, 32.00s/it]  2%|▏         | 27/1444 [15:19<12:58:14, 32.95s/it]  2%|▏         | 28/1444 [15:56<13:24:59, 34.11s/it]  2%|▏         | 29/1444 [16:38<14:24:40, 36.66s/it]  2%|▏         | 30/1444 [17:11<13:57:46, 35.55s/it]                                                      2%|▏         | 30/1444 [17:11<13:57:46, 35.55s/it]  2%|▏         | 31/1444 [17:42<13:22:59, 34.10s/it]  2%|▏         | 32/1444 [18:19<13:43:08, 34.98s/it]  2%|▏         | 33/1444 [18:53<13:39:25, 34.84s/it]  2%|▏         | 34/1444 [19:28<13:39:54, 34.89s/it]  2%|▏         | 35/1444 [20:04<13:42:16, 35.01s/it]  2%|▏         | 36/1444 [20:41<13:56:56, 35.66s/it]  3%|▎         | 37/1444 [21:19<14:11:55, 36.33s/it]  3%|▎         | 38/1444 [21:55<14:10:43, 36.30s/it]  3%|▎         | 39/1444 [22:29<13:50:23, 35.46s/it]  3%|▎         | 40/1444 [23:03<13:40:34, 35.07s/it]                                                      3%|▎         | 40/1444 [23:03<13:40:34, 35.07s/it]  3%|▎         | 41/1444 [23:37<13:34:53, 34.85s/it]  3%|▎         | 42/1444 [24:15<13:56:01, 35.78s/it]  3%|▎         | 43/1444 [24:47<13:26:59, 34.56s/it]  3%|▎         | 44/1444 [25:20<13:17:24, 34.17s/it]  3%|▎         | 45/1444 [25:55<13:22:57, 34.44s/it]  3%|▎         | 46/1444 [26:28<13:10:09, 33.91s/it]  3%|▎         | 47/1444 [27:00<12:58:20, 33.43s/it]  3%|▎         | 48/1444 [27:35<13:08:30, 33.89s/it]  3%|▎         | 49/1444 [28:08<13:03:44, 33.71s/it]  3%|▎         | 50/1444 [28:42<13:02:07, 33.66s/it]                                                      3%|▎         | 50/1444 [28:42<13:02:07, 33.66s/it]  4%|▎         | 51/1444 [29:15<12:58:08, 33.52s/it]  4%|▎         | 52/1444 [29:54<13:32:55, 35.04s/it]  4%|▎         | 53/1444 [30:32<13:53:30, 35.95s/it]  4%|▎         | 54/1444 [31:06<13:43:31, 35.55s/it]  4%|▍         | 55/1444 [31:37<13:09:36, 34.11s/it]  4%|▍         | 56/1444 [32:15<13:35:34, 35.26s/it]  4%|▍         | 57/1444 [32:47<13:09:51, 34.17s/it]  4%|▍         | 58/1444 [33:26<13:43:58, 35.67s/it]  4%|▍         | 59/1444 [33:59<13:25:35, 34.90s/it]  4%|▍         | 60/1444 [34:33<13:20:34, 34.71s/it]                                                      4%|▍         | 60/1444 [34:33<13:20:34, 34.71s/it]  4%|▍         | 61/1444 [35:05<13:02:10, 33.93s/it]  4%|▍         | 62/1444 [35:42<13:19:30, 34.71s/it]  4%|▍         | 63/1444 [36:14<12:58:27, 33.82s/it]  4%|▍         | 64/1444 [36:48<13:04:52, 34.12s/it]  5%|▍         | 65/1444 [37:23<13:07:46, 34.28s/it]  5%|▍         | 66/1444 [38:03<13:49:35, 36.12s/it]  5%|▍         | 67/1444 [38:43<14:12:15, 37.14s/it]  5%|▍         | 68/1444 [39:16<13:43:17, 35.90s/it]  5%|▍         | 69/1444 [39:48<13:17:31, 34.80s/it]  5%|▍         | 70/1444 [40:19<12:50:41, 33.65s/it]                                                      5%|▍         | 70/1444 [40:19<12:50:41, 33.65s/it]  5%|▍         | 71/1444 [40:54<12:57:30, 33.98s/it]  5%|▍         | 72/1444 [41:30<13:09:43, 34.54s/it]  5%|▌         | 73/1444 [42:04<13:05:50, 34.39s/it]  5%|▌         | 74/1444 [42:42<13:30:42, 35.51s/it]  5%|▌         | 75/1444 [43:16<13:22:36, 35.18s/it]  5%|▌         | 76/1444 [43:49<13:06:51, 34.51s/it]  5%|▌         | 77/1444 [44:23<12:58:53, 34.19s/it]  5%|▌         | 78/1444 [44:57<13:01:58, 34.35s/it]  5%|▌         | 79/1444 [45:31<12:54:12, 34.03s/it]  6%|▌         | 80/1444 [46:09<13:21:10, 35.24s/it]                                                      6%|▌         | 80/1444 [46:09<13:21:10, 35.24s/it]  6%|▌         | 81/1444 [46:42<13:08:29, 34.71s/it]  6%|▌         | 82/1444 [47:23<13:47:32, 36.46s/it]  6%|▌         | 83/1444 [47:55<13:20:45, 35.30s/it]  6%|▌         | 84/1444 [48:25<12:44:30, 33.73s/it]  6%|▌         | 85/1444 [48:58<12:37:14, 33.43s/it]  6%|▌         | 86/1444 [49:30<12:24:56, 32.91s/it]  6%|▌         | 87/1444 [50:06<12:47:18, 33.93s/it]  6%|▌         | 88/1444 [50:43<13:03:16, 34.66s/it]  6%|▌         | 89/1444 [51:20<13:22:13, 35.52s/it]  6%|▌         | 90/1444 [51:53<13:05:09, 34.79s/it]                                                      6%|▌         | 90/1444 [51:53<13:05:09, 34.79s/it]  6%|▋         | 91/1444 [52:27<12:56:08, 34.42s/it]  6%|▋         | 92/1444 [52:59<12:44:32, 33.93s/it]  6%|▋         | 93/1444 [53:29<12:15:43, 32.67s/it]  7%|▋         | 94/1444 [54:06<12:45:38, 34.03s/it]  7%|▋         | 95/1444 [54:38<12:30:05, 33.36s/it]  7%|▋         | 96/1444 [55:16<12:57:38, 34.61s/it]  7%|▋         | 97/1444 [55:49<12:50:52, 34.34s/it]  7%|▋         | 98/1444 [56:26<13:06:12, 35.05s/it]  7%|▋         | 99/1444 [57:07<13:42:33, 36.69s/it]  7%|▋         | 100/1444 [57:40<13:16:03, 35.54s/it]                                                       7%|▋         | 100/1444 [57:40<13:16:03, 35.54s/it][INFO|trainer.py:4289] 2025-10-03 13:33:23,526 >> Saving model checkpoint to /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100
[INFO|configuration_utils.py:491] 2025-10-03 13:33:23,540 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100/config.json
[INFO|configuration_utils.py:826] 2025-10-03 13:33:23,542 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100/generation_config.json
[INFO|modeling_utils.py:4308] 2025-10-03 13:33:42,752 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2394] 2025-10-03 13:33:42,754 >> chat template saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100/chat_template.jinja
[INFO|tokenization_utils_base.py:2563] 2025-10-03 13:33:42,756 >> tokenizer config file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2572] 2025-10-03 13:33:42,757 >> Special tokens file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100/special_tokens_map.json
  7%|▋         | 101/1444 [58:42<16:14:39, 43.54s/it]  7%|▋         | 102/1444 [59:15<15:02:19, 40.34s/it]  7%|▋         | 103/1444 [59:49<14:20:29, 38.50s/it]  7%|▋         | 104/1444 [1:00:19<13:25:59, 36.09s/it]  7%|▋         | 105/1444 [1:00:54<13:14:57, 35.62s/it]  7%|▋         | 106/1444 [1:01:27<12:58:51, 34.93s/it]  7%|▋         | 107/1444 [1:02:07<13:32:32, 36.46s/it]  7%|▋         | 108/1444 [1:02:38<12:55:38, 34.83s/it]  8%|▊         | 109/1444 [1:03:18<13:25:27, 36.20s/it]  8%|▊         | 110/1444 [1:03:57<13:47:54, 37.24s/it]                                                         8%|▊         | 110/1444 [1:03:57<13:47:54, 37.24s/it]  8%|▊         | 111/1444 [1:04:28<13:06:12, 35.39s/it]  8%|▊         | 112/1444 [1:05:05<13:15:59, 35.86s/it]  8%|▊         | 113/1444 [1:05:38<12:55:57, 34.98s/it]  8%|▊         | 114/1444 [1:06:12<12:50:29, 34.76s/it]  8%|▊         | 115/1444 [1:06:46<12:41:15, 34.37s/it]  8%|▊         | 116/1444 [1:07:23<12:59:15, 35.21s/it]  8%|▊         | 117/1444 [1:07:55<12:36:06, 34.19s/it]  8%|▊         | 118/1444 [1:08:29<12:32:51, 34.07s/it]  8%|▊         | 119/1444 [1:09:02<12:30:32, 33.99s/it]  8%|▊         | 120/1444 [1:09:35<12:21:35, 33.61s/it]                                                         8%|▊         | 120/1444 [1:09:35<12:21:35, 33.61s/it]  8%|▊         | 121/1444 [1:10:12<12:44:52, 34.69s/it]  8%|▊         | 122/1444 [1:10:46<12:34:34, 34.25s/it]  9%|▊         | 123/1444 [1:11:23<12:54:23, 35.17s/it]  9%|▊         | 124/1444 [1:11:55<12:35:56, 34.36s/it]  9%|▊         | 125/1444 [1:12:29<12:33:25, 34.27s/it]  9%|▊         | 126/1444 [1:13:03<12:28:26, 34.07s/it]  9%|▉         | 127/1444 [1:13:41<12:52:35, 35.20s/it]  9%|▉         | 128/1444 [1:14:16<12:50:49, 35.14s/it]  9%|▉         | 129/1444 [1:14:55<13:13:42, 36.21s/it]  9%|▉         | 130/1444 [1:15:31<13:11:59, 36.16s/it]                                                         9%|▉         | 130/1444 [1:15:31<13:11:59, 36.16s/it]  9%|▉         | 131/1444 [1:16:03<12:49:04, 35.14s/it]  9%|▉         | 132/1444 [1:16:37<12:39:32, 34.73s/it]  9%|▉         | 133/1444 [1:17:14<12:53:31, 35.40s/it]  9%|▉         | 134/1444 [1:17:45<12:22:46, 34.02s/it]  9%|▉         | 135/1444 [1:18:18<12:15:53, 33.73s/it]  9%|▉         | 136/1444 [1:18:51<12:09:39, 33.47s/it]  9%|▉         | 137/1444 [1:19:28<12:30:14, 34.44s/it] 10%|▉         | 138/1444 [1:20:03<12:36:28, 34.75s/it] 10%|▉         | 139/1444 [1:20:39<12:44:41, 35.16s/it] 10%|▉         | 140/1444 [1:21:18<13:10:26, 36.37s/it]                                                        10%|▉         | 140/1444 [1:21:18<13:10:26, 36.37s/it] 10%|▉         | 141/1444 [1:21:50<12:37:41, 34.89s/it] 10%|▉         | 142/1444 [1:22:23<12:24:23, 34.30s/it] 10%|▉         | 143/1444 [1:22:57<12:21:31, 34.20s/it] 10%|▉         | 144/1444 [1:23:32<12:29:28, 34.59s/it] 10%|█         | 145/1444 [1:24:07<12:28:00, 34.55s/it] 10%|█         | 146/1444 [1:24:44<12:45:08, 35.37s/it] 10%|█         | 147/1444 [1:25:15<12:15:48, 34.04s/it] 10%|█         | 148/1444 [1:25:48<12:07:48, 33.70s/it] 10%|█         | 149/1444 [1:26:20<11:58:01, 33.27s/it] 10%|█         | 150/1444 [1:27:03<13:00:15, 36.18s/it]                                                        10%|█         | 150/1444 [1:27:03<13:00:15, 36.18s/it] 10%|█         | 151/1444 [1:27:41<13:11:12, 36.72s/it] 11%|█         | 152/1444 [1:28:12<12:35:41, 35.09s/it] 11%|█         | 153/1444 [1:28:47<12:32:24, 34.97s/it] 11%|█         | 154/1444 [1:29:20<12:21:26, 34.49s/it] 11%|█         | 155/1444 [1:29:55<12:22:04, 34.54s/it] 11%|█         | 156/1444 [1:30:28<12:14:23, 34.21s/it] 11%|█         | 157/1444 [1:31:02<12:08:44, 33.97s/it] 11%|█         | 158/1444 [1:31:36<12:06:58, 33.92s/it] 11%|█         | 159/1444 [1:32:13<12:29:38, 35.00s/it] 11%|█         | 160/1444 [1:32:46<12:17:28, 34.46s/it]                                                        11%|█         | 160/1444 [1:32:46<12:17:28, 34.46s/it] 11%|█         | 161/1444 [1:33:17<11:54:50, 33.43s/it] 11%|█         | 162/1444 [1:33:50<11:49:06, 33.19s/it] 11%|█▏        | 163/1444 [1:34:30<12:31:54, 35.22s/it] 11%|█▏        | 164/1444 [1:35:06<12:37:28, 35.51s/it] 11%|█▏        | 165/1444 [1:35:40<12:26:24, 35.01s/it] 11%|█▏        | 166/1444 [1:36:13<12:12:47, 34.40s/it] 12%|█▏        | 167/1444 [1:36:46<12:02:56, 33.97s/it] 12%|█▏        | 168/1444 [1:37:18<11:50:48, 33.42s/it] 12%|█▏        | 169/1444 [1:37:52<11:56:04, 33.70s/it] 12%|█▏        | 170/1444 [1:38:26<11:51:51, 33.53s/it]                                                        12%|█▏        | 170/1444 [1:38:26<11:51:51, 33.53s/it] 12%|█▏        | 171/1444 [1:38:56<11:33:50, 32.70s/it] 12%|█▏        | 172/1444 [1:39:28<11:29:14, 32.51s/it] 12%|█▏        | 173/1444 [1:40:03<11:42:26, 33.16s/it] 12%|█▏        | 174/1444 [1:40:33<11:18:42, 32.07s/it] 12%|█▏        | 175/1444 [1:41:10<11:53:36, 33.74s/it] 12%|█▏        | 176/1444 [1:41:45<11:56:54, 33.92s/it] 12%|█▏        | 177/1444 [1:42:17<11:49:38, 33.61s/it] 12%|█▏        | 178/1444 [1:42:50<11:41:52, 33.26s/it] 12%|█▏        | 179/1444 [1:43:21<11:26:22, 32.56s/it] 12%|█▏        | 180/1444 [1:43:56<11:45:07, 33.47s/it]                                                        12%|█▏        | 180/1444 [1:43:56<11:45:07, 33.47s/it] 13%|█▎        | 181/1444 [1:44:29<11:36:13, 33.07s/it] 13%|█▎        | 182/1444 [1:45:04<11:48:43, 33.70s/it] 13%|█▎        | 183/1444 [1:45:37<11:43:08, 33.46s/it] 13%|█▎        | 184/1444 [1:46:08<11:27:10, 32.72s/it] 13%|█▎        | 185/1444 [1:46:38<11:12:22, 32.04s/it] 13%|█▎        | 186/1444 [1:47:15<11:41:39, 33.47s/it] 13%|█▎        | 187/1444 [1:47:46<11:29:02, 32.89s/it] 13%|█▎        | 188/1444 [1:48:21<11:40:41, 33.47s/it] 13%|█▎        | 189/1444 [1:48:55<11:41:36, 33.54s/it] 13%|█▎        | 190/1444 [1:49:27<11:33:00, 33.16s/it]                                                        13%|█▎        | 190/1444 [1:49:27<11:33:00, 33.16s/it] 13%|█▎        | 191/1444 [1:50:00<11:31:04, 33.09s/it] 13%|█▎        | 192/1444 [1:50:34<11:33:52, 33.25s/it] 13%|█▎        | 193/1444 [1:51:14<12:19:16, 35.46s/it] 13%|█▎        | 194/1444 [1:51:49<12:13:17, 35.20s/it] 14%|█▎        | 195/1444 [1:52:19<11:43:08, 33.78s/it] 14%|█▎        | 196/1444 [1:52:54<11:48:58, 34.09s/it] 14%|█▎        | 197/1444 [1:53:27<11:38:14, 33.60s/it] 14%|█▎        | 198/1444 [1:54:01<11:39:19, 33.68s/it] 14%|█▍        | 199/1444 [1:54:31<11:20:15, 32.78s/it] 14%|█▍        | 200/1444 [1:55:08<11:41:20, 33.83s/it]                                                        14%|█▍        | 200/1444 [1:55:08<11:41:20, 33.83s/it][INFO|trainer.py:4289] 2025-10-03 14:30:51,415 >> Saving model checkpoint to /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-200
[INFO|configuration_utils.py:491] 2025-10-03 14:30:51,423 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-200/config.json
[INFO|configuration_utils.py:826] 2025-10-03 14:30:51,424 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:4308] 2025-10-03 14:31:10,631 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-200/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2394] 2025-10-03 14:31:10,631 >> chat template saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2563] 2025-10-03 14:31:10,632 >> tokenizer config file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2572] 2025-10-03 14:31:10,632 >> Special tokens file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-200/special_tokens_map.json
 14%|█▍        | 201/1444 [1:56:13<14:54:46, 43.19s/it] 14%|█▍        | 202/1444 [1:56:43<13:36:58, 39.47s/it] 14%|█▍        | 203/1444 [1:57:20<13:21:00, 38.73s/it] 14%|█▍        | 204/1444 [1:58:00<13:24:55, 38.95s/it] 14%|█▍        | 205/1444 [1:58:31<12:35:30, 36.59s/it] 14%|█▍        | 206/1444 [1:59:05<12:18:37, 35.80s/it] 14%|█▍        | 207/1444 [1:59:39<12:09:14, 35.37s/it] 14%|█▍        | 208/1444 [2:00:13<11:57:27, 34.83s/it] 14%|█▍        | 209/1444 [2:00:43<11:27:21, 33.39s/it] 15%|█▍        | 210/1444 [2:01:19<11:43:38, 34.21s/it]                                                        15%|█▍        | 210/1444 [2:01:19<11:43:38, 34.21s/it] 15%|█▍        | 211/1444 [2:01:50<11:22:25, 33.21s/it] 15%|█▍        | 212/1444 [2:02:21<11:10:58, 32.68s/it] 15%|█▍        | 213/1444 [2:02:59<11:44:16, 34.33s/it] 15%|█▍        | 214/1444 [2:03:33<11:40:29, 34.17s/it] 15%|█▍        | 215/1444 [2:04:08<11:44:29, 34.39s/it] 15%|█▍        | 216/1444 [2:04:45<11:58:33, 35.11s/it] 15%|█▌        | 217/1444 [2:05:17<11:38:07, 34.14s/it] 15%|█▌        | 218/1444 [2:05:49<11:27:14, 33.63s/it] 15%|█▌        | 219/1444 [2:06:26<11:43:57, 34.48s/it] 15%|█▌        | 220/1444 [2:07:01<11:48:13, 34.72s/it]                                                        15%|█▌        | 220/1444 [2:07:01<11:48:13, 34.72s/it] 15%|█▌        | 221/1444 [2:07:34<11:38:06, 34.25s/it] 15%|█▌        | 222/1444 [2:08:07<11:29:22, 33.85s/it] 15%|█▌        | 223/1444 [2:08:40<11:26:00, 33.71s/it] 16%|█▌        | 224/1444 [2:09:12<11:13:14, 33.11s/it] 16%|█▌        | 225/1444 [2:09:45<11:10:08, 32.98s/it] 16%|█▌        | 226/1444 [2:10:19<11:13:41, 33.19s/it] 16%|█▌        | 227/1444 [2:10:50<11:04:15, 32.75s/it] 16%|█▌        | 228/1444 [2:11:21<10:54:05, 32.27s/it] 16%|█▌        | 229/1444 [2:11:50<10:30:56, 31.16s/it] 16%|█▌        | 230/1444 [2:12:22<10:38:03, 31.53s/it]                                                        16%|█▌        | 230/1444 [2:12:22<10:38:03, 31.53s/it] 16%|█▌        | 231/1444 [2:12:55<10:45:43, 31.94s/it] 16%|█▌        | 232/1444 [2:13:30<11:01:18, 32.74s/it] 16%|█▌        | 233/1444 [2:14:07<11:24:52, 33.93s/it] 16%|█▌        | 234/1444 [2:14:40<11:19:42, 33.70s/it] 16%|█▋        | 235/1444 [2:15:12<11:11:57, 33.35s/it] 16%|█▋        | 236/1444 [2:15:41<10:45:19, 32.05s/it] 16%|█▋        | 237/1444 [2:16:14<10:47:49, 32.20s/it] 16%|█▋        | 238/1444 [2:16:46<10:44:28, 32.06s/it] 17%|█▋        | 239/1444 [2:17:17<10:41:45, 31.95s/it] 17%|█▋        | 240/1444 [2:17:48<10:36:22, 31.71s/it]                                                        17%|█▋        | 240/1444 [2:17:48<10:36:22, 31.71s/it] 17%|█▋        | 241/1444 [2:18:23<10:53:38, 32.60s/it] 17%|█▋        | 242/1444 [2:18:54<10:39:55, 31.94s/it] 17%|█▋        | 243/1444 [2:19:32<11:21:12, 34.03s/it] 17%|█▋        | 244/1444 [2:20:06<11:20:06, 34.01s/it] 17%|█▋        | 245/1444 [2:20:37<11:01:21, 33.10s/it] 17%|█▋        | 246/1444 [2:21:12<11:11:55, 33.65s/it] 17%|█▋        | 247/1444 [2:21:48<11:23:27, 34.26s/it] 17%|█▋        | 248/1444 [2:22:27<11:53:47, 35.81s/it] 17%|█▋        | 249/1444 [2:23:02<11:43:38, 35.33s/it] 17%|█▋        | 250/1444 [2:23:36<11:38:50, 35.12s/it]                                                        17%|█▋        | 250/1444 [2:23:36<11:38:50, 35.12s/it] 17%|█▋        | 251/1444 [2:24:08<11:15:51, 33.99s/it] 17%|█▋        | 252/1444 [2:24:44<11:27:26, 34.60s/it] 18%|█▊        | 253/1444 [2:25:16<11:15:51, 34.05s/it] 18%|█▊        | 254/1444 [2:25:51<11:16:03, 34.09s/it] 18%|█▊        | 255/1444 [2:26:22<11:00:17, 33.32s/it] 18%|█▊        | 256/1444 [2:27:02<11:39:58, 35.35s/it] 18%|█▊        | 257/1444 [2:27:39<11:46:39, 35.72s/it] 18%|█▊        | 258/1444 [2:28:10<11:21:26, 34.47s/it] 18%|█▊        | 259/1444 [2:28:42<11:05:15, 33.68s/it] 18%|█▊        | 260/1444 [2:29:17<11:12:34, 34.08s/it]                                                        18%|█▊        | 260/1444 [2:29:17<11:12:34, 34.08s/it] 18%|█▊        | 261/1444 [2:29:50<11:03:24, 33.65s/it] 18%|█▊        | 262/1444 [2:30:20<10:39:48, 32.48s/it] 18%|█▊        | 263/1444 [2:30:52<10:39:48, 32.51s/it] 18%|█▊        | 264/1444 [2:31:25<10:42:31, 32.67s/it] 18%|█▊        | 265/1444 [2:31:56<10:29:10, 32.02s/it] 18%|█▊        | 266/1444 [2:32:33<11:02:38, 33.75s/it] 18%|█▊        | 267/1444 [2:33:06<10:57:09, 33.50s/it] 19%|█▊        | 268/1444 [2:33:42<11:10:11, 34.19s/it] 19%|█▊        | 269/1444 [2:34:15<10:59:44, 33.69s/it] 19%|█▊        | 270/1444 [2:34:46<10:45:15, 32.98s/it]                                                        19%|█▊        | 270/1444 [2:34:46<10:45:15, 32.98s/it] 19%|█▉        | 271/1444 [2:35:25<11:19:28, 34.76s/it] 19%|█▉        | 272/1444 [2:36:00<11:18:54, 34.76s/it] 19%|█▉        | 273/1444 [2:36:32<11:03:38, 34.00s/it] 19%|█▉        | 274/1444 [2:37:03<10:43:03, 32.98s/it] 19%|█▉        | 275/1444 [2:37:36<10:42:31, 32.98s/it] 19%|█▉        | 276/1444 [2:38:12<11:00:40, 33.94s/it] 19%|█▉        | 277/1444 [2:38:46<11:04:44, 34.18s/it] 19%|█▉        | 278/1444 [2:39:18<10:50:09, 33.46s/it] 19%|█▉        | 279/1444 [2:39:51<10:45:05, 33.22s/it] 19%|█▉        | 280/1444 [2:40:24<10:41:03, 33.04s/it]                                                        19%|█▉        | 280/1444 [2:40:24<10:41:03, 33.04s/it] 19%|█▉        | 281/1444 [2:40:56<10:38:30, 32.94s/it] 20%|█▉        | 282/1444 [2:41:32<10:56:40, 33.91s/it] 20%|█▉        | 283/1444 [2:42:03<10:36:57, 32.92s/it] 20%|█▉        | 284/1444 [2:42:44<11:24:06, 35.38s/it] 20%|█▉        | 285/1444 [2:43:18<11:16:51, 35.04s/it] 20%|█▉        | 286/1444 [2:43:55<11:26:07, 35.55s/it] 20%|█▉        | 287/1444 [2:44:31<11:25:25, 35.54s/it] 20%|█▉        | 288/1444 [2:45:02<10:59:53, 34.25s/it] 20%|██        | 289/1444 [2:45:36<10:58:24, 34.20s/it] 20%|██        | 290/1444 [2:46:10<10:59:37, 34.30s/it]                                                        20%|██        | 290/1444 [2:46:10<10:59:37, 34.30s/it] 20%|██        | 291/1444 [2:46:44<10:54:51, 34.08s/it] 20%|██        | 292/1444 [2:47:15<10:37:05, 33.18s/it] 20%|██        | 293/1444 [2:47:46<10:22:37, 32.46s/it] 20%|██        | 294/1444 [2:48:16<10:06:12, 31.63s/it] 20%|██        | 295/1444 [2:48:47<10:03:17, 31.50s/it] 20%|██        | 296/1444 [2:49:21<10:19:13, 32.36s/it] 21%|██        | 297/1444 [2:49:53<10:16:04, 32.23s/it] 21%|██        | 298/1444 [2:50:25<10:14:23, 32.17s/it] 21%|██        | 299/1444 [2:50:58<10:19:22, 32.46s/it] 21%|██        | 300/1444 [2:51:33<10:32:53, 33.19s/it]                                                        21%|██        | 300/1444 [2:51:33<10:32:53, 33.19s/it][INFO|trainer.py:4289] 2025-10-03 15:27:17,124 >> Saving model checkpoint to /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-300
[INFO|configuration_utils.py:491] 2025-10-03 15:27:17,128 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-300/config.json
[INFO|configuration_utils.py:826] 2025-10-03 15:27:17,128 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-300/generation_config.json
[INFO|modeling_utils.py:4308] 2025-10-03 15:27:36,060 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-300/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2394] 2025-10-03 15:27:36,061 >> chat template saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-300/chat_template.jinja
[INFO|tokenization_utils_base.py:2563] 2025-10-03 15:27:36,062 >> tokenizer config file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2572] 2025-10-03 15:27:36,062 >> Special tokens file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-300/special_tokens_map.json
 21%|██        | 301/1444 [2:52:38<13:32:03, 42.63s/it] 21%|██        | 302/1444 [2:53:11<12:40:16, 39.94s/it] 21%|██        | 303/1444 [2:53:45<12:01:45, 37.95s/it] 21%|██        | 304/1444 [2:54:22<11:54:59, 37.63s/it] 21%|██        | 305/1444 [2:54:58<11:48:57, 37.35s/it] 21%|██        | 306/1444 [2:55:31<11:21:43, 35.94s/it] 21%|██▏       | 307/1444 [2:56:05<11:07:49, 35.24s/it] 21%|██▏       | 308/1444 [2:56:38<10:57:48, 34.74s/it] 21%|██▏       | 309/1444 [2:57:17<11:21:12, 36.01s/it] 21%|██▏       | 310/1444 [2:57:46<10:42:26, 33.99s/it]                                                        21%|██▏       | 310/1444 [2:57:46<10:42:26, 33.99s/it] 22%|██▏       | 311/1444 [2:58:18<10:29:56, 33.36s/it] 22%|██▏       | 312/1444 [2:58:49<10:11:51, 32.43s/it] 22%|██▏       | 313/1444 [2:59:22<10:19:22, 32.86s/it] 22%|██▏       | 314/1444 [2:59:58<10:33:12, 33.62s/it] 22%|██▏       | 315/1444 [3:00:32<10:35:26, 33.77s/it] 22%|██▏       | 316/1444 [3:01:05<10:29:56, 33.51s/it] 22%|██▏       | 317/1444 [3:01:35<10:09:56, 32.47s/it] 22%|██▏       | 318/1444 [3:02:13<10:41:58, 34.21s/it] 22%|██▏       | 319/1444 [3:02:46<10:31:04, 33.66s/it] 22%|██▏       | 320/1444 [3:03:20<10:35:30, 33.92s/it]                                                        22%|██▏       | 320/1444 [3:03:20<10:35:30, 33.92s/it] 22%|██▏       | 321/1444 [3:03:55<10:37:51, 34.08s/it] 22%|██▏       | 322/1444 [3:04:30<10:43:27, 34.41s/it] 22%|██▏       | 323/1444 [3:05:05<10:47:24, 34.65s/it] 22%|██▏       | 324/1444 [3:05:39<10:45:08, 34.56s/it] 23%|██▎       | 325/1444 [3:06:15<10:48:56, 34.80s/it] 23%|██▎       | 326/1444 [3:06:48<10:41:39, 34.44s/it] 23%|██▎       | 327/1444 [3:07:20<10:27:09, 33.69s/it] 23%|██▎       | 328/1444 [3:07:53<10:23:31, 33.52s/it] 23%|██▎       | 329/1444 [3:08:23<10:01:52, 32.39s/it] 23%|██▎       | 330/1444 [3:09:03<10:44:39, 34.72s/it]                                                        23%|██▎       | 330/1444 [3:09:03<10:44:39, 34.72s/it] 23%|██▎       | 331/1444 [3:09:34<10:22:19, 33.55s/it] 23%|██▎       | 332/1444 [3:10:10<10:35:42, 34.30s/it] 23%|██▎       | 333/1444 [3:10:47<10:47:42, 34.98s/it] 23%|██▎       | 334/1444 [3:11:16<10:13:30, 33.16s/it] 23%|██▎       | 335/1444 [3:11:52<10:31:19, 34.16s/it] 23%|██▎       | 336/1444 [3:12:30<10:54:23, 35.44s/it] 23%|██▎       | 337/1444 [3:13:06<10:52:18, 35.36s/it] 23%|██▎       | 338/1444 [3:13:41<10:51:17, 35.33s/it] 23%|██▎       | 339/1444 [3:14:12<10:27:05, 34.05s/it] 24%|██▎       | 340/1444 [3:14:44<10:17:57, 33.58s/it]                                                        24%|██▎       | 340/1444 [3:14:44<10:17:57, 33.58s/it] 24%|██▎       | 341/1444 [3:15:24<10:48:25, 35.27s/it] 24%|██▎       | 342/1444 [3:16:02<11:06:05, 36.27s/it] 24%|██▍       | 343/1444 [3:16:35<10:47:18, 35.28s/it] 24%|██▍       | 344/1444 [3:17:13<10:59:57, 36.00s/it] 24%|██▍       | 345/1444 [3:17:47<10:49:44, 35.47s/it] 24%|██▍       | 346/1444 [3:18:26<11:10:17, 36.63s/it] 24%|██▍       | 347/1444 [3:19:06<11:26:17, 37.54s/it] 24%|██▍       | 348/1444 [3:19:40<11:07:55, 36.57s/it] 24%|██▍       | 349/1444 [3:20:16<11:01:39, 36.26s/it] 24%|██▍       | 350/1444 [3:20:54<11:09:40, 36.73s/it]                                                        24%|██▍       | 350/1444 [3:20:54<11:09:40, 36.73s/it] 24%|██▍       | 351/1444 [3:21:26<10:46:33, 35.49s/it] 24%|██▍       | 352/1444 [3:22:06<11:05:59, 36.59s/it] 24%|██▍       | 353/1444 [3:22:36<10:31:14, 34.72s/it] 25%|██▍       | 354/1444 [3:23:14<10:49:11, 35.74s/it] 25%|██▍       | 355/1444 [3:23:47<10:34:32, 34.96s/it] 25%|██▍       | 356/1444 [3:24:18<10:11:54, 33.75s/it] 25%|██▍       | 357/1444 [3:24:46<9:41:10, 32.08s/it]  25%|██▍       | 358/1444 [3:25:17<9:35:43, 31.81s/it] 25%|██▍       | 359/1444 [3:25:50<9:41:44, 32.17s/it] 25%|██▍       | 360/1444 [3:26:25<9:52:37, 32.80s/it]                                                       25%|██▍       | 360/1444 [3:26:25<9:52:37, 32.80s/it] 25%|██▌       | 361/1444 [3:27:05<10:29:44, 34.89s/it] 25%|██▌       | 362/1444 [3:27:42<10:44:31, 35.74s/it] 25%|██▌       | 363/1444 [3:28:16<10:31:57, 35.08s/it] 25%|██▌       | 364/1444 [3:28:49<10:23:36, 34.64s/it] 25%|██▌       | 365/1444 [3:29:24<10:23:48, 34.69s/it] 25%|██▌       | 366/1444 [3:29:59<10:25:17, 34.80s/it] 25%|██▌       | 367/1444 [3:30:35<10:29:59, 35.10s/it] 25%|██▌       | 368/1444 [3:31:08<10:16:16, 34.36s/it] 26%|██▌       | 369/1444 [3:31:46<10:37:42, 35.59s/it] 26%|██▌       | 370/1444 [3:32:18<10:18:44, 34.57s/it]                                                        26%|██▌       | 370/1444 [3:32:18<10:18:44, 34.57s/it] 26%|██▌       | 371/1444 [3:32:49<9:58:29, 33.47s/it]  26%|██▌       | 372/1444 [3:33:24<10:07:09, 33.98s/it] 26%|██▌       | 373/1444 [3:34:01<10:21:53, 34.84s/it] 26%|██▌       | 374/1444 [3:34:41<10:45:12, 36.18s/it] 26%|██▌       | 375/1444 [3:35:16<10:40:29, 35.95s/it] 26%|██▌       | 376/1444 [3:35:49<10:26:35, 35.20s/it] 26%|██▌       | 377/1444 [3:36:27<10:37:54, 35.87s/it] 26%|██▌       | 378/1444 [3:37:00<10:22:23, 35.03s/it] 26%|██▌       | 379/1444 [3:37:38<10:40:16, 36.07s/it] 26%|██▋       | 380/1444 [3:38:10<10:15:26, 34.71s/it]                                                        26%|██▋       | 380/1444 [3:38:10<10:15:26, 34.71s/it] 26%|██▋       | 381/1444 [3:38:40<9:51:11, 33.37s/it]  26%|██▋       | 382/1444 [3:39:12<9:39:53, 32.76s/it] 27%|██▋       | 383/1444 [3:39:42<9:27:45, 32.11s/it] 27%|██▋       | 384/1444 [3:40:20<9:56:46, 33.78s/it] 27%|██▋       | 385/1444 [3:40:53<9:52:25, 33.57s/it] 27%|██▋       | 386/1444 [3:41:26<9:47:23, 33.31s/it] 27%|██▋       | 387/1444 [3:42:00<9:53:10, 33.67s/it] 27%|██▋       | 388/1444 [3:42:32<9:41:30, 33.04s/it] 27%|██▋       | 389/1444 [3:43:07<9:51:49, 33.66s/it] 27%|██▋       | 390/1444 [3:43:37<9:30:42, 32.49s/it]                                                       27%|██▋       | 390/1444 [3:43:37<9:30:42, 32.49s/it] 27%|██▋       | 391/1444 [3:44:10<9:35:34, 32.80s/it] 27%|██▋       | 392/1444 [3:44:42<9:29:44, 32.50s/it] 27%|██▋       | 393/1444 [3:45:19<9:53:32, 33.88s/it] 27%|██▋       | 394/1444 [3:45:51<9:45:05, 33.43s/it] 27%|██▋       | 395/1444 [3:46:27<9:58:23, 34.23s/it] 27%|██▋       | 396/1444 [3:47:05<10:17:48, 35.37s/it] 27%|██▋       | 397/1444 [3:47:36<9:49:24, 33.78s/it]  28%|██▊       | 398/1444 [3:48:13<10:08:20, 34.90s/it] 28%|██▊       | 399/1444 [3:48:49<10:15:12, 35.32s/it] 28%|██▊       | 400/1444 [3:49:22<9:58:51, 34.42s/it]                                                        28%|██▊       | 400/1444 [3:49:22<9:58:51, 34.42s/it][INFO|trainer.py:4289] 2025-10-03 16:25:05,258 >> Saving model checkpoint to /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-400
[INFO|configuration_utils.py:491] 2025-10-03 16:25:05,267 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-400/config.json
[INFO|configuration_utils.py:826] 2025-10-03 16:25:05,267 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-400/generation_config.json
[INFO|modeling_utils.py:4308] 2025-10-03 16:25:24,322 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-400/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2394] 2025-10-03 16:25:24,323 >> chat template saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2563] 2025-10-03 16:25:24,324 >> tokenizer config file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2572] 2025-10-03 16:25:24,324 >> Special tokens file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-400/special_tokens_map.json
 28%|██▊       | 401/1444 [3:50:24<12:22:49, 42.73s/it] 28%|██▊       | 402/1444 [3:50:56<11:28:03, 39.62s/it] 28%|██▊       | 403/1444 [3:51:28<10:47:47, 37.34s/it] 28%|██▊       | 404/1444 [3:52:02<10:31:22, 36.43s/it] 28%|██▊       | 405/1444 [3:52:36<10:17:26, 35.66s/it] 28%|██▊       | 406/1444 [3:53:07<9:50:25, 34.13s/it]  28%|██▊       | 407/1444 [3:53:45<10:08:25, 35.20s/it] 28%|██▊       | 408/1444 [3:54:19<10:06:15, 35.11s/it] 28%|██▊       | 409/1444 [3:54:52<9:52:24, 34.34s/it]  28%|██▊       | 410/1444 [3:55:27<9:55:07, 34.53s/it]                                                       28%|██▊       | 410/1444 [3:55:27<9:55:07, 34.53s/it] 28%|██▊       | 411/1444 [3:56:00<9:47:32, 34.13s/it] 29%|██▊       | 412/1444 [3:56:33<9:38:25, 33.63s/it] 29%|██▊       | 413/1444 [3:57:05<9:32:31, 33.32s/it] 29%|██▊       | 414/1444 [3:57:36<9:18:50, 32.55s/it] 29%|██▊       | 415/1444 [3:58:11<9:29:32, 33.21s/it] 29%|██▉       | 416/1444 [3:58:44<9:26:51, 33.09s/it] 29%|██▉       | 417/1444 [3:59:18<9:34:14, 33.55s/it] 29%|██▉       | 418/1444 [3:59:50<9:22:30, 32.90s/it] 29%|██▉       | 419/1444 [4:00:25<9:34:50, 33.65s/it] 29%|██▉       | 420/1444 [4:00:57<9:23:45, 33.03s/it]                                                       29%|██▉       | 420/1444 [4:00:57<9:23:45, 33.03s/it] 29%|██▉       | 421/1444 [4:01:31<9:29:56, 33.43s/it] 29%|██▉       | 422/1444 [4:02:06<9:37:59, 33.93s/it] 29%|██▉       | 423/1444 [4:02:39<9:32:48, 33.66s/it] 29%|██▉       | 424/1444 [4:03:16<9:46:30, 34.50s/it] 29%|██▉       | 425/1444 [4:03:50<9:43:42, 34.37s/it] 30%|██▉       | 426/1444 [4:04:26<9:55:05, 35.07s/it] 30%|██▉       | 427/1444 [4:04:58<9:36:32, 34.01s/it] 30%|██▉       | 428/1444 [4:05:36<9:56:48, 35.24s/it] 30%|██▉       | 429/1444 [4:06:12<9:58:30, 35.38s/it] 30%|██▉       | 430/1444 [4:06:41<9:29:35, 33.70s/it]                                                       30%|██▉       | 430/1444 [4:06:41<9:29:35, 33.70s/it] 30%|██▉       | 431/1444 [4:07:11<9:09:11, 32.53s/it] 30%|██▉       | 432/1444 [4:07:45<9:15:57, 32.96s/it] 30%|██▉       | 433/1444 [4:08:19<9:20:14, 33.25s/it] 30%|███       | 434/1444 [4:09:02<10:07:13, 36.07s/it] 30%|███       | 435/1444 [4:09:35<9:52:00, 35.20s/it]  30%|███       | 436/1444 [4:10:09<9:43:29, 34.73s/it] 30%|███       | 437/1444 [4:10:39<9:23:27, 33.57s/it] 30%|███       | 438/1444 [4:11:13<9:20:28, 33.43s/it] 30%|███       | 439/1444 [4:11:47<9:27:09, 33.86s/it] 30%|███       | 440/1444 [4:12:18<9:08:24, 32.77s/it]                                                       30%|███       | 440/1444 [4:12:18<9:08:24, 32.77s/it] 31%|███       | 441/1444 [4:12:50<9:05:58, 32.66s/it] 31%|███       | 442/1444 [4:13:28<9:29:27, 34.10s/it] 31%|███       | 443/1444 [4:13:57<9:07:47, 32.83s/it] 31%|███       | 444/1444 [4:14:31<9:10:13, 33.01s/it] 31%|███       | 445/1444 [4:15:04<9:11:04, 33.10s/it] 31%|███       | 446/1444 [4:15:35<9:00:08, 32.47s/it] 31%|███       | 447/1444 [4:16:10<9:12:19, 33.24s/it] 31%|███       | 448/1444 [4:16:46<9:24:05, 33.98s/it] 31%|███       | 449/1444 [4:17:18<9:13:00, 33.35s/it] 31%|███       | 450/1444 [4:17:59<9:51:25, 35.70s/it]                                                       31%|███       | 450/1444 [4:17:59<9:51:25, 35.70s/it] 31%|███       | 451/1444 [4:18:31<9:34:55, 34.74s/it] 31%|███▏      | 452/1444 [4:19:06<9:32:11, 34.61s/it] 31%|███▏      | 453/1444 [4:19:40<9:27:50, 34.38s/it] 31%|███▏      | 454/1444 [4:20:20<9:54:44, 36.05s/it] 32%|███▏      | 455/1444 [4:20:56<9:57:21, 36.24s/it] 32%|███▏      | 456/1444 [4:21:26<9:24:42, 34.29s/it] 32%|███▏      | 457/1444 [4:21:58<9:13:38, 33.66s/it] 32%|███▏      | 458/1444 [4:22:29<9:01:46, 32.97s/it] 32%|███▏      | 459/1444 [4:23:05<9:13:02, 33.69s/it] 32%|███▏      | 460/1444 [4:23:38<9:08:39, 33.46s/it]                                                       32%|███▏      | 460/1444 [4:23:38<9:08:39, 33.46s/it] 32%|███▏      | 461/1444 [4:24:11<9:08:44, 33.49s/it] 32%|███▏      | 462/1444 [4:24:50<9:32:05, 34.95s/it] 32%|███▏      | 463/1444 [4:25:28<9:46:28, 35.87s/it] 32%|███▏      | 464/1444 [4:26:04<9:46:57, 35.94s/it] 32%|███▏      | 465/1444 [4:26:34<9:16:50, 34.13s/it] 32%|███▏      | 466/1444 [4:27:13<9:39:29, 35.55s/it] 32%|███▏      | 467/1444 [4:27:50<9:46:45, 36.03s/it] 32%|███▏      | 468/1444 [4:28:25<9:42:09, 35.79s/it] 32%|███▏      | 469/1444 [4:29:02<9:49:16, 36.26s/it] 33%|███▎      | 470/1444 [4:29:36<9:33:53, 35.35s/it]                                                       33%|███▎      | 470/1444 [4:29:36<9:33:53, 35.35s/it] 33%|███▎      | 471/1444 [4:30:07<9:13:52, 34.15s/it] 33%|███▎      | 472/1444 [4:30:43<9:20:13, 34.58s/it] 33%|███▎      | 473/1444 [4:31:20<9:34:43, 35.51s/it] 33%|███▎      | 474/1444 [4:31:57<9:42:28, 36.03s/it] 33%|███▎      | 475/1444 [4:32:29<9:19:11, 34.63s/it] 33%|███▎      | 476/1444 [4:33:05<9:25:11, 35.03s/it] 33%|███▎      | 477/1444 [4:33:39<9:22:15, 34.89s/it] 33%|███▎      | 478/1444 [4:34:15<9:25:05, 35.10s/it] 33%|███▎      | 479/1444 [4:34:51<9:29:37, 35.42s/it] 33%|███▎      | 480/1444 [4:35:24<9:18:01, 34.73s/it]                                                       33%|███▎      | 480/1444 [4:35:24<9:18:01, 34.73s/it] 33%|███▎      | 481/1444 [4:36:03<9:38:40, 36.05s/it] 33%|███▎      | 482/1444 [4:36:40<9:39:28, 36.14s/it] 33%|███▎      | 483/1444 [4:37:15<9:33:43, 35.82s/it] 34%|███▎      | 484/1444 [4:37:45<9:07:30, 34.22s/it] 34%|███▎      | 485/1444 [4:38:16<8:52:15, 33.30s/it] 34%|███▎      | 486/1444 [4:38:49<8:48:27, 33.10s/it] 34%|███▎      | 487/1444 [4:39:18<8:30:22, 32.00s/it] 34%|███▍      | 488/1444 [4:39:54<8:46:25, 33.04s/it] 34%|███▍      | 489/1444 [4:40:27<8:47:56, 33.17s/it] 34%|███▍      | 490/1444 [4:41:01<8:49:52, 33.33s/it]                                                       34%|███▍      | 490/1444 [4:41:01<8:49:52, 33.33s/it] 34%|███▍      | 491/1444 [4:41:31<8:33:57, 32.36s/it] 34%|███▍      | 492/1444 [4:42:03<8:30:35, 32.18s/it] 34%|███▍      | 493/1444 [4:42:37<8:40:23, 32.83s/it] 34%|███▍      | 494/1444 [4:43:14<8:56:18, 33.87s/it] 34%|███▍      | 495/1444 [4:43:50<9:06:58, 34.58s/it] 34%|███▍      | 496/1444 [4:44:22<8:55:46, 33.91s/it] 34%|███▍      | 497/1444 [4:44:53<8:40:06, 32.95s/it] 34%|███▍      | 498/1444 [4:45:27<8:44:24, 33.26s/it] 35%|███▍      | 499/1444 [4:45:58<8:35:11, 32.71s/it] 35%|███▍      | 500/1444 [4:46:34<8:49:02, 33.63s/it]                                                       35%|███▍      | 500/1444 [4:46:34<8:49:02, 33.63s/it]/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:4623] 2025-10-03 17:22:06,245 >> 
***** Running Evaluation *****
[INFO|trainer.py:4625] 2025-10-03 17:22:06,245 >>   Num examples = 1867
[INFO|trainer.py:4628] 2025-10-03 17:22:06,246 >>   Batch size = 1
[INFO|trainer.py:4623] 2025-10-03 17:22:06,246 >> 
***** Running Evaluation *****
[INFO|trainer.py:4623] 2025-10-03 17:22:06,246 >> 
***** Running Evaluation *****
[INFO|trainer.py:4623] 2025-10-03 17:22:06,246 >> 
***** Running Evaluation *****
[INFO|trainer.py:4625] 2025-10-03 17:22:06,246 >>   Num examples = 1867
[INFO|trainer.py:4625] 2025-10-03 17:22:06,246 >>   Num examples = 1867
[INFO|trainer.py:4628] 2025-10-03 17:22:06,246 >>   Batch size = 1
[INFO|trainer.py:4628] 2025-10-03 17:22:06,246 >>   Batch size = 1
[INFO|trainer.py:4625] 2025-10-03 17:22:06,246 >>   Num examples = 1867
[INFO|trainer.py:4628] 2025-10-03 17:22:06,246 >>   Batch size = 1
[INFO|trainer.py:4623] 2025-10-03 17:22:06,246 >> 
***** Running Evaluation *****
[INFO|trainer.py:4625] 2025-10-03 17:22:06,246 >>   Num examples = 1867
[INFO|trainer.py:4628] 2025-10-03 17:22:06,246 >>   Batch size = 1
[INFO|trainer.py:4623] 2025-10-03 17:22:06,246 >> 
***** Running Evaluation *****
[INFO|trainer.py:4625] 2025-10-03 17:22:06,246 >>   Num examples = 1867
[INFO|trainer.py:4623] 2025-10-03 17:22:06,246 >> 
***** Running Evaluation *****
[INFO|trainer.py:4628] 2025-10-03 17:22:06,246 >>   Batch size = 1
[INFO|trainer.py:4625] 2025-10-03 17:22:06,246 >>   Num examples = 1867
[INFO|trainer.py:4628] 2025-10-03 17:22:06,246 >>   Batch size = 1
[INFO|trainer.py:4623] 2025-10-03 17:22:06,246 >> 
***** Running Evaluation *****
[INFO|trainer.py:4625] 2025-10-03 17:22:06,247 >>   Num examples = 1867
[INFO|trainer.py:4628] 2025-10-03 17:22:06,247 >>   Batch size = 1

  0%|          | 0/59 [00:00<?, ?it/s][A
[rank28]: Traceback (most recent call last):
[rank28]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/train.py", line 27, in <module>
[rank28]:     main()
[rank28]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/train.py", line 18, in main
[rank28]:     run_exp()
[rank28]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank28]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank28]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank28]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank28]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/sft/workflow.py", line 96, in run_sft
[rank28]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank28]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 2328, in train
[rank28]:     return inner_training_loop(
[rank28]:            ^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 2754, in _inner_training_loop
[rank28]:     self._maybe_log_save_evaluate(
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 3227, in _maybe_log_save_evaluate
[rank28]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank28]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 3176, in _evaluate
[rank28]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank28]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
[rank28]:     return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 4469, in evaluate
[rank28]:     output = eval_loop(
[rank28]:              ^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 4665, in evaluation_loop
[rank28]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank28]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/sft/trainer.py", line 137, in prediction_step
[rank28]:     loss, generated_tokens, _ = super().prediction_step(
[rank28]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 289, in prediction_step
[rank28]:     return super().prediction_step(
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 4881, in prediction_step
[rank28]:     loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
[rank28]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/sft/trainer.py", line 117, in compute_loss
[rank28]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 4099, in compute_loss
[rank28]:     outputs = model(**inputs)
[rank28]:               ^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank28]:     return self._call_impl(*args, **kwargs)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank28]:     return inner()
[rank28]:            ^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank28]:     result = forward_call(*args, **kwargs)
[rank28]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/Liger-Kernel/src/liger_kernel/transformers/model/gpt_oss.py", line 92, in lce_forward
[rank28]:     loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 67, in ForCausalLMLoss
[rank28]:     loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 36, in fixed_cross_entropy
[rank28]:     loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/nn/functional.py", line 3462, in cross_entropy
[rank28]:     return torch._C._nn.cross_entropy_loss(
[rank28]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank28]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.56 GiB. GPU 0 has a total capacity of 79.18 GiB of which 14.87 GiB is free. Including non-PyTorch memory, this process has 64.29 GiB memory in use. Of the allocated memory 60.42 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W1003 17:22:35.008000 3141086 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3141149 closing signal SIGTERM
W1003 17:22:35.012000 3141086 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3141150 closing signal SIGTERM
W1003 17:22:35.125000 3141086 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3141151 closing signal SIGTERM
E1003 17:22:35.165000 3141086 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 3141148) of binary: /scratch/gpfs/yl7690/.conda/envs/oss/bin/python3.1
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
src/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-03_17:22:35
  host      : della-j17g1
  rank      : 28 (local_rank: 0)
  exitcode  : 1 (pid: 3141148)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: della-j17g1: task 7: Exited with exit code 1
srun: Terminating StepId=1159862.0
  3%|▎         | 2/59 [00:03<01:50,  1.94s/it][Aslurmstepd: error: *** STEP 1159862.0 ON della-j12g1 CANCELLED AT 2025-10-03T17:22:35 ***
W1003 17:22:35.671000 3246996 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 17:22:35.672000 2348438 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 17:22:35.671000 2414669 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 17:22:35.672000 2896856 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 17:22:35.672000 1615146 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 17:22:35.675000 3246996 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3247066 closing signal SIGTERM
W1003 17:22:35.675000 2348438 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2348487 closing signal SIGTERM
W1003 17:22:35.675000 2896856 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2896906 closing signal SIGTERM
W1003 17:22:35.675000 2414669 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2414720 closing signal SIGTERM
W1003 17:22:35.672000 1612067 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 17:22:35.675000 1615146 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1615203 closing signal SIGTERM
W1003 17:22:35.671000 1198023 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 17:22:35.676000 1198023 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1198086 closing signal SIGTERM
W1003 17:22:35.676000 1612067 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1612124 closing signal SIGTERM
W1003 17:22:35.680000 2896856 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2896907 closing signal SIGTERM
W1003 17:22:35.682000 1615146 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1615204 closing signal SIGTERM
W1003 17:22:35.687000 1615146 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1615205 closing signal SIGTERM
W1003 17:22:35.686000 3246996 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3247067 closing signal SIGTERM
W1003 17:22:35.689000 3246996 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3247068 closing signal SIGTERM
W1003 17:22:35.695000 1198023 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1198087 closing signal SIGTERM
W1003 17:22:35.696000 3246996 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3247069 closing signal SIGTERM
W1003 17:22:35.701000 2414669 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2414721 closing signal SIGTERM
W1003 17:22:35.722000 1612067 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1612125 closing signal SIGTERM
W1003 17:22:35.735000 2896856 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2896908 closing signal SIGTERM
W1003 17:22:35.744000 2348438 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2348488 closing signal SIGTERM
W1003 17:22:35.767000 2896856 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2896909 closing signal SIGTERM
W1003 17:22:35.772000 1198023 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1198088 closing signal SIGTERM
W1003 17:22:35.785000 1612067 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1612126 closing signal SIGTERM
W1003 17:22:35.801000 1615146 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1615206 closing signal SIGTERM
W1003 17:22:35.810000 1612067 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1612127 closing signal SIGTERM
W1003 17:22:35.827000 2348438 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2348489 closing signal SIGTERM
W1003 17:22:35.843000 1198023 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1198089 closing signal SIGTERM
W1003 17:22:35.852000 2414669 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2414722 closing signal SIGTERM
W1003 17:22:35.904000 2414669 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2414723 closing signal SIGTERM
W1003 17:22:35.924000 2348438 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2348490 closing signal SIGTERM
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2414669 got signal: 15
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1198023 got signal: 15
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1615146 got signal: 15
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1612067 got signal: 15
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3246996 got signal: 15
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2896856 got signal: 15
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2348438 got signal: 15
srun: error: della-j15g2: task 3: Exited with exit code 1
srun: error: della-j15g3: task 4: Exited with exit code 1
srun: error: della-j16g3: task 6: Exited with exit code 1
srun: error: della-j16g1: task 5: Exited with exit code 1
srun: error: della-j12g1: task 0: Exited with exit code 1
srun: error: della-j15g1: task 2: Exited with exit code 1
srun: error: della-j12g2: task 1: Exited with exit code 1
