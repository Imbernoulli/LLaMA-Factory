+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=2 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=1 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=7 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 -+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
-enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ head -n 1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=3 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=5 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=0 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=6 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=4 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset formal --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 3.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
W1003 11:37:03.194000 1179174 site-packages/torch/distributed/run.py:774] 
W1003 11:37:03.194000 1179174 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:03.194000 1179174 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 11:37:03.194000 1179174 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:03.200000 3712844 site-packages/torch/distributed/run.py:774] 
W1003 11:37:03.200000 3712844 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:03.200000 3712844 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 11:37:03.200000 3712844 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:04.088000 1594999 site-packages/torch/distributed/run.py:774] 
W1003 11:37:04.088000 1594999 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:04.088000 1594999 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 11:37:04.088000 1594999 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:04.089000 2397575 site-packages/torch/distributed/run.py:774] 
W1003 11:37:04.089000 2397575 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:04.089000 2397575 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 11:37:04.089000 2397575 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:04.689000 3124052 site-packages/torch/distributed/run.py:774] 
W1003 11:37:04.689000 3124052 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:04.689000 3124052 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 11:37:04.689000 3124052 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:05.575000 2879060 site-packages/torch/distributed/run.py:774] 
W1003 11:37:05.575000 2879060 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:05.575000 2879060 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 11:37:05.575000 2879060 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:05.660000 3229170 site-packages/torch/distributed/run.py:774] 
W1003 11:37:05.660000 3229170 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:05.660000 3229170 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 11:37:05.660000 3229170 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:05.724000 2328484 site-packages/torch/distributed/run.py:774] 
W1003 11:37:05.724000 2328484 site-packages/torch/distributed/run.py:774] *****************************************
W1003 11:37:05.724000 2328484 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 11:37:05.724000 2328484 site-packages/torch/distributed/run.py:774] *****************************************
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,550 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,550 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,550 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,550 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,550 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,550 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,553 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,553 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,553 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,553 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,553 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,553 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,558 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,558 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,558 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,558 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,558 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,558 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,559 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,559 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,559 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,560 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,571 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,571 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,571 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,571 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,571 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,571 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,577 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,577 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,577 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,577 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,577 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:30,578 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,214 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 11:37:31,215 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 11:37:31,220 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,220 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,220 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,220 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,220 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,220 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,220 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,226 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 11:37:31,226 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,226 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 11:37:31,227 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,227 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 11:37:31,228 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 11:37:31,229 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,229 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,229 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,229 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,229 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,229 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,229 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,230 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 11:37:31,231 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 11:37:31,231 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,231 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,231 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,232 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,232 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,232 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,232 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,233 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:839] 2025-10-03 11:37:31,233 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-03 11:37:31,233 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,233 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,233 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,233 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,233 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,233 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,233 >> loading file chat_template.jinja
[INFO|configuration_utils.py:839] 2025-10-03 11:37:31,233 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,234 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,234 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,234 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,234 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,234 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,234 >> loading file chat_template.jinja
[INFO|configuration_utils.py:839] 2025-10-03 11:37:31,235 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,235 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,236 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,236 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,236 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,236 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,236 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,237 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 11:37:31,238 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 11:37:31,240 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,240 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,240 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,240 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,240 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,240 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,240 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,241 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 11:37:31,241 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 11:37:31,243 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,244 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,244 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,244 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,244 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,244 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 11:37:31,244 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,902 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,936 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,939 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,939 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,941 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,945 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,951 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 11:37:31,967 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187200 examples [00:01, 391.36 examples/s]            Converting format of dataset (num_proc=64): 197866 examples [00:01, 10848.66 examples/s]Converting format of dataset (num_proc=64): 223492 examples [00:01, 41079.46 examples/s]Converting format of dataset (num_proc=64): 252210 examples [00:01, 77872.03 examples/s]Converting format of dataset (num_proc=64): 272677 examples [00:01, 99850.67 examples/s]Converting format of dataset (num_proc=64): 296981 examples [00:01, 128367.46 examples/s]Converting format of dataset (num_proc=64): 318859 examples [00:01, 146166.85 examples/s]Converting format of dataset (num_proc=64): 340188 examples [00:02, 160489.77 examples/s]Converting format of dataset (num_proc=64): 361196 examples [00:02, 137806.48 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 60470.89 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187252 examples [00:01, 428.99 examples/s]            Converting format of dataset (num_proc=64): 196192 examples [00:01, 9142.86 examples/s]Converting format of dataset (num_proc=64): 221496 examples [00:01, 38999.10 examples/s]Converting format of dataset (num_proc=64): 247746 examples [00:01, 72061.32 examples/s]Converting format of dataset (num_proc=64): 273304 examples [00:01, 103935.25 examples/s]Converting format of dataset (num_proc=64): 300668 examples [00:01, 136912.71 examples/s]Converting format of dataset (num_proc=64): 323734 examples [00:01, 148581.01 examples/s]Converting format of dataset (num_proc=64): 345804 examples [00:02, 164464.21 examples/s]Converting format of dataset (num_proc=64): 367411 examples [00:02, 124212.58 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 51635.32 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187251 examples [00:01, 431.24 examples/s]            Converting format of dataset (num_proc=64): 194348 examples [00:01, 7386.16 examples/s]Converting format of dataset (num_proc=64): 213331 examples [00:01, 29875.00 examples/s]Converting format of dataset (num_proc=64): 239858 examples [00:01, 65077.05 examples/s]Converting format of dataset (num_proc=64): 264517 examples [00:01, 97020.44 examples/s]Converting format of dataset (num_proc=64): 291960 examples [00:01, 132541.90 examples/s]Converting format of dataset (num_proc=64): 314464 examples [00:01, 146943.87 examples/s]Converting format of dataset (num_proc=64): 335877 examples [00:02, 159773.53 examples/s]Converting format of dataset (num_proc=64): 356917 examples [00:02, 147976.03 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 51506.63 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187027 examples [00:01, 245.71 examples/s]            Converting format of dataset (num_proc=64): 213473 examples [00:01, 24974.72 examples/s]Converting format of dataset (num_proc=64): 233368 examples [00:01, 45488.61 examples/s]Converting format of dataset (num_proc=64): 262849 examples [00:01, 81245.88 examples/s]Converting format of dataset (num_proc=64): 284911 examples [00:01, 103873.34 examples/s]Converting format of dataset (num_proc=64): 308205 examples [00:01, 127729.10 examples/s]Converting format of dataset (num_proc=64): 330229 examples [00:02, 144343.94 examples/s]Converting format of dataset (num_proc=64): 351754 examples [00:02, 158524.35 examples/s]Converting format of dataset (num_proc=64): 372817 examples [00:02, 103050.47 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 51140.27 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187174 examples [00:01, 348.14 examples/s]            Converting format of dataset (num_proc=64): 207404 examples [00:01, 19094.26 examples/s]Converting format of dataset (num_proc=64): 231480 examples [00:01, 44998.41 examples/s]Converting format of dataset (num_proc=64): 257147 examples [00:01, 75256.30 examples/s]Converting format of dataset (num_proc=64): 280183 examples [00:01, 101249.28 examples/s]Converting format of dataset (num_proc=64): 303363 examples [00:01, 125708.83 examples/s]Converting format of dataset (num_proc=64): 327671 examples [00:02, 148728.75 examples/s]Converting format of dataset (num_proc=64): 349733 examples [00:02, 150304.94 examples/s]Converting format of dataset (num_proc=64): 369849 examples [00:02, 103168.50 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 51160.55 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187000 examples [00:01, 232.98 examples/s]            Converting format of dataset (num_proc=64): 192344 examples [00:01, 5361.08 examples/s]Converting format of dataset (num_proc=64): 216822 examples [00:01, 34331.41 examples/s]Converting format of dataset (num_proc=64): 241767 examples [00:01, 65537.58 examples/s]Converting format of dataset (num_proc=64): 267540 examples [00:01, 98552.73 examples/s]Converting format of dataset (num_proc=64): 292570 examples [00:01, 128417.75 examples/s]Converting format of dataset (num_proc=64): 314395 examples [00:01, 132978.91 examples/s]Converting format of dataset (num_proc=64): 340900 examples [00:02, 161880.26 examples/s]Converting format of dataset (num_proc=64): 362650 examples [00:02, 133260.92 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 50727.96 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187205 examples [00:01, 391.14 examples/s]            Converting format of dataset (num_proc=64): 189960 examples [00:01, 3011.03 examples/s]Converting format of dataset (num_proc=64): 211839 examples [00:01, 29177.85 examples/s]Converting format of dataset (num_proc=64): 237560 examples [00:01, 62751.29 examples/s]Converting format of dataset (num_proc=64): 255052 examples [00:01, 81598.25 examples/s]Converting format of dataset (num_proc=64): 278119 examples [00:01, 110366.81 examples/s]Converting format of dataset (num_proc=64): 296871 examples [00:01, 127040.75 examples/s]Converting format of dataset (num_proc=64): 315758 examples [00:02, 139088.29 examples/s]Converting format of dataset (num_proc=64): 339658 examples [00:02, 163484.55 examples/s]Converting format of dataset (num_proc=64): 359736 examples [00:02, 145297.06 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:03, 50640.96 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 186688/186688 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 187112 examples [00:01, 281.11 examples/s]            Converting format of dataset (num_proc=64): 211287 examples [00:01, 21040.16 examples/s]Converting format of dataset (num_proc=64): 232465 examples [00:01, 41851.80 examples/s]Converting format of dataset (num_proc=64): 254504 examples [00:01, 65881.74 examples/s]Converting format of dataset (num_proc=64): 273585 examples [00:01, 85338.07 examples/s]Converting format of dataset (num_proc=64): 292187 examples [00:02, 84374.98 examples/s]Converting format of dataset (num_proc=64): 307239 examples [00:03, 29288.87 examples/s]Converting format of dataset (num_proc=64): 317726 examples [00:04, 21001.09 examples/s]Converting format of dataset (num_proc=64): 325201 examples [00:05, 17696.37 examples/s]Converting format of dataset (num_proc=64): 330665 examples [00:05, 15800.04 examples/s]Converting format of dataset (num_proc=64): 334758 examples [00:06, 14649.50 examples/s]Converting format of dataset (num_proc=64): 337912 examples [00:06, 13794.72 examples/s]Converting format of dataset (num_proc=64): 340418 examples [00:06, 13032.47 examples/s]Converting format of dataset (num_proc=64): 342453 examples [00:07, 11017.04 examples/s]Converting format of dataset (num_proc=64): 344044 examples [00:07, 9679.60 examples/s] Converting format of dataset (num_proc=64): 345322 examples [00:07, 9346.24 examples/s]Converting format of dataset (num_proc=64): 346449 examples [00:07, 8491.78 examples/s]Converting format of dataset (num_proc=64): 347396 examples [00:07, 8011.18 examples/s]Converting format of dataset (num_proc=64): 348904 examples [00:08, 9056.31 examples/s]Converting format of dataset (num_proc=64): 351826 examples [00:08, 12596.38 examples/s]Converting format of dataset (num_proc=64): 354228 examples [00:08, 14886.40 examples/s]Converting format of dataset (num_proc=64): 356359 examples [00:08, 16265.84 examples/s]Converting format of dataset (num_proc=64): 358512 examples [00:08, 17523.00 examples/s]Converting format of dataset (num_proc=64): 360566 examples [00:08, 17884.75 examples/s]Converting format of dataset (num_proc=64): 362605 examples [00:08, 17838.25 examples/s]Converting format of dataset (num_proc=64): 364664 examples [00:08, 18403.78 examples/s]Converting format of dataset (num_proc=64): 366610 examples [00:08, 18394.73 examples/s]Converting format of dataset (num_proc=64): 368607 examples [00:09, 18708.35 examples/s]Converting format of dataset (num_proc=64): 370559 examples [00:09, 17845.00 examples/s]Converting format of dataset (num_proc=64): 372395 examples [00:09, 17444.82 examples/s]Converting format of dataset (num_proc=64): 373376 examples [00:09, 19261.91 examples/s]
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:14<44:08, 70.11 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:15<19:53, 154.80 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<07:48, 389.78 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:16<05:50, 518.62 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:16<03:25, 873.70 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:17<02:11, 1347.01 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:17<01:51, 1580.74 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:17<01:16, 2297.88 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 13Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<41:54, 73.85 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<18:04, 170.30 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:15<11:06, 275.55 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<07:18, 416.29 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:16<05:12, 581.46 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▍         | 8000/186688 [00:16<02:18, 1287.36 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▌         | 11000/186688 [00:16<01:20, 2180.15 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:17<01:37, 1791.42 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:14<45:09, 68.53 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<19:02, 161.72 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:15<10:40, 286.95 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:15<05:00, 603.96 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:15<02:00, 1478.44 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 13000/186688 [00:16<01:23, 2075.61 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         | 15000/186688 [00:18<01:23, 2055.43 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:18<01:17, 2215.22 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         | Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:12<40:01, 77.32 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<18:24, 167.18 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:14<10:37, 288.29 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<07:10, 424.12 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:16<03:25, 874.55 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:17<02:03, 1428.52 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:17<01:39, 1756.68 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         | 15000/186688 [00:18<01:14, 2315.29 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         | 1Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:14<44:23, 69.70 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:14<11:54, 257.25 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<08:13, 370.45 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 6000/186688 [00:15<04:27, 676.61 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▍         | 8000/186688 [00:16<03:03, 972.12 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▌         | 11000/186688 [00:16<01:48, 1626.20 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:17<01:15, 2293.27 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         | 15000/186688 [00:18<01:22, 2076.33 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         |Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:13<43:14, 71.58 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:14<18:48, 163.66 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:15<11:02, 277.47 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<07:25, 410.28 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:16<05:15, 575.34 examples/s]Running tokenizer on dataset (num_proc=64):   4%|▎         | 7000/186688 [00:16<02:54, 1030.77 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▌         | 10000/186688 [00:17<01:42, 1728.93 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▌         | 11000/186688 [00:18<01:43, 1694.46 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 13Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:14<46:04, 67.16 examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 2000/186688 [00:15<19:48, 155.40 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 4000/186688 [00:15<07:47, 390.42 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 6000/186688 [00:16<04:25, 679.91 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:16<02:19, 1272.89 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:17<01:10, 2443.77 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         | 15000/186688 [00:18<01:25, 2010.30 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         | 19000/186688 [00:18<00:56, 2982.83 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | Running tokenizer on dataset (num_proc=64):   0%|          | 0/186688 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1000/186688 [00:15<47:02, 65.79 examples/s]Running tokenizer on dataset (num_proc=64):   2%|▏         | 3000/186688 [00:15<12:39, 241.76 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 5000/186688 [00:16<06:29, 467.03 examples/s]Running tokenizer on dataset (num_proc=64):   3%|▎         | 6000/186688 [00:16<05:09, 584.39 examples/s]Running tokenizer on dataset (num_proc=64):   5%|▍         | 9000/186688 [00:17<02:37, 1128.96 examples/s]Running tokenizer on dataset (num_proc=64):   6%|▋         | 12000/186688 [00:17<01:41, 1729.10 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 13000/186688 [00:18<01:38, 1760.67 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         | 15000/186688 [00:18<01:20, 2123.91 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         7000/186688 [00:18<01:05, 2584.34 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:19<01:10, 2392.93 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         | 19000/186688 [00:19<01:00, 2770.25 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 20000/186688 [00:19<00:54, 3054.22 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:20<00:54, 3043.89 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:20<00:46, 3574.38 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:20<00:49, 3311.65 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:20<00:41, 3876.94 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:20<00:42, 3826.34 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:21<00:36000/186688 [00:18<01:18, 2204.70 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:18<01:06, 2582.42 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:19<00:58, 2880.88 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:19<00:39, 4135.76 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:20<00:45, 3567.21 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:20<00:52, 3119.30 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:21<00:55, 2901.66 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:22<00:42, 3700.56 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:22<00:38, 4079.85 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:23<00000/186688 [00:18<01:18, 2222.74 examples/s]Running tokenizer on dataset (num_proc=64):   8%|▊         | 15000/186688 [00:18<01:03, 2715.08 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:19<00:46, 3601.57 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:19<00:39, 4243.70 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:20<00:59, 2759.49 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:21<00:46, 3488.60 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:21<00:44, 3587.65 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:21<00:33, 4644.23 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:22<00:59, 2631.29 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:23<00:4000/186688 [00:19<01:37, 1785.37 examples/s]Running tokenizer on dataset (num_proc=64):   7%|▋         | 14000/186688 [00:19<01:41, 1705.80 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▊         | 16000/186688 [00:20<01:20, 2123.79 examples/s]Running tokenizer on dataset (num_proc=64):   9%|▉         | 17000/186688 [00:20<01:23, 2025.69 examples/s]Running tokenizer on dataset (num_proc=64):  10%|█         | 19000/186688 [00:21<01:13, 2293.59 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 20000/186688 [00:22<01:19, 2092.07 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 22000/186688 [00:22<01:02, 2652.49 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:23<00:53, 3045.52 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:23<00:53, 3017.11 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:23<00:45, 317000/186688 [00:18<01:17, 2184.37 examples/s]Running tokenizer on dataset (num_proc=64):  10%|▉         | 18000/186688 [00:19<01:19, 2124.58 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:20<01:19, 2071.53 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:21<01:00, 2695.75 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:21<01:04, 2519.23 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:22<00:42, 3704.18 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:23<00:45, 3394.34 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:24<01:02, 2485.94 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:24<01:11, 2158.72 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:25< 20000/186688 [00:18<00:50, 3299.47 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:19<00:48, 3384.58 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:19<00:46, 3544.78 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:21<01:24, 1920.62 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:22<01:06, 2389.62 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:24<01:18, 1984.77 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:24<01:16, 2048.21 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:25<01:15, 2052.61 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:25<01:06, 2310.90 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:21000/186688 [00:19<00:52, 3131.55 examples/s]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 23000/186688 [00:20<01:10, 2306.79 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 25000/186688 [00:21<01:05, 2466.53 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:21<00:58, 2749.85 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:21<00:45, 3518.26 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:22<00:40, 3911.17 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:22<00:55, 2837.59 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:23<01:19, 1948.67 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:23<01:05, 2378.40 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00, 4347.56 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:21<00:47, 3353.47 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:22<01:03, 2480.41 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:22<00:50, 3103.71 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:24<01:58, 1327.22 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:24<01:38, 1580.25 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:25<01:56, 1330.83 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:25<01:30, 1700.81 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:26<01:31, 1662.27 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:27<01:20, 1872.82 exampl545.04 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 27000/186688 [00:23<00:45, 3471.75 examples/s]Running tokenizer on dataset (num_proc=64):  15%|█▍        | 28000/186688 [00:24<01:10, 2257.01 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:25<01:16, 2052.20 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 30000/186688 [00:25<01:04, 2412.89 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 31000/186688 [00:25<00:51, 3009.90 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:25<00:46, 3335.15 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:26<00:39, 3935.56 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:26<00:57, 2642.37 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:28<01:46, 1422.37 examples/7, 3237.87 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 33000/186688 [00:23<00:45, 3403.60 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:24<00:59, 2566.27 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:24<00:52, 2870.27 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:24<00:58, 2578.01 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:27<02:14, 1110.67 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:28<02:38, 936.53 examples/s] Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:29<02:21, 1041.92 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:29<01:54, 1283.80 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:30<01:12, 1998.80 e| 19000/186688 [00:19<00:51, 3250.62 examples/s]Running tokenizer on dataset (num_proc=64):  11%|█         | 21000/186688 [00:20<01:00, 2721.17 examples/s]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 24000/186688 [00:21<00:48, 3336.17 examples/s]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 26000/186688 [00:21<00:46, 3442.57 examples/s]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 29000/186688 [00:22<00:40, 3938.82 examples/s]Running tokenizer on dataset (num_proc=64):  17%|█▋        | 32000/186688 [00:22<00:35, 4410.09 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:23<00:35, 4311.70 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:27<01:59, 1259.26 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:29<02:23, 1039.89 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:39, 3889.61 examples/s]Running tokenizer on dataset (num_proc=64):  18%|█▊        | 34000/186688 [00:25<01:27, 1751.47 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:26<01:58, 1275.79 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:27<01:48, 1384.70 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:28<02:04, 1199.91 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:28<01:46, 1393.94 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:29<01:57, 1255.50 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:30<01:41, 1442.77 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:30<01:19, 1837.25 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:30<01:02, 2329.es/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:27<01:07, 2211.58 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:28<01:14, 1990.64 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:28<01:15, 1943.70 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:29<01:08, 2148.61 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:30<01:45, 1386.77 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:30<01:36, 1496.62 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:31<01:43, 1384.16 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:31<01:20, 1772.66 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<01:22, 1717.72 examples/ss]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:28<01:35, 1575.46 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:28<01:14, 2007.84 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:29<01:11, 2076.24 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:29<01:16, 1921.79 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:30<01:12, 2027.23 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:30<01:03, 2312.29 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:31<01:10, 2041.38 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:31<01:10, 2043.86 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:31<00:54, 2633.56 examples/s]Run01:14, 2039.20 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:27<01:35, 1578.29 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:27<01:29, 1670.00 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:30<03:00, 822.45 examples/s] Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:31<02:22, 1035.75 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:31<01:33, 1561.74 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:31<01:18, 1842.20 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:32<01:31, 1574.47 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:32<01:13, 1933.57 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:33<01:07,26<01:23, 1833.86 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:26<01:10, 2134.33 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:27<00:46, 3154.86 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:27<00:46, 3155.24 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:28<00:57, 2544.83 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:28<00:49, 2911.80 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:29<01:10, 2044.99 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:30<01:36, 1485.55 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<02:27, 962.52 examples/s] Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:33:26<02:23, 1071.64 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▊        | 35000/186688 [00:27<02:06, 1200.85 examples/s]Running tokenizer on dataset (num_proc=64):  19%|█▉        | 36000/186688 [00:27<01:45, 1424.27 examples/s]Running tokenizer on dataset (num_proc=64):  20%|█▉        | 37000/186688 [00:28<01:48, 1379.13 examples/s]Running tokenizer on dataset (num_proc=64):  20%|██        | 38000/186688 [00:30<02:27, 1005.87 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:31<02:30, 981.49 examples/s] Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:31<01:32, 1570.87 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:32<01:15, 1910.55 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:33<01:37, 1467.08 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:34<01:4xamples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:30<01:23, 1721.80 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:32<01:43, 1373.67 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<01:40, 1403.80 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:33<01:23, 1694.01 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<01:07, 2078.50 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:33<01:08, 2038.66 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:34<00:58, 2345.02 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:34<00:50, 2688.16 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:34<01:00, 2248.10:30<02:16, 1091.02 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██        | 39000/186688 [00:31<02:12, 1112.94 examples/s]Running tokenizer on dataset (num_proc=64):  21%|██▏       | 40000/186688 [00:32<02:15, 1079.57 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 41000/186688 [00:32<01:58, 1225.60 examples/s]Running tokenizer on dataset (num_proc=64):  22%|██▏       | 42000/186688 [00:33<01:39, 1447.85 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:33<01:31, 1574.28 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:34<01:39, 1429.53 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:34<01:23, 1687.53 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:34<01:05, 2152.25 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [0040 examples/s]Running tokenizer on dataset (num_proc=64):  23%|██▎       | 43000/186688 [00:31<01:32, 1557.12 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▎       | 44000/186688 [00:31<01:09, 2057.39 examples/s]Running tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<01:11, 1978.62 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:32<00:57, 2435.96 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<01:11, 1959.04 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:33<00:55, 2490.76 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:33<00:47, 2897.41 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:34<00:50, 2690.83 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:35<01:13, 184]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:33<01:19, 1776.18 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<00:59, 2349.90 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:33<00:55, 2478.30 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:34<00:36, 3706.54 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:34<00:33, 3986.28 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:34<00:32, 4126.48 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:35<00:49, 2700.53 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:35<00:54, 2415.60 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:35<00:36, 3546.61 examples 2113.88 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:33<01:03, 2203.12 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<00:52, 2643.51 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:34<00:55, 2478.32 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:34<00:51, 2681.41 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:35<01:03, 2159.18 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:35<00:42, 3163.85 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:35<00:38, 3462.92 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:36<00:38, 3426.54 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:36<00:34, 1367.18 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▍       | 46000/186688 [00:34<01:10, 1995.29 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:35<01:14, 1883.09 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:35<01:07, 2055.94 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:36<01:17, 1773.35 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:36<00:47, 2832.81 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:37<00:45, 2967.32 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:37<00:47, 2766.40 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:37<00:42, 3095.85 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:37<00ning tokenizer on dataset (num_proc=64):  24%|██▍       | 45000/186688 [00:32<01:17, 1822.42 examples/s]Running tokenizer on dataset (num_proc=64):  25%|██▌       | 47000/186688 [00:33<00:56, 2453.09 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:34<01:35, 1459.62 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:36<01:54, 1205.89 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:36<01:38, 1385.22 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:36<00:59, 2253.49 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:37<00:55, 2410.11 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:37<00:44, 2972.26 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:37<00:49, 2636.89 examples/s]R<02:12, 1064.16 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 48000/186688 [00:34<01:45, 1309.58 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:35<01:46, 1296.48 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:35<01:18, 1731.78 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:36<01:17, 1732.98 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:36<01:04, 2082.08 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:37<01:12, 1835.61 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:37<01:02, 2105.03 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:38<01:06, 1971.72 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00::35<00:55, 2509.53 examples/s]Running tokenizer on dataset (num_proc=64):  26%|██▌       | 49000/186688 [00:35<00:54, 2547.81 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 50000/186688 [00:36<00:45, 2984.63 examples/s]Running tokenizer on dataset (num_proc=64):  27%|██▋       | 51000/186688 [00:36<00:45, 2979.18 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 52000/186688 [00:36<00:51, 2591.68 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:37<00:37, 3515.30 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:37<00:41, 3200.54 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:38<01:07, 1949.85 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:38<00:57, 2262.76 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:35<00:39, 3421.04 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 54000/186688 [00:35<00:37, 3535.25 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:35<00:34, 3817.91 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:37<00:59, 2163.80 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:38<00:45, 2732.70 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:38<00:46, 2661.34 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:39<00:40, 3022.37 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:39<00:48, 2508.34 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:40<00/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:36<00:26, 4844.22 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:36<00:27, 4572.59 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:36<00:21, 5757.46 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:37<00:21, 5631.89 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:38<00:38, 3080.45 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:39<00:47, 2508.43 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:39<00:42, 2758.17 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:39<00:43, 2662.71 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:40<00:2.01 examples/s]Running tokenizer on dataset (num_proc=64):  28%|██▊       | 53000/186688 [00:35<01:04, 2084.63 examples/s]Running tokenizer on dataset (num_proc=64):  29%|██▉       | 55000/186688 [00:36<00:39, 3303.98 examples/s]Running tokenizer on dataset (num_proc=64):  30%|██▉       | 56000/186688 [00:37<01:12, 1804.43 examples/s]Running tokenizer on dataset (num_proc=64):  30%|███       | 56917/186688 [00:38<01:22, 1566.60 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57917/186688 [00:39<01:29, 1446.05 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 58917/186688 [00:39<01:08, 1865.70 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59917/186688 [00:40<01:21, 1553.05 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 60917/186688 [00:40<01:10, 1791.41 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 62917/186688 [00:40<9, 3333.55 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:37<01:01, 2095.34 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:37<00:55, 2320.47 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:38<00:38, 3333.58 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:38<00:42, 2941.91 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:38<00:36, 3445.95 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:39<00:37, 3300.05 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:40<00:47, 2548.13 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:40<00:47, 2518.23 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/18 [00:39<00:50, 2539.06 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:39<00:46, 2734.63 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:40<00:43, 2908.69 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:40<00:38, 3212.25 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:40<00:25, 4741.02 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:40<00:25, 4761.35 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 66000/186688 [00:40<00:22, 5344.03 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:40<00:20, 5816.68 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:41<00:19, 5997.95 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋unning tokenizer on dataset (num_proc=64):  31%|███       | 57000/186688 [00:38<00:38, 3330.65 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:38<00:48, 2629.80 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:38<00:39, 3203.22 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:38<00:32, 3841.49 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:39<00:35, 3566.47 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:40<01:03, 1959.43 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:41<01:02, 1955.55 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 65000/186688 [00:41<00:58, 2097.05 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:41<00:36, 3246.:38, 3388.23 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 58000/186688 [00:38<00:49, 2623.57 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59000/186688 [00:39<00:42, 3001.33 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 60000/186688 [00:39<00:37, 3354.66 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61000/186688 [00:39<00:38, 3246.38 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 62000/186688 [00:40<00:45, 2729.24 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 63000/186688 [00:40<00:49, 2490.15 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 64000/186688 [00:40<00:38, 3151.04 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:41<00:27, 4284.33 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 700038<00:59, 2179.31 examples/s]Running tokenizer on dataset (num_proc=64):  31%|███       | 57917/186688 [00:38<00:56, 2260.79 examples/s]Running tokenizer on dataset (num_proc=64):  32%|███▏      | 59917/186688 [00:39<00:33, 3764.21 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 60917/186688 [00:39<00:28, 4390.11 examples/s]Running tokenizer on dataset (num_proc=64):  33%|███▎      | 61917/186688 [00:40<00:51, 2437.79 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▎      | 62917/186688 [00:40<00:46, 2685.30 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 63917/186688 [00:40<00:37, 3304.16 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 64917/186688 [00:40<00:40, 2984.06 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 65917/186688 [00:41<00:55, 2190.37 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      |:46, 2571.64 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 67000/186688 [00:40<00:40, 2963.45 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:41<01:14, 1589.07 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 70917/186688 [00:42<00:40, 2886.87 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 72917/186688 [00:42<00:28, 3971.00 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 73917/186688 [00:42<00:26, 4304.85 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 74917/186688 [00:42<00:25, 4323.03 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 75917/186688 [00:42<00:23, 4665.24 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 77917/186688 [00:42<00:17, 6171.57 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     00:41, 2962.72 examples/s]Running tokenizer on dataset (num_proc=64):  34%|███▍      | 63917/186688 [00:40<00:34, 3524.26 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▍      | 64917/186688 [00:41<00:33, 3658.26 examples/s]Running tokenizer on dataset (num_proc=64):  35%|███▌      | 65917/186688 [00:41<00:33, 3588.16 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▌      | 66917/186688 [00:41<00:27, 4338.96 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 67917/186688 [00:41<00:36, 3286.90 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 68917/186688 [00:42<00:51, 2282.26 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69917/186688 [00:42<00:39, 2924.93 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 70917/186688 [00:43<00:47, 2427.34 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 6688 [00:40<00:36, 3250.71 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:41<00:34, 3380.43 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:41<00:29, 3908.91 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:41<00:35, 3282.96 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:41<00:30, 3795.17 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:42<00:23, 4828.14 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:43<00:31, 3567.73 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:43<00:30, 3559.23 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78000/186688 [00:43<00:28, 3751.67 examples/s]Running tokenizer on dataset (num_proc=64):  42%|██ 67917/186688 [00:41<00:37, 3143.38 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 68917/186688 [00:42<00:31, 3721.97 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69917/186688 [00:42<00:28, 4103.80 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 70917/186688 [00:42<00:30, 3783.68 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 72917/186688 [00:42<00:19, 5764.78 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 73917/186688 [00:42<00:24, 4626.58 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 74917/186688 [00:43<00:21, 5206.20 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 75917/186688 [00:43<00:25, 4394.33 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76917/186688 [00:44<00:36, 3031.66 examples/s]Running tokenizer on dataset (num_proc=64):  42%53, 2142.89 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:41<00:46, 2464.82 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:41<00:43, 2600.14 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:42<00:46, 2409.79 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:43<01:10, 1578.23 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:43<00:55, 1977.98 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78000/186688 [00:44<00:58, 1853.96 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 79000/186688 [00:44<00:53, 2009.83 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80000/186688 [00:44<00:43, 2469.66 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍        | 69000/186688 [00:41<00:23, 5056.92 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:41<00:25, 4535.19 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:41<00:28, 4120.94 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:42<00:24, 4763.32 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:42<00:23, 4739.57 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:42<00:27, 4087.09 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:43<00:43, 2586.64 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:43<00:33, 3264.82 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78000/186688 [00:44<00:30, 3544.74 examples/s]Running tokenizer on dataset (num_proc=0/186688 [00:41<00:27, 4296.27 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:42<00:25, 4546.79 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:42<00:29, 3928.10 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:42<00:25, 4404.21 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:42<00:22, 5053.88 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:42<00:20, 5432.18 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/186688 [00:44<00:59, 1869.20 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:44<00:48, 2250.83 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78000/186688 [00:45<00:54, 1988.84 examples/s]Running tokenizer on dataset (num_proc=64):  42%|64 examples/s]Running tokenizer on dataset (num_proc=64):  36%|███▋      | 68000/186688 [00:41<00:31, 3791.96 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 69000/186688 [00:42<00:46, 2523.38 examples/s]Running tokenizer on dataset (num_proc=64):  37%|███▋      | 70000/186688 [00:43<00:40, 2867.48 examples/s]Running tokenizer on dataset (num_proc=64):  38%|███▊      | 71000/186688 [00:43<00:44, 2578.20 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▊      | 72000/186688 [00:44<00:49, 2304.96 examples/s]Running tokenizer on dataset (num_proc=64):  39%|███▉      | 73000/186688 [00:44<00:47, 2390.38 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 74000/186688 [00:45<01:08, 1642.33 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 75000/186688 [00:46<01:08, 1628.30 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76000/18668871917/186688 [00:43<00:39, 2941.39 examples/s]Running tokenizer on dataset (num_proc=64):  40%|███▉      | 73917/186688 [00:44<00:34, 3244.34 examples/s]Running tokenizer on dataset (num_proc=64):  40%|████      | 74834/186688 [00:44<00:41, 2703.31 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 76834/186688 [00:45<00:35, 3098.89 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 77834/186688 [00:45<00:30, 3590.92 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78834/186688 [00:45<00:27, 3860.09 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 79834/186688 [00:45<00:26, 4084.30 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80834/186688 [00:46<00:28, 3681.93 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 81834/186688 [00:46<00:26, 4008.18 examples/s]Running tokenizer on dataset (num_proc=| 78917/186688 [00:42<00:16, 6462.24 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 79917/186688 [00:43<00:34, 3094.31 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80917/186688 [00:44<00:31, 3346.75 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 81917/186688 [00:44<00:36, 2852.59 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83834/186688 [00:44<00:23, 4365.46 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84834/186688 [00:44<00:22, 4566.65 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85751/186688 [00:45<00:23, 4298.21 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▋     | 86751/186688 [00:45<00:28, 3564.90 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87668/186688 [00:46<00:41, 2409.73 examples/s]Running tokenizer on dataset (n|████▏     | 77917/186688 [00:44<00:47, 2313.17 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 78917/186688 [00:44<00:41, 2615.06 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 79917/186688 [00:45<00:36, 2957.33 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80917/186688 [00:45<00:28, 3689.86 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82917/186688 [00:45<00:26, 3978.62 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83917/186688 [00:46<00:26, 3865.17 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84917/186688 [00:46<00:38, 2675.83 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85917/186688 [00:47<00:36, 2758.58 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 86834/186688 [00:47<00:39, 2557.20 examples/s]Running to   | 82000/186688 [00:45<00:35, 2956.66 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 83000/186688 [00:45<00:42, 2422.91 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 84000/186688 [00:46<00:43, 2382.35 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85000/186688 [00:46<00:41, 2462.52 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 86917/186688 [00:46<00:31, 3176.98 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87917/186688 [00:47<00:30, 3289.47 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 88917/186688 [00:48<00:53, 1831.48 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89917/186688 [00:48<00:50, 1925.03 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90917/186688 [00:49<00:39, 2447.00 examples/s]Running tokenizer on dataset [00:46<00:51, 2146.61 examples/s]Running tokenizer on dataset (num_proc=64):  41%|████      | 77000/186688 [00:46<00:40, 2730.34 examples/s]Running tokenizer on dataset (num_proc=64):  42%|████▏     | 79000/186688 [00:47<00:40, 2653.06 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80000/186688 [00:47<00:38, 2802.43 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 81000/186688 [00:47<00:34, 3074.94 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 83000/186688 [00:47<00:25, 4106.61 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 84000/186688 [00:48<00:39, 2572.03 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85000/186688 [00:49<00:52, 1935.47 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 86000/186688 [00:49<00:42, 2382.71 examples/s]Running tokenizer on dataset (num_proc=64):  47██▏     | 78917/186688 [00:43<00:25, 4249.83 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 79917/186688 [00:45<01:00, 1768.19 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 81917/186688 [00:45<00:39, 2679.70 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82834/186688 [00:46<00:49, 2090.08 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83751/186688 [00:47<01:19, 1292.16 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▋     | 86751/186688 [00:47<00:40, 2444.51 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 88668/186688 [00:48<00:35, 2799.43 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89668/186688 [00:50<00:59, 1634.27 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90668/186688 [00:50<00:50, 1916.17 examples/s]Running tokenizer███▏     | 78917/186688 [00:45<00:53, 2007.14 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 79917/186688 [00:46<00:50, 2125.14 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80917/186688 [00:46<00:39, 2678.34 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82917/186688 [00:46<00:25, 4101.34 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 83834/186688 [00:47<00:39, 2624.03 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85834/186688 [00:48<00:41, 2450.19 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87834/186688 [00:49<00:49, 2001.98 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89751/186688 [00:49<00:35, 2704.90 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90751/186688 [00:50<00:49, 1957.41 examples/s]Running token64):  42%|████▏     | 79000/186688 [00:45<00:56, 1922.41 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 80000/186688 [00:45<00:46, 2300.61 examples/s]Running tokenizer on dataset (num_proc=64):  43%|████▎     | 81000/186688 [00:45<00:43, 2421.48 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 82000/186688 [00:45<00:36, 2872.25 examples/s]Running tokenizer on dataset (num_proc=64):  44%|████▍     | 83000/186688 [00:46<00:43, 2365.53 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▍     | 84000/186688 [00:46<00:36, 2797.37 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85000/186688 [00:48<01:20, 1258.03 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 86000/186688 [00:48<01:00, 1654.16 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 87000/186688 [00:49<01:08, 1460.88 examples/s]R64):  44%|████▍     | 82834/186688 [00:46<00:34, 2976.92 examples/s]Running tokenizer on dataset (num_proc=64):  45%|████▌     | 84834/186688 [00:46<00:22, 4465.93 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▌     | 85834/186688 [00:47<00:25, 4032.64 examples/s]Running tokenizer on dataset (num_proc=64):  46%|████▋     | 86751/186688 [00:47<00:26, 3741.55 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 88751/186688 [00:47<00:18, 5161.43 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89751/186688 [00:47<00:18, 5259.04 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90751/186688 [00:48<00:34, 2773.00 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91751/186688 [00:49<00:35, 2697.94 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92751/186688 [00:50<00:58, 1614.06 examples/s]Rkenizer on dataset (num_proc=64):  47%|████▋     | 87834/186688 [00:47<00:31, 3096.23 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 88751/186688 [00:47<00:26, 3727.34 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89751/186688 [00:48<00:27, 3488.41 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▊     | 90751/186688 [00:49<00:53, 1789.84 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91668/186688 [00:50<01:13, 1297.01 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92668/186688 [00:50<00:53, 1745.33 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94585/186688 [00:51<00:45, 2039.00 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95585/186688 [00:52<00:54, 1660.65 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96502/186688 [00:um_proc=64):  47%|████▋     | 88668/186688 [00:47<00:58, 1682.92 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 90502/186688 [00:47<00:45, 2131.02 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91502/186688 [00:47<00:36, 2608.84 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92502/186688 [00:49<00:53, 1754.73 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93502/186688 [00:50<01:20, 1162.85 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94502/186688 [00:51<01:13, 1257.15 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95419/186688 [00:51<00:56, 1608.78 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96336/186688 [00:51<00:44, 2019.55 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97253/186688 [00:52<01:05, 1375.38 e%|████▋     | 87000/186688 [00:50<00:49, 1997.21 examples/s]Running tokenizer on dataset (num_proc=64):  47%|████▋     | 88000/186688 [00:50<00:41, 2350.66 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89000/186688 [00:51<00:36, 2645.37 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 90000/186688 [00:51<00:41, 2312.17 examples/s]Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91917/186688 [00:51<00:26, 3622.94 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92917/186688 [00:52<00:27, 3370.36 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94917/186688 [00:52<00:23, 3866.22 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████▏    | 95917/186688 [00:52<00:21, 4175.95 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96917/186688 [00:53<00:25, 3530.38 examples/s]Runni (num_proc=64):  49%|████▉     | 91917/186688 [00:49<00:40, 2327.48 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92917/186688 [00:49<00:37, 2478.85 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93834/186688 [00:50<00:31, 2981.88 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████▏    | 95834/186688 [00:51<00:43, 2068.11 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96834/186688 [00:51<00:41, 2146.75 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97751/186688 [00:52<00:49, 1800.47 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98668/186688 [00:52<00:45, 1919.97 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99668/186688 [00:53<00:39, 2194.54 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 100585/186688 [00:53<00:3 on dataset (num_proc=64):  50%|████▉     | 92668/186688 [00:50<00:35, 2648.06 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94668/186688 [00:51<00:30, 3049.62 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95585/186688 [00:51<00:30, 3034.67 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96585/186688 [00:51<00:26, 3400.21 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97585/186688 [00:51<00:26, 3407.65 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98502/186688 [00:52<00:37, 2376.11 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99419/186688 [00:52<00:37, 2349.05 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 100419/186688 [00:53<00:48, 1789.85 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101336/186688unning tokenizer on dataset (num_proc=64):  50%|█████     | 93751/186688 [00:51<01:07, 1368.20 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94751/186688 [00:52<01:10, 1299.47 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████▏    | 95751/186688 [00:53<01:22, 1102.94 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96668/186688 [00:54<01:13, 1229.78 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97668/186688 [00:54<00:59, 1505.86 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98585/186688 [00:54<00:47, 1859.59 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 100419/186688 [00:54<00:28, 3068.80 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101336/186688 [00:54<00:24, 3539.95 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍izer on dataset (num_proc=64):  49%|████▉     | 91751/186688 [00:50<00:42, 2230.45 examples/s]Running tokenizer on dataset (num_proc=64):  50%|█████     | 93751/186688 [00:51<00:34, 2729.08 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 94668/186688 [00:52<01:00, 1511.57 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95585/186688 [00:53<00:54, 1671.04 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 96502/186688 [00:53<00:48, 1856.02 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97502/186688 [00:54<00:57, 1553.46 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98502/186688 [00:55<00:58, 1502.09 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99502/186688 [00:55<00:48, 1796.76 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101336/1866852<00:47, 1896.62 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97502/186688 [00:53<00:54, 1633.71 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98502/186688 [00:53<00:46, 1907.84 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99502/186688 [00:54<00:40, 2157.95 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 100502/186688 [00:54<00:40, 2113.62 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102336/186688 [00:54<00:30, 2807.11 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103253/186688 [00:55<00:28, 2974.66 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104253/186688 [00:55<00:30, 2698.31 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▋    | 105253/186688 [00:55<00:29, 2777.83 examples/s]Running tokenizer on dataset xamples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98170/186688 [00:53<00:57, 1528.00 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99087/186688 [00:53<00:45, 1906.88 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 100921/186688 [00:53<00:31, 2690.86 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 102921/186688 [00:54<00:23, 3623.25 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 105921/186688 [00:54<00:15, 5252.18 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 107755/186688 [00:54<00:13, 5868.80 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109755/186688 [00:54<00:11, 6412.84 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111672/186688 [00:55<00:11, 6659.62 examples/s]Running tokenizer on dataset (num_proc=64):  61unning tokenizer on dataset (num_proc=64):  47%|████▋     | 87917/186688 [00:50<01:27, 1123.77 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 88834/186688 [00:51<01:12, 1346.66 examples/s]Running tokenizer on dataset (num_proc=64):  48%|████▊     | 89834/186688 [00:53<01:47, 903.22 examples/s] Running tokenizer on dataset (num_proc=64):  49%|████▉     | 91751/186688 [00:54<01:16, 1238.54 examples/s]Running tokenizer on dataset (num_proc=64):  50%|████▉     | 92751/186688 [00:54<01:03, 1486.89 examples/s]Running tokenizer on dataset (num_proc=64):  51%|█████     | 95585/186688 [00:54<00:35, 2587.94 examples/s]Running tokenizer on dataset (num_proc=64):  52%|█████▏    | 97502/186688 [00:56<00:48, 1823.64 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98419/186688 [00:56<00:42, 2069.74 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 99339, 2156.67 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101585/186688 [00:54<00:47, 1787.63 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103419/186688 [00:54<00:29, 2800.94 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104336/186688 [00:55<00:40, 2030.31 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 107170/186688 [00:56<00:34, 2275.79 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108087/186688 [00:56<00:30, 2568.80 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 109004/186688 [00:57<00:32, 2410.27 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110838/186688 [00:57<00:22, 3346.33 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112755/186688 [00:57<00:16, 4606.72 examples/s]Running tokenizer on dataset (num [00:54<00:49, 1711.18 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104170/186688 [00:54<00:28, 2944.72 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▋    | 105170/186688 [00:55<00:32, 2511.22 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106087/186688 [00:55<00:27, 2930.38 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 107087/186688 [00:55<00:22, 3486.84 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108004/186688 [00:56<00:27, 2840.57 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109921/186688 [00:56<00:18, 4098.86 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110921/186688 [00:56<00:21, 3501.04 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111838/186688 [00:57<00:29, 2518.42 examples/s]Running tokenizer on ng tokenizer on dataset (num_proc=64):  52%|█████▏    | 97917/186688 [00:53<00:28, 3072.36 examples/s]Running tokenizer on dataset (num_proc=64):  53%|█████▎    | 98917/186688 [00:54<00:31, 2800.71 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▎    | 99917/186688 [00:55<00:46, 1858.37 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 100834/186688 [00:55<00:46, 1841.35 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 101834/186688 [00:56<00:48, 1741.41 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 103668/186688 [00:57<00:49, 1671.73 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 105502/186688 [00:57<00:36, 2228.65 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106419/186688 [00:58<00:33, 2413.22 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████8 [00:55<00:33, 2582.49 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103253/186688 [00:56<00:24, 3385.34 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104253/186688 [00:56<00:25, 3294.00 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▋    | 105170/186688 [00:56<00:22, 3563.61 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106170/186688 [00:57<00:24, 3350.47 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 107087/186688 [00:57<00:20, 3962.17 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108004/186688 [00:57<00:17, 4417.21 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 109004/186688 [00:57<00:26, 2906.89 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109921/186688 [00:58<00:22, 3440.56 examples/s]Running tokenizer on    | 102253/186688 [00:55<00:24, 3447.85 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▌    | 103253/186688 [00:56<00:40, 2063.78 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▌    | 104253/186688 [00:56<00:42, 1936.88 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▋    | 105170/186688 [00:56<00:34, 2390.02 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106170/186688 [00:57<00:28, 2838.94 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108087/186688 [00:57<00:30, 2594.75 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110004/186688 [00:58<00:21, 3545.69 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 110921/186688 [00:58<00:19, 3829.11 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111921/186688 [00:58<00:16, 4508.64 examples/s]Ru(num_proc=64):  57%|█████▋    | 106170/186688 [00:56<00:23, 3411.05 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 107087/186688 [00:56<00:19, 3994.87 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108004/186688 [00:57<00:34, 2279.50 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108921/186688 [00:57<00:33, 2332.87 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109921/186688 [00:57<00:26, 2951.28 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111838/186688 [00:57<00:18, 4048.84 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112755/186688 [00:58<00:20, 3622.83 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113672/186688 [00:58<00:20, 3542.34 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114589/186%|██████    | 113506/186688 [00:56<00:29, 2495.28 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117174/186688 [00:57<00:20, 3406.97 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118091/186688 [00:57<00:21, 3219.56 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119091/186688 [00:58<00:24, 2777.05 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120008/186688 [00:58<00:22, 2975.48 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 120925/186688 [00:58<00:19, 3316.10 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 121842/186688 [00:59<00:16, 3839.35 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 123759/186688 [00:59<00:12, 4854.91 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124759/186688 _proc=64):  61%|██████    | 113672/186688 [00:57<00:14, 4884.44 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114672/186688 [00:57<00:14, 4896.63 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116589/186688 [00:58<00:12, 5450.78 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117506/186688 [00:58<00:13, 4998.88 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118423/186688 [00:58<00:15, 4521.62 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120257/186688 [00:59<00:15, 4360.46 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121174/186688 [00:59<00:23, 2808.90 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122174/186688 [00:59<00:19, 3290.59 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | dataset (num_proc=64):  61%|██████    | 113672/186688 [00:57<00:20, 3586.16 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114589/186688 [00:58<00:24, 2890.59 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115506/186688 [00:58<00:24, 2951.45 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116506/186688 [00:59<00:27, 2588.23 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117423/186688 [00:59<00:23, 2923.27 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119257/186688 [00:59<00:16, 3989.69 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121091/186688 [01:00<00:21, 3009.11 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 122925/186688 [01:00<00:18, 3476.89 examples/s]Running tokenizer on dataset (num_proc=64):  66%|████6/186688 [00:57<00:44, 1977.59 examples/s]Running tokenizer on dataset (num_proc=64):  54%|█████▍    | 101170/186688 [00:57<00:29, 2935.60 examples/s]Running tokenizer on dataset (num_proc=64):  55%|█████▍    | 102170/186688 [00:57<00:34, 2434.96 examples/s]Running tokenizer on dataset (num_proc=64):  56%|█████▋    | 105087/186688 [00:58<00:28, 2873.93 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 106087/186688 [00:59<00:30, 2633.48 examples/s]Running tokenizer on dataset (num_proc=64):  57%|█████▋    | 107004/186688 [00:59<00:31, 2527.64 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 107921/186688 [00:59<00:29, 2695.09 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108838/186688 [01:00<00:32, 2370.66 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▉    | 109755/186688 [01:00<00:31, 2439.51 examples/s]Running token▊    | 107419/186688 [00:58<00:33, 2372.99 examples/s]Running tokenizer on dataset (num_proc=64):  58%|█████▊    | 108419/186688 [00:58<00:27, 2892.16 examples/s]Running tokenizer on dataset (num_proc=64):  59%|█████▊    | 109336/186688 [00:59<00:35, 2176.38 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112087/186688 [00:59<00:20, 3672.64 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113004/186688 [00:59<00:20, 3537.96 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113921/186688 [01:00<00:27, 2654.02 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 114838/186688 [01:00<00:24, 2911.33 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 116755/186688 [01:01<00:19, 3636.48 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117672/186688 [01:01<00:19, 3472.07 examp688 [00:58<00:19, 3683.39 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116506/186688 [00:59<00:16, 4144.48 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117423/186688 [00:59<00:15, 4523.61 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 118423/186688 [00:59<00:22, 2997.34 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119423/186688 [01:00<00:21, 3169.23 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120340/186688 [01:00<00:29, 2271.43 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122174/186688 [01:00<00:17, 3599.93 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 124091/186688 [01:01<00:13, 4813.94 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125008/186688 [01:01<00:15, 3932.37 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112921/186688 [00:58<00:14, 5075.91 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113838/186688 [00:59<00:23, 3131.90 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115672/186688 [00:59<00:19, 3617.19 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▎   | 118589/186688 [01:00<00:15, 4523.92 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119506/186688 [01:01<00:26, 2537.46 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121340/186688 [01:01<00:18, 3505.10 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122257/186688 [01:01<00:19, 3363.75 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 123174/186688 [01:01<00:17, 3662.04 examples/s]Running tokenizer on dataset (num_proc=64):  6 dataset (num_proc=64):  59%|█████▉    | 110838/186688 [00:58<00:26, 2848.99 examples/s]Running tokenizer on dataset (num_proc=64):  60%|█████▉    | 111755/186688 [00:58<00:26, 2863.29 examples/s]Running tokenizer on dataset (num_proc=64):  60%|██████    | 112672/186688 [00:59<00:30, 2392.31 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113589/186688 [00:59<00:26, 2731.60 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114506/186688 [00:59<00:22, 3154.48 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115506/186688 [01:00<00:28, 2526.59 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 116506/186688 [01:00<00:26, 2681.92 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117506/186688 [01:02<00:47, 1445.95 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████123091/186688 [01:00<00:18, 3435.36 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▋   | 124008/186688 [01:00<00:15, 4010.58 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124925/186688 [01:00<00:14, 4385.30 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125842/186688 [01:00<00:12, 4886.62 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126842/186688 [01:00<00:13, 4570.07 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129593/186688 [01:01<00:07, 7486.39 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132344/186688 [01:01<00:07, 7264.83 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133261/186688 [01:01<00:07, 6794.53 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134261/186688 [01:02<00:19, 2663.46 [00:59<00:13, 4641.08 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125759/186688 [01:00<00:21, 2791.54 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127593/186688 [01:00<00:14, 4147.54 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128510/186688 [01:00<00:14, 3982.41 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130344/186688 [01:01<00:22, 2527.63 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131261/186688 [01:02<00:21, 2627.73 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132178/186688 [01:02<00:17, 3028.53 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133095/186688 [01:02<00:19, 2797.90 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134012/186688 [01:03<00:21, 2488.65 examples/s]Ruizer on dataset (num_proc=64):  60%|█████▉    | 111589/186688 [01:01<00:28, 2660.64 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████    | 113506/186688 [01:01<00:19, 3724.02 examples/s]Running tokenizer on dataset (num_proc=64):  61%|██████▏   | 114506/186688 [01:01<00:16, 4268.44 examples/s]Running tokenizer on dataset (num_proc=64):  62%|██████▏   | 115423/186688 [01:01<00:14, 4774.72 examples/s]Running tokenizer on dataset (num_proc=64):  63%|██████▎   | 117340/186688 [01:02<00:15, 4423.14 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119174/186688 [01:02<00:13, 4978.16 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120091/186688 [01:02<00:12, 5187.40 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122008/186688 [01:03<00:14, 4356.23 examples/s]Running tokenizer on dataset (num_proc=64):  66%|████▋   | 123842/186688 [01:01<00:21, 2953.38 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124842/186688 [01:01<00:22, 2752.87 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125759/186688 [01:01<00:19, 3132.14 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126676/186688 [01:02<00:18, 3308.27 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129510/186688 [01:02<00:11, 5167.71 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130427/186688 [01:02<00:12, 4490.30 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131344/186688 [01:02<00:13, 4062.82 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132261/186688 [01:03<00:12, 4470.71 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133178/186688 [01:03<00:15les/s]Running tokenizer on dataset (num_proc=64):  64%|██████▎   | 118589/186688 [01:01<00:18, 3649.95 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▍   | 121340/186688 [01:01<00:10, 6520.83 examples/s]Running tokenizer on dataset (num_proc=64):  66%|██████▌   | 123174/186688 [01:01<00:08, 7253.96 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125174/186688 [01:02<00:09, 6195.37 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126091/186688 [01:02<00:14, 4243.75 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127008/186688 [01:03<00:13, 4440.33 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▊   | 128008/186688 [01:03<00:12, 4688.66 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128925/186688 [01:03<00:20, 2878.85 examples/s]Running tokenizer on dataset (num   | 118423/186688 [01:02<00:38, 1795.79 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 119340/186688 [01:02<00:41, 1627.42 examples/s]Running tokenizer on dataset (num_proc=64):  64%|██████▍   | 120257/186688 [01:03<00:31, 2140.66 examples/s]Running tokenizer on dataset (num_proc=64):  65%|██████▌   | 122091/186688 [01:03<00:18, 3484.13 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 124842/186688 [01:03<00:10, 5782.75 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126676/186688 [01:03<00:08, 6668.39 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128593/186688 [01:03<00:07, 7777.85 examples/s]Running tokenizer on dataset (num_proc=64):  70%|██████▉   | 130427/186688 [01:03<00:07, 7819.57 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132261/186688 [01:04<00:08, 6768.nning tokenizer on dataset (num_proc=64):  67%|██████▋   | 125925/186688 [01:01<00:14, 4167.34 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 126842/186688 [01:01<00:15, 3831.38 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128676/186688 [01:02<00:12, 4700.56 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 129593/186688 [01:02<00:11, 4764.32 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131510/186688 [01:03<00:13, 4074.66 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132427/186688 [01:03<00:18, 3002.33 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133344/186688 [01:03<00:18, 2846.66 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134261/186688 [01:04<00:16, 3226.43 examples/s]Running tokenizer on dataset (num_proc6%|██████▋   | 124091/186688 [01:02<00:21, 2977.77 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 126008/186688 [01:02<00:17, 3451.23 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127008/186688 [01:02<00:15, 3796.56 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▊   | 127925/186688 [01:03<00:27, 2157.89 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131593/186688 [01:04<00:13, 4088.67 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132510/186688 [01:04<00:15, 3561.45 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133427/186688 [01:04<00:13, 3894.16 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134427/186688 [01:05<00:14, 3611.81 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 13817███▌   | 122925/186688 [01:03<00:14, 4360.66 examples/s]Running tokenizer on dataset (num_proc=64):  67%|██████▋   | 125759/186688 [01:03<00:09, 6643.09 examples/s]Running tokenizer on dataset (num_proc=64):  68%|██████▊   | 127593/186688 [01:03<00:09, 6473.47 examples/s]Running tokenizer on dataset (num_proc=64):  69%|██████▉   | 128510/186688 [01:04<00:09, 5918.81 examples/s]Running tokenizer on dataset (num_proc=64):  70%|███████   | 131427/186688 [01:04<00:10, 5424.38 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████   | 132344/186688 [01:05<00:12, 4456.57 examples/s]Running tokenizer on dataset (num_proc=64):  71%|███████▏  | 133344/186688 [01:05<00:11, 4756.32 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134344/186688 [01:05<00:11, 4451.11 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135261/186688 [0examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136178/186688 [01:02<00:13, 3875.17 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 139012/186688 [01:03<00:07, 5987.33 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140846/186688 [01:03<00:09, 4836.44 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142680/186688 [01:04<00:10, 4319.94 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144597/186688 [01:04<00:08, 4959.86 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145514/186688 [01:04<00:07, 5302.34 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146514/186688 [01:05<00:13, 3028.06 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148348/186688 [01:05<00:11, 3262.63 examples/s]Running toke, 3433.10 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135012/186688 [01:04<00:18, 2827.49 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136012/186688 [01:04<00:19, 2641.88 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136929/186688 [01:04<00:16, 3091.56 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 137846/186688 [01:05<00:17, 2777.25 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138763/186688 [01:05<00:17, 2763.76 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139680/186688 [01:05<00:13, 3386.02 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140680/186688 [01:06<00:13, 3491.42 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141597/186688 [01:06<00:11, 3900.16 examples/s]Ru_proc=64):  70%|███████   | 130759/186688 [01:04<00:13, 4049.88 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 133510/186688 [01:04<00:13, 3966.51 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134510/186688 [01:04<00:12, 4183.27 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 135427/186688 [01:05<00:13, 3839.60 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136344/186688 [01:05<00:13, 3725.54 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138261/186688 [01:05<00:09, 5209.71 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140095/186688 [01:05<00:06, 6956.00 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141929/186688 [01:06<00:09, 4931.74 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███=64):  72%|███████▏  | 135178/186688 [01:04<00:17, 2894.31 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136095/186688 [01:05<00:25, 2008.76 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 137012/186688 [01:05<00:21, 2292.52 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138012/186688 [01:05<00:19, 2448.55 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138929/186688 [01:06<00:18, 2592.56 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139846/186688 [01:06<00:15, 2991.21 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141680/186688 [01:06<00:11, 3801.37 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143597/186688 [01:06<00:08, 5100.90 examples/s]Running tokenizer on dataset (num_proc=64):  77%|████94 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 134095/186688 [01:04<00:07, 6589.29 examples/s]Running tokenizer on dataset (num_proc=64):  72%|███████▏  | 135012/186688 [01:04<00:09, 5453.59 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 135929/186688 [01:05<00:09, 5393.99 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136929/186688 [01:05<00:13, 3704.60 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 137929/186688 [01:05<00:14, 3349.36 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138846/186688 [01:06<00:14, 3302.26 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139846/186688 [01:06<00:18, 2568.90 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141763/186688 [01:07<00:12, 3737.46 examples/s]Running tnning tokenizer on dataset (num_proc=64):  72%|███████▏  | 135012/186688 [01:03<00:18, 2815.88 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 135929/186688 [01:04<00:31, 1599.68 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136929/186688 [01:04<00:26, 1901.34 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139680/186688 [01:05<00:15, 3061.02 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142514/186688 [01:05<00:08, 4958.85 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144431/186688 [01:06<00:14, 2973.52 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145348/186688 [01:06<00:12, 3316.81 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146265/186688 [01:07<00:15, 2548.92 examples/s]Running tokenizer on datas8/186688 [01:05<00:08, 5910.67 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139095/186688 [01:05<00:11, 4324.03 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 140012/186688 [01:06<00:12, 3649.94 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140929/186688 [01:06<00:10, 4174.08 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141846/186688 [01:06<00:14, 3098.13 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142763/186688 [01:07<00:12, 3567.04 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 143680/186688 [01:07<00:11, 3693.45 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144680/186688 [01:07<00:13, 3199.51 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145597/186688 [01:08<00:19, 2nizer on dataset (num_proc=64):  80%|███████▉  | 149348/186688 [01:06<00:11, 3361.93 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150265/186688 [01:06<00:12, 2997.69 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151182/186688 [01:06<00:12, 2939.35 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████▏ | 152099/186688 [01:07<00:12, 2857.65 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153016/186688 [01:07<00:12, 2712.21 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 154016/186688 [01:07<00:11, 2755.51 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154933/186688 [01:08<00:13, 2332.28 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 155933/186688 [01:08<00:11, 2660.98 examples/s]Running tokenizer on datas1:05<00:13, 3878.42 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 136178/186688 [01:06<00:12, 4063.77 examples/s]Running tokenizer on dataset (num_proc=64):  73%|███████▎  | 137095/186688 [01:06<00:10, 4716.21 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 138012/186688 [01:06<00:12, 3787.20 examples/s]Running tokenizer on dataset (num_proc=64):  74%|███████▍  | 139012/186688 [01:06<00:12, 3885.66 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▍  | 139929/186688 [01:06<00:10, 4518.58 examples/s]Running tokenizer on dataset (num_proc=64):  75%|███████▌  | 140846/186688 [01:07<00:19, 2311.40 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▌  | 141763/186688 [01:07<00:15, 2883.86 examples/s]Running tokenizer on dataset (num_proc=64):  76%|███████▋  | 142680/186688 [01:09<00:28, 1556.43 exam███▋  | 143763/186688 [01:06<00:06, 6453.83 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145680/186688 [01:06<00:06, 5913.44 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147514/186688 [01:07<00:07, 5285.90 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 148431/186688 [01:07<00:10, 3645.94 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 149431/186688 [01:08<00:09, 3922.31 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 150431/186688 [01:08<00:10, 3555.80 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151348/186688 [01:08<00:08, 4099.15 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154099/186688 [01:08<00:05, 6268.35 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 15okenizer on dataset (num_proc=64):  77%|███████▋  | 144680/186688 [01:07<00:08, 4834.24 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145597/186688 [01:07<00:07, 5240.43 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146514/186688 [01:07<00:08, 4787.42 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148348/186688 [01:08<00:05, 6408.20 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149348/186688 [01:08<00:07, 4723.31 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151265/186688 [01:08<00:05, 6583.76 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153099/186688 [01:09<00:11, 2957.47 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154099/186688 [01:10<00:13, 2498.08 examples/s]Running tokenizer on dataset et (num_proc=64):  79%|███████▉  | 147182/186688 [01:07<00:15, 2522.39 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148182/186688 [01:08<00:14, 2608.20 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149182/186688 [01:08<00:15, 2371.43 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151182/186688 [01:08<00:09, 3688.94 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 152182/186688 [01:09<00:09, 3654.32 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153099/186688 [01:09<00:09, 3608.21 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 154016/186688 [01:09<00:10, 3261.25 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154933/186688 [01:09<00:08, 3824.74 examples/s]Running tokenizer on dataset (num_proc=64)██▋  | 144514/186688 [01:07<00:07, 5380.53 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145431/186688 [01:07<00:12, 3282.53 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146348/186688 [01:08<00:17, 2357.32 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148265/186688 [01:08<00:10, 3557.55 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149265/186688 [01:09<00:11, 3219.72 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151265/186688 [01:09<00:07, 4492.30 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153182/186688 [01:10<00:12, 2773.18 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 155016/186688 [01:10<00:08, 3594.43 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 156nning tokenizer on dataset (num_proc=64):  77%|███████▋  | 143514/186688 [01:06<00:08, 5047.58 examples/s]Running tokenizer on dataset (num_proc=64):  77%|███████▋  | 144431/186688 [01:06<00:08, 4823.70 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147265/186688 [01:07<00:06, 5767.47 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148265/186688 [01:07<00:11, 3412.11 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149182/186688 [01:08<00:11, 3170.63 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 150099/186688 [01:08<00:11, 3212.10 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████▏ | 152016/186688 [01:09<00:16, 2146.70 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153016/186688 [01:12<00:29, 1141.42 examples/s]Running tokenizer on d133.67 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147431/186688 [01:08<00:13, 2948.25 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 148348/186688 [01:09<00:16, 2318.25 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 149265/186688 [01:10<00:21, 1764.22 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151265/186688 [01:10<00:14, 2494.78 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 152182/186688 [01:11<00:13, 2477.57 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 153099/186688 [01:11<00:11, 2816.31 examples/s]Running tokenizer on dataset (num_proc=64):  83%|████████▎ | 154099/186688 [01:12<00:15, 2152.32 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 156016/186688 [01:12<00:10, 3007.24 examples/ples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 145514/186688 [01:09<00:14, 2863.43 examples/s]Running tokenizer on dataset (num_proc=64):  78%|███████▊  | 146514/186688 [01:09<00:15, 2664.27 examples/s]Running tokenizer on dataset (num_proc=64):  79%|███████▉  | 147431/186688 [01:11<00:22, 1735.07 examples/s]Running tokenizer on dataset (num_proc=64):  80%|███████▉  | 148431/186688 [01:11<00:22, 1669.43 examples/s]Running tokenizer on dataset (num_proc=64):  80%|████████  | 149431/186688 [01:12<00:19, 1906.20 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 150348/186688 [01:12<00:15, 2338.85 examples/s]Running tokenizer on dataset (num_proc=64):  81%|████████  | 151265/186688 [01:12<00:12, 2864.74 examples/s]Running tokenizer on dataset (num_proc=64):  82%|████████▏ | 154016/186688 [01:12<00:08, 3952.03 examples/s]Running tokeni5099/186688 [01:09<00:08, 3877.03 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 156016/186688 [01:09<00:06, 4421.38 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 157016/186688 [01:09<00:06, 4306.64 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157933/186688 [01:10<00:09, 2944.60 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:11<00:13, 2106.06 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:12<00:17, 1576.12 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:12<00:16, 1572.98 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:20<01:03, 391.47 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162850/186(num_proc=64):  83%|████████▎ | 155099/186688 [01:10<00:12, 2608.77 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 156099/186688 [01:10<00:10, 2911.33 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 157016/186688 [01:11<00:15, 1969.69 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157933/186688 [01:12<00:17, 1642.08 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158933/186688 [01:12<00:13, 1991.33 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:13<00:16, 1593.08 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:21<01:07, 385.04 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161767/186688 [01:26<01:28, 280.55 examples/s]Running tokenizer on dataset (num_proczer on dataset (num_proc=64):  83%|████████▎ | 154933/186688 [01:13<00:12, 2456.34 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▎ | 155933/186688 [01:14<00:11, 2791.42 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156850/186688 [01:14<00:09, 3056.30 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:15<00:17, 1632.88 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:16<00:18, 1540.66 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:17<00:18, 1432.00 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:19<00:30, 855.93 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161767/186688 [01:26<01:14, 335.56 examples/s]Running tokenizer on da016/186688 [01:11<00:11, 2648.93 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156933/186688 [01:11<00:10, 2929.15 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:12<00:17, 1608.19 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:13<00:15, 1748.71 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:15<00:24, 1083.22 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:18<00:40, 642.11 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:25<01:16, 326.41 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162850/186688 [01:26<00:57, 417.23 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688et (num_proc=64):  84%|████████▍ | 156933/186688 [01:08<00:08, 3401.87 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157933/186688 [01:09<00:12, 2311.62 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158933/186688 [01:10<00:13, 2078.23 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:10<00:12, 2224.24 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161767/186688 [01:18<00:51, 486.16 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:23<01:07, 355.26 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:25<01:03, 362.46 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:30<01:10, 312.93 examples/s]Running tokenizer on dataset (num_pro688 [01:24<01:17, 309.04 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:25<00:59, 384.42 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:29<01:06, 331.18 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:30<00:46, 453.11 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:30<00:31, 635.53 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:31<00:29, 634.75 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:32<00:23, 775.84 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:32<00:15, 1070.96 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:35<00c=64):  89%|████████▉ | 165767/186688 [01:30<00:52, 401.24 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:32<00:43, 462.75 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:33<00:35, 538.04 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:33<00:26, 685.25 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:34<00:20, 835.43 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:34<00:15, 1043.63 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:35<00:10, 1360.39 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:35<00:09, 1457.24 examples/s]Running tokenizer on dataset (num_proc=64): ataset (num_proc=64):  83%|████████▎ | 155016/186688 [01:12<00:17, 1760.76 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156850/186688 [01:12<00:11, 2512.37 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157850/186688 [01:13<00:14, 1958.98 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:14<00:21, 1308.00 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:20<00:49, 541.13 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:28<00:57, 419.55 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:33<01:06, 343.83 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:36<01:04, 341.70 examples/s]Running tokenizer on dataset (num:  83%|████████▎ | 155850/186688 [01:10<00:13, 2344.57 examples/s]Running tokenizer on dataset (num_proc=64):  84%|████████▍ | 156850/186688 [01:11<00:17, 1749.56 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:12<00:12, 2273.28 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:14<00:26, 1020.37 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161850/186688 [01:28<01:24, 292.86 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:29<01:08, 348.50 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:30<00:55, 415.07 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:33<00:57, 382.55 examples/s]Running tokenizer on dataset (num_proc=64):  89%|█s]Running tokenizer on dataset (num_proc=64):  85%|████████▍ | 157933/186688 [01:12<00:07, 4008.41 examples/s]Running tokenizer on dataset (num_proc=64):  85%|████████▌ | 158850/186688 [01:13<00:12, 2157.60 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 159850/186688 [01:14<00:11, 2350.04 examples/s]Running tokenizer on dataset (num_proc=64):  86%|████████▌ | 160850/186688 [01:14<00:10, 2461.01 examples/s]Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 161767/186688 [01:21<00:55, 448.13 examples/s] Running tokenizer on dataset (num_proc=64):  87%|████████▋ | 162767/186688 [01:22<00:43, 552.04 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:22<00:33, 694.39 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:26<00:48, 453.90 examples/s]Running =64):  87%|████████▋ | 162767/186688 [01:26<01:00, 396.03 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:34<01:31, 249.76 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:37<01:22, 266.13 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:37<00:56, 368.59 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:38<00:44, 447.66 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:38<00:30, 618.32 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:39<00:21, 841.40 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:39<00:15, 1092.51 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█ [01:29<01:05, 350.14 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:30<00:50, 430.77 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:32<00:44, 475.40 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:38<01:05, 305.15 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:38<00:45, 416.61 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:41<00:44, 398.63 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:41<00:30, 551.29 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:41<00:15, 986.77 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:43<00:1taset (num_proc=64):  87%|████████▋ | 162767/186688 [01:28<01:05, 366.92 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 163767/186688 [01:32<01:07, 339.42 examples/s]Running tokenizer on dataset (num_proc=64):  88%|████████▊ | 164767/186688 [01:32<00:47, 459.75 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:38<01:07, 310.36 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:40<00:59, 334.30 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:41<00:43, 435.59 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:41<00:30, 594.68 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:41<00:21, 793.50 examples/s]Running tokenizer on dataset (num_proc=tokenizer on dataset (num_proc=64):  89%|████████▉ | 165767/186688 [01:37<01:39, 210.24 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:37<01:07, 295.59 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:37<00:45, 413.19 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:39<00:36, 490.82 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:40<00:31, 536.83 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:43<00:32, 485.49 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:43<00:22, 675.86 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:43<00:16, 819.79 examples/s]Running tokenizer ██████▉ | 165767/186688 [01:37<01:01, 338.26 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:38<00:50, 393.65 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:39<00:35, 525.83 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:39<00:27, 656.77 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:40<00:21, 797.70 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████▏| 170767/186688 [01:40<00:15, 1024.15 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:41<00:15, 943.25 examples/s] Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:44<00:20, 686.80 examples/s]Running tokenizer on dataset (num_proc=64):  93%|████:23, 670.68 examples/s] Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:35<00:16, 909.43 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:35<00:12, 1083.28 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:40<00:25, 505.24 examples/s] Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:40<00:17, 668.86 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:41<00:14, 774.34 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:49<00:34, 291.83 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:49<00:22, 403.99 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:49_proc=64):  89%|████████▉ | 165767/186688 [01:37<00:49, 420.43 examples/s]Running tokenizer on dataset (num_proc=64):  89%|████████▉ | 166767/186688 [01:38<00:43, 462.84 examples/s]Running tokenizer on dataset (num_proc=64):  90%|████████▉ | 167767/186688 [01:38<00:31, 603.59 examples/s]Running tokenizer on dataset (num_proc=64):  90%|█████████ | 168767/186688 [01:39<00:22, 807.90 examples/s]Running tokenizer on dataset (num_proc=64):  91%|█████████ | 169767/186688 [01:41<00:26, 645.54 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171684/186688 [01:42<00:18, 829.57 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 172684/186688 [01:44<00:16, 846.77 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173684/186688 [01:46<00:18, 709.60 examples/s]Running tokenizer on dataset (num_proc=64): 93%|█████████▎| 173767/186688 [01:37<00:11, 1101.82 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:40<00:19, 601.16 examples/s] Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:42<00:21, 512.44 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:47<00:28, 353.14 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:52<00:33, 275.85 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [01:57<00:34, 237.32 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [01:58<00:23, 310.90 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [01:58<00:14, 429.10 examples/s]Running tokenizer on dataset (num_proc=64)64):  91%|█████████▏| 170767/186688 [01:43<00:20, 767.20 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:45<00:23, 624.67 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:47<00:25, 552.53 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:48<00:20, 628.77 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:49<00:14, 813.57 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:51<00:16, 686.96 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:57<00:29, 342.24 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [02:01<00:30, 299.48 examples/s]Running tokenizer on dataset (num_proc=███████▏| 170767/186688 [01:39<00:12, 1319.82 examples/s]Running tokenizer on dataset (num_proc=64):  92%|█████████▏| 171767/186688 [01:40<00:08, 1682.39 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 172767/186688 [01:40<00:08, 1710.80 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:41<00:08, 1510.31 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:49<00:35, 333.69 examples/s] Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:52<00:31, 344.55 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:58<00:39, 257.52 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [02:01<00:36, 254.65 examples/s]Running tokenizer on dataset (num_proc=64):  96%<00:15, 546.91 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [01:53<00:18, 386.20 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [01:56<00:17, 358.47 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [01:59<00:14, 367.49 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:00<00:10, 424.26 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:01<00:06, 541.79 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:01<00:03, 749.67 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:01<00:02, 899.63 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:05<00:01, 533.25 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:05<00:00, 678.77 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:06<00:00, 1476.06 examples/s]
4, 960.16 examples/s]Running tokenizer on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:45<00:19, 647.81 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:47<00:17, 675.93 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:48<00:14, 755.47 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:55<00:32, 310.14 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:56<00:24, 377.94 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:02<00:31, 261.55 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:06<00:27, 265.00 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:07<00:1:  97%|█████████▋| 181186/186688 [01:59<00:09, 580.32 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [01:59<00:03, 971.26 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [01:59<00:02, 1122.50 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:01<00:02, 889.54 examples/s] Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:02<00:01, 827.91 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:06<00:00, 485.81 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:07<00:00, 1465.93 examples/s]
█████▎| 173767/186688 [01:44<00:14, 874.67 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174767/186688 [01:45<00:10, 1123.94 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:52<00:30, 356.70 examples/s] Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:54<00:28, 348.74 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [02:02<00:41, 222.70 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:03<00:28, 285.70 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:05<00:22, 324.87 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:06<00:16, 391.96 examples/s]Running tokenizer on dataset (num_proc=64):  97%|███on dataset (num_proc=64):  93%|█████████▎| 173767/186688 [01:44<00:12, 1043.12 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▎| 174684/186688 [01:44<00:10, 1096.97 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:45<00:10, 1100.03 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:50<00:22, 439.24 examples/s] Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:52<00:19, 474.91 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:00<00:33, 245.99 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:04<00:30, 241.31 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:06<00:22, 288.12 examples/s]Running tokeni  94%|█████████▎| 174684/186688 [01:50<00:25, 463.25 examples/s]Running tokenizer on dataset (num_proc=64):  94%|█████████▍| 175684/186688 [01:51<00:22, 493.77 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▍| 176601/186688 [01:57<00:32, 308.98 examples/s]Running tokenizer on dataset (num_proc=64):  95%|█████████▌| 177518/186688 [01:59<00:24, 369.83 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 178435/186688 [02:05<00:31, 261.69 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:06<00:21, 336.82 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:06<00:14, 450.27 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:10<00:16, 333.35 examples/s]Running tokenizer on dataset (num_proc=64):|█████████▌| 178435/186688 [02:03<00:28, 294.06 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:06<00:23, 309.09 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:07<00:17, 367.60 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:08<00:11, 473.04 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:08<00:07, 632.42 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:08<00:04, 824.60 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:09<00:02, 1066.58 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:11<00:02, 723.79 examples/s] Running tokenizer on dataset (num_proc=64): 10zer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:08<00:17, 318.14 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:08<00:10, 426.14 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:10<00:07, 476.44 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:10<00:04, 590.28 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:12<00:01, 743.04 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:13<00:00, 824.37 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:14<00:00, 1390.62 examples/s]
█████▋| 181186/186688 [02:07<00:10, 505.89 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:09<00:09, 505.07 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:10<00:06, 585.46 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:11<00:04, 560.40 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:12<00:02, 629.68 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:13<00:00, 985.32 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:14<00:00, 1388.98 examples/s]
8, 340.65 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:08<00:13, 394.55 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:09<00:08, 514.49 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:09<00:06, 608.63 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:10<00:03, 746.39 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:10<00:01, 1000.95 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:11<00:00, 979.85 examples/s] Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:13<00:00, 723.60 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:14<00:00, 1387.49 examples/s]
64):  96%|█████████▌| 178435/186688 [02:02<00:22, 371.01 examples/s]Running tokenizer on dataset (num_proc=64):  96%|█████████▌| 179352/186688 [02:08<00:28, 257.39 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 180269/186688 [02:09<00:20, 317.20 examples/s]Running tokenizer on dataset (num_proc=64):  97%|█████████▋| 181186/186688 [02:09<00:12, 438.32 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 182103/186688 [02:10<00:08, 564.74 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:12<00:06, 536.50 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▊| 183937/186688 [02:13<00:04, 612.11 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:15<00:03, 533.37 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:16<00:01, 603.64 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:16<00:00, 817.06 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:17<00:00, 1356.00 examples/s]
0%|█████████▉| 185771/186688 [02:11<00:00, 960.24 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:17<00:00, 382.90 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:18<00:00, 1350.03 examples/s]
  98%|█████████▊| 182103/186688 [02:11<00:09, 459.52 examples/s]Running tokenizer on dataset (num_proc=64):  98%|█████████▊| 183020/186688 [02:11<00:06, 544.62 examples/s]Running tokenizer on dataset (num_proc=64):  99%|█████████▉| 184854/186688 [02:14<00:03, 607.49 examples/s]Running tokenizer on dataset (num_proc=64): 100%|█████████▉| 185771/186688 [02:17<00:01, 479.67 examples/s]Running tokenizer on dataset (num_proc=64): 100%|██████████| 186688/186688 [02:18<00:00, 1347.18 examples/s]
[INFO|configuration_utils.py:763] 2025-10-03 11:40:01,457 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 11:40:01,457 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 11:40:01,457 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 11:40:01,457 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 11:40:01,457 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-03 11:40:01,457 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 11:40:01,458 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-03 11:40:01,458 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-03 11:40:01,458 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-03 11:40:01,458 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-03 11:40:01,461 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 11:40:01,462 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-03 11:40:01,464 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 11:40:01,465 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|configuration_utils.py:763] 2025-10-03 11:40:01,465 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-03 11:40:01,466 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|modeling_utils.py:1277] 2025-10-03 11:40:02,064 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-03 11:40:02,064 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-03 11:40:02,064 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 11:40:02,065 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-03 11:40:02,065 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-03 11:40:02,065 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-03 11:40:02,077 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|modeling_utils.py:1277] 2025-10-03 11:40:02,077 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 11:40:02,078 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-03 11:40:02,078 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 11:40:02,080 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 11:40:02,093 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|modeling_utils.py:1277] 2025-10-03 11:40:02,129 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-03 11:40:02,129 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 11:40:02,130 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-03 11:40:02,130 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-03 11:40:02,130 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:1277] 2025-10-03 11:40:02,131 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 11:40:02,131 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:4492] 2025-10-03 11:40:02,131 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-03 11:40:02,142 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 11:40:02,143 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 11:40:02,145 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 11:40:02,158 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:07<00:56,  7.07s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.53s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.96s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:55,  6.97s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.49s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:13<00:45,  6.50s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]L35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]L35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.96s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.30s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.40s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]L35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]L35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]L35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]L35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]L35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:35,  5.95s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:25<00:31,  6.29s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:31<00:25,  6.39s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.12s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.12s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 11:40:56,581 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5724] 2025-10-03 11:40:56,581 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 11:40:56,581 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|modeling_utils.py:5732] 2025-10-03 11:40:56,581 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
oading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]

Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it][INFO|modeling_utils.py:5724] 2025-10-03 11:40:56,582 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

oading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██oading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:37<00:18,  6.11s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:11,  5.83s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.06s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██[INFO|modeling_utils.py:5724] 2025-10-03 11:40:56,582 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
[INFO|modeling_utils.py:5732] 2025-10-03 11:40:56,582 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|modeling_utils.py:5732] 2025-10-03 11:40:56,582 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 11:40:56,582 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 11:40:56,582 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 11:40:56,582 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
[INFO|modeling_utils.py:5732] 2025-10-03 11:40:56,582 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.84s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 11:40:56,582 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 11:40:56,582 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1008] 2025-10-03 11:40:56,582 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 11:40:56,583 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-03 11:40:56,583 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 11:40:56,583 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-03 11:40:56,584 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 11:40:56,584 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-03 11:40:56,584 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1008] 2025-10-03 11:40:56,584 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1008] 2025-10-03 11:40:56,584 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 11:40:56,584 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-03 11:40:56,584 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 11:40:56,584 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1055] 2025-10-03 11:40:56,584 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1055] 2025-10-03 11:40:56,584 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 11:40:56,606 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 11:40:56,607 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 11:40:56,607 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 11:40:56,607 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 11:40:56,608 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 11:40:56,608 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 11:40:56,610 >> Using auto half precision backend
[INFO|trainer.py:757] 2025-10-03 11:40:56,610 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 11:40:56,610 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 11:40:56,610 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 11:40:56,612 >> Using auto half precision backend
Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.22s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:52<00:00,  5.85s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 11:40:56,613 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 11:40:56,613 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 11:40:56,613 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 11:40:56,613 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 11:40:56,614 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|configuration_utils.py:1008] 2025-10-03 11:40:56,615 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 11:40:56,615 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|trainer.py:757] 2025-10-03 11:40:56,635 >> Using auto half precision backend
[WARNING|trainer.py:985] 2025-10-03 11:40:56,636 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 11:40:56,880 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 11:40:56,899 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 11:40:56,907 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 11:40:56,907 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[INFO|deepspeed.py:380] 2025-10-03 11:40:56,908 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 11:40:56,912 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 2. Using DeepSpeed's value.
[INFO|deepspeed.py:380] 2025-10-03 11:40:56,918 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[INFO|deepspeed.py:380] 2025-10-03 11:40:56,918 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[rank30]:W1003 11:41:01.523000 3712898 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank30]:W1003 11:41:01.523000 3712898 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank19]:W1003 11:41:01.601000 2879112 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank19]:W1003 11:41:01.601000 2879112 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank1]:W1003 11:41:01.627000 2397635 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank1]:W1003 11:41:01.627000 2397635 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank21]:W1003 11:41:01.653000 1179228 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank21]:W1003 11:41:01.653000 1179228 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank18]:W1003 11:41:01.676000 2879111 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank18]:W1003 11:41:01.676000 2879111 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank27]:W1003 11:41:01.704000 3124111 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank27]:W1003 11:41:01.704000 3124111 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank14]:W1003 11:41:01.981000 1595056 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank14]:W1003 11:41:01.981000 1595056 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[INFO|trainer.py:2523] 2025-10-03 11:41:10,292 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 11:41:10,292 >>   Num examples = 184,821
[INFO|trainer.py:2525] 2025-10-03 11:41:10,292 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 11:41:10,292 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 11:41:10,292 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2530] 2025-10-03 11:41:10,292 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2531] 2025-10-03 11:41:10,292 >>   Total optimization steps = 2,888
[INFO|trainer.py:2523] 2025-10-03 11:41:10,292 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 11:41:10,292 >>   Num examples = 184,821
[INFO|trainer.py:2525] 2025-10-03 11:41:10,292 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 11:41:10,292 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 11:41:10,292 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2523] 2025-10-03 11:41:10,292 >> ***** Running training *****
[INFO|trainer.py:2530] 2025-10-03 11:41:10,292 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2531] 2025-10-03 11:41:10,292 >>   Total optimization steps = 2,888
[INFO|trainer.py:2524] 2025-10-03 11:41:10,292 >>   Num examples = 184,821
[INFO|trainer.py:2525] 2025-10-03 11:41:10,292 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 11:41:10,292 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 11:41:10,292 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2530] 2025-10-03 11:41:10,292 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2531] 2025-10-03 11:41:10,292 >>   Total optimization steps = 2,888
[INFO|trainer.py:2523] 2025-10-03 11:41:10,292 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 11:41:10,293 >>   Num examples = 184,821
[INFO|trainer.py:2525] 2025-10-03 11:41:10,293 >>   Num Epochs = 1
[INFO|trainer.py:2532] 2025-10-03 11:41:10,293 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2523] 2025-10-03 11:41:10,293 >> ***** Running training *****
[INFO|trainer.py:2526] 2025-10-03 11:41:10,293 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2532] 2025-10-03 11:41:10,293 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2524] 2025-10-03 11:41:10,293 >>   Num examples = 184,821
[INFO|trainer.py:2529] 2025-10-03 11:41:10,293 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2525] 2025-10-03 11:41:10,293 >>   Num Epochs = 1
[INFO|trainer.py:2530] 2025-10-03 11:41:10,293 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2526] 2025-10-03 11:41:10,293 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2531] 2025-10-03 11:41:10,293 >>   Total optimization steps = 2,888
[INFO|trainer.py:2529] 2025-10-03 11:41:10,293 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2530] 2025-10-03 11:41:10,293 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2531] 2025-10-03 11:41:10,293 >>   Total optimization steps = 2,888
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2532] 2025-10-03 11:41:10,293 >>   Number of trainable parameters = 20,914,757,184
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2523] 2025-10-03 11:41:10,293 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 11:41:10,293 >>   Num examples = 184,821
[INFO|trainer.py:2525] 2025-10-03 11:41:10,293 >>   Num Epochs = 1
[INFO|trainer.py:2532] 2025-10-03 11:41:10,293 >>   Number of trainable parameters = 20,914,757,184
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2526] 2025-10-03 11:41:10,293 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 11:41:10,293 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2530] 2025-10-03 11:41:10,293 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2531] 2025-10-03 11:41:10,293 >>   Total optimization steps = 2,888
[INFO|trainer.py:2532] 2025-10-03 11:41:10,294 >>   Number of trainable parameters = 20,914,757,184
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2532] 2025-10-03 11:41:10,294 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2523] 2025-10-03 11:41:10,294 >> ***** Running training *****
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2524] 2025-10-03 11:41:10,294 >>   Num examples = 184,821
[INFO|trainer.py:2525] 2025-10-03 11:41:10,294 >>   Num Epochs = 1
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2526] 2025-10-03 11:41:10,294 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 11:41:10,294 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2530] 2025-10-03 11:41:10,294 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2531] 2025-10-03 11:41:10,294 >>   Total optimization steps = 2,888
[INFO|trainer.py:2532] 2025-10-03 11:41:10,295 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2523] 2025-10-03 11:41:10,522 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 11:41:10,522 >>   Num examples = 184,821
[INFO|trainer.py:2525] 2025-10-03 11:41:10,522 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 11:41:10,522 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 11:41:10,522 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2530] 2025-10-03 11:41:10,522 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2531] 2025-10-03 11:41:10,522 >>   Total optimization steps = 2,888
[INFO|trainer.py:2532] 2025-10-03 11:41:10,523 >>   Number of trainable parameters = 20,914,757,184
  0%|          | 0/2888 [00:00<?, ?it/s]  0%|          | 1/2888 [00:29<23:44:07, 29.60s/it]  0%|          | 2/2888 [00:51<20:18:22, 25.33s/it]  0%|          | 3/2888 [01:08<17:05:58, 21.34s/it]  0%|          | 4/2888 [01:25<15:40:20, 19.56s/it]  0%|          | 5/2888 [01:43<15:10:48, 18.96s/it]  0%|          | 6/2888 [02:01<14:51:07, 18.55s/it]  0%|          | 7/2888 [02:18<14:33:15, 18.19s/it]  0%|          | 8/2888 [02:33<13:45:21, 17.19s/it]  0%|          | 9/2888 [02:49<13:29:27, 16.87s/it]  0%|          | 10/2888 [03:06<13:27:58, 16.84s/it]                                                      0%|          | 10/2888 [03:06<13:27:58, 16.84s/it]  0%|          | 11/2888 [03:23<13:34:15, 16.98s/it]  0%|          | 12/2888 [03:41<13:40:00, 17.11s/it]  0%|          | 13/2888 [03:59<13:59:35, 17.52s/it]  0%|          | 14/2888 [04:18<14:19:13, 17.94s/it]  1%|          | 15/2888 [04:39<15:08:22, 18.97s/it]  1%|          | 16/2888 [05:00<15:37:20, 19.58s/it]  1%|          | 17/2888 [05:16<14:40:12, 18.39s/it]  1%|          | 18/2888 [05:35<14:42:03, 18.44s/it]  1%|          | 19/2888 [05:54<14:51:34, 18.65s/it]  1%|          | 20/2888 [06:11<14:28:34, 18.17s/it]                                                      1%|          | 20/2888 [06:11<14:28:34, 18.17s/it]  1%|          | 21/2888 [06:27<13:57:11, 17.52s/it]  1%|          | 22/2888 [06:43<13:45:20, 17.28s/it]  1%|          | 23/2888 [07:01<13:53:14, 17.45s/it]  1%|          | 24/2888 [07:20<14:07:22, 17.75s/it]  1%|          | 25/2888 [07:40<14:37:46, 18.40s/it]  1%|          | 26/2888 [08:00<15:09:33, 19.07s/it]  1%|          | 27/2888 [08:18<14:50:37, 18.68s/it]  1%|          | 28/2888 [08:32<13:49:02, 17.39s/it]  1%|          | 29/2888 [08:50<13:46:18, 17.34s/it]  1%|          | 30/2888 [09:06<13:34:24, 17.10s/it]                                                      1%|          | 30/2888 [09:06<13:34:24, 17.10s/it]  1%|          | 31/2888 [09:26<14:06:59, 17.79s/it]  1%|          | 32/2888 [09:43<13:55:49, 17.56s/it]  1%|          | 33/2888 [10:01<14:00:40, 17.67s/it]  1%|          | 34/2888 [10:20<14:31:45, 18.33s/it]  1%|          | 35/2888 [10:36<13:57:48, 17.62s/it]  1%|          | 36/2888 [10:52<13:29:03, 17.02s/it]  1%|▏         | 37/2888 [11:07<12:57:17, 16.36s/it]  1%|▏         | 38/2888 [11:24<13:14:35, 16.73s/it]  1%|▏         | 39/2888 [11:42<13:19:34, 16.84s/it]  1%|▏         | 40/2888 [12:00<13:47:49, 17.44s/it]                                                      1%|▏         | 40/2888 [12:00<13:47:49, 17.44s/it]  1%|▏         | 41/2888 [12:18<13:45:59, 17.41s/it]  1%|▏         | 42/2888 [12:35<13:37:34, 17.24s/it]  1%|▏         | 43/2888 [12:54<14:14:50, 18.03s/it]  2%|▏         | 44/2888 [13:11<13:57:09, 17.66s/it]  2%|▏         | 45/2888 [13:28<13:50:27, 17.53s/it]  2%|▏         | 46/2888 [13:44<13:26:28, 17.03s/it]  2%|▏         | 47/2888 [14:00<13:05:33, 16.59s/it]  2%|▏         | 48/2888 [14:17<13:06:15, 16.61s/it]  2%|▏         | 49/2888 [14:32<12:56:24, 16.41s/it]  2%|▏         | 50/2888 [14:49<12:59:38, 16.48s/it]                                                      2%|▏         | 50/2888 [14:49<12:59:38, 16.48s/it]  2%|▏         | 51/2888 [15:09<13:44:51, 17.45s/it]  2%|▏         | 52/2888 [15:24<13:08:58, 16.69s/it]  2%|▏         | 53/2888 [15:43<13:50:43, 17.58s/it]  2%|▏         | 54/2888 [16:00<13:39:52, 17.36s/it]  2%|▏         | 55/2888 [16:17<13:29:12, 17.14s/it]  2%|▏         | 56/2888 [16:39<14:45:04, 18.75s/it]  2%|▏         | 57/2888 [16:59<14:56:52, 19.01s/it]  2%|▏         | 58/2888 [17:24<16:14:51, 20.67s/it]  2%|▏         | 59/2888 [17:41<15:29:17, 19.71s/it]  2%|▏         | 60/2888 [17:57<14:43:28, 18.74s/it]                                                      2%|▏         | 60/2888 [17:58<14:43:28, 18.74s/it]  2%|▏         | 61/2888 [18:15<14:22:07, 18.30s/it]  2%|▏         | 62/2888 [18:30<13:36:07, 17.33s/it]  2%|▏         | 63/2888 [18:52<14:42:37, 18.75s/it]  2%|▏         | 64/2888 [19:10<14:29:30, 18.47s/it]  2%|▏         | 65/2888 [19:29<14:41:24, 18.73s/it]  2%|▏         | 66/2888 [19:46<14:11:24, 18.10s/it]  2%|▏         | 67/2888 [20:01<13:29:26, 17.22s/it]  2%|▏         | 68/2888 [20:23<14:36:21, 18.65s/it]  2%|▏         | 69/2888 [20:43<14:54:45, 19.04s/it]  2%|▏         | 70/2888 [20:59<14:20:54, 18.33s/it]                                                      2%|▏         | 70/2888 [20:59<14:20:54, 18.33s/it]  2%|▏         | 71/2888 [21:20<14:54:28, 19.05s/it]  2%|▏         | 72/2888 [21:38<14:33:55, 18.62s/it]  3%|▎         | 73/2888 [21:59<15:03:31, 19.26s/it]  3%|▎         | 74/2888 [22:16<14:43:44, 18.84s/it]  3%|▎         | 75/2888 [22:34<14:30:57, 18.58s/it]  3%|▎         | 76/2888 [22:55<14:54:41, 19.09s/it]  3%|▎         | 77/2888 [23:11<14:09:21, 18.13s/it]  3%|▎         | 78/2888 [23:29<14:10:07, 18.15s/it]  3%|▎         | 79/2888 [23:47<14:11:56, 18.20s/it]  3%|▎         | 80/2888 [24:05<14:02:18, 18.00s/it]                                                      3%|▎         | 80/2888 [24:05<14:02:18, 18.00s/it]  3%|▎         | 81/2888 [24:24<14:23:09, 18.45s/it]  3%|▎         | 82/2888 [24:40<13:50:23, 17.76s/it]  3%|▎         | 83/2888 [24:58<13:47:03, 17.69s/it]  3%|▎         | 84/2888 [25:20<14:52:43, 19.10s/it]  3%|▎         | 85/2888 [25:37<14:23:24, 18.48s/it]  3%|▎         | 86/2888 [25:54<14:02:12, 18.03s/it]  3%|▎         | 87/2888 [26:11<13:51:36, 17.81s/it]  3%|▎         | 88/2888 [26:30<13:57:31, 17.95s/it]  3%|▎         | 89/2888 [26:47<13:50:01, 17.79s/it]  3%|▎         | 90/2888 [27:07<14:24:08, 18.53s/it]                                                      3%|▎         | 90/2888 [27:07<14:24:08, 18.53s/it]  3%|▎         | 91/2888 [27:24<13:58:30, 17.99s/it]  3%|▎         | 92/2888 [27:41<13:43:20, 17.67s/it]  3%|▎         | 93/2888 [27:59<13:39:48, 17.60s/it]  3%|▎         | 94/2888 [28:14<13:14:46, 17.07s/it]  3%|▎         | 95/2888 [28:32<13:24:14, 17.28s/it]  3%|▎         | 96/2888 [28:50<13:31:44, 17.44s/it]  3%|▎         | 97/2888 [29:07<13:29:53, 17.41s/it]  3%|▎         | 98/2888 [29:25<13:36:17, 17.55s/it]  3%|▎         | 99/2888 [29:41<13:11:23, 17.03s/it]  3%|▎         | 100/2888 [30:00<13:35:19, 17.55s/it]                                                       3%|▎         | 100/2888 [30:00<13:35:19, 17.55s/it][INFO|trainer.py:4289] 2025-10-03 12:11:22,537 >> Saving model checkpoint to /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100
[INFO|configuration_utils.py:491] 2025-10-03 12:11:22,542 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100/config.json
[INFO|configuration_utils.py:826] 2025-10-03 12:11:22,542 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100/generation_config.json
[INFO|modeling_utils.py:4308] 2025-10-03 12:11:41,528 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2394] 2025-10-03 12:11:41,529 >> chat template saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100/chat_template.jinja
[INFO|tokenization_utils_base.py:2563] 2025-10-03 12:11:41,530 >> tokenizer config file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2572] 2025-10-03 12:11:41,530 >> Special tokens file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_formal_gpt-oss-20b-bf16_lr3.0e-5_cosine_dxy_full_finetune/checkpoint-100/special_tokens_map.json
  3%|▎         | 101/2888 [30:48<20:42:30, 26.75s/it]  4%|▎         | 102/2888 [31:05<18:26:50, 23.84s/it]  4%|▎         | 103/2888 [31:28<18:15:58, 23.61s/it]  4%|▎         | 104/2888 [31:45<16:47:21, 21.71s/it]  4%|▎         | 105/2888 [32:07<16:42:48, 21.62s/it]  4%|▎         | 106/2888 [32:25<15:50:48, 20.51s/it]  4%|▎         | 107/2888 [32:46<15:59:48, 20.71s/it]  4%|▎         | 108/2888 [33:01<14:39:34, 18.98s/it]  4%|▍         | 109/2888 [33:18<14:10:51, 18.37s/it]  4%|▍         | 110/2888 [33:33<13:29:07, 17.48s/it]                                                       4%|▍         | 110/2888 [33:33<13:29:07, 17.48s/it]  4%|▍         | 111/2888 [33:54<14:15:01, 18.47s/it]  4%|▍         | 112/2888 [34:12<14:15:30, 18.49s/it]  4%|▍         | 113/2888 [34:27<13:26:52, 17.45s/it]  4%|▍         | 114/2888 [34:46<13:38:49, 17.71s/it]  4%|▍         | 115/2888 [35:09<14:56:49, 19.40s/it]  4%|▍         | 116/2888 [35:26<14:21:44, 18.65s/it]  4%|▍         | 117/2888 [35:42<13:42:15, 17.80s/it]  4%|▍         | 118/2888 [36:00<13:43:06, 17.83s/it]  4%|▍         | 119/2888 [36:17<13:40:15, 17.77s/it]  4%|▍         | 120/2888 [36:35<13:40:51, 17.79s/it]                                                       4%|▍         | 120/2888 [36:35<13:40:51, 17.79s/it]  4%|▍         | 121/2888 [36:51<13:14:16, 17.22s/it]  4%|▍         | 122/2888 [37:09<13:20:08, 17.36s/it]  4%|▍         | 123/2888 [37:31<14:29:50, 18.88s/it]  4%|▍         | 124/2888 [37:48<13:57:42, 18.18s/it]  4%|▍         | 125/2888 [38:04<13:29:29, 17.58s/it]  4%|▍         | 126/2888 [38:21<13:22:45, 17.44s/it]  4%|▍         | 127/2888 [38:40<13:38:55, 17.80s/it]  4%|▍         | 128/2888 [38:57<13:34:13, 17.70s/it]  4%|▍         | 129/2888 [39:16<13:45:04, 17.94s/it]  5%|▍         | 130/2888 [39:33<13:39:23, 17.83s/it]                                                       5%|▍         | 130/2888 [39:33<13:39:23, 17.83s/it]  5%|▍         | 131/2888 [39:52<13:58:34, 18.25s/it]  5%|▍         | 132/2888 [40:15<14:53:31, 19.45s/it]  5%|▍         | 133/2888 [40:31<14:07:10, 18.45s/it]  5%|▍         | 134/2888 [40:56<15:35:14, 20.38s/it]  5%|▍         | 135/2888 [41:14<15:07:38, 19.78s/it]  5%|▍         | 136/2888 [41:30<14:19:09, 18.73s/it]  5%|▍         | 137/2888 [41:45<13:28:40, 17.64s/it]  5%|▍         | 138/2888 [42:04<13:40:27, 17.90s/it]  5%|▍         | 139/2888 [42:19<13:03:12, 17.09s/it]  5%|▍         | 140/2888 [42:36<12:57:44, 16.98s/it]                                                       5%|▍         | 140/2888 [42:36<12:57:44, 16.98s/it]  5%|▍         | 141/2888 [42:54<13:11:20, 17.28s/it]  5%|▍         | 142/2888 [43:13<13:32:06, 17.74s/it]  5%|▍         | 143/2888 [43:30<13:21:17, 17.51s/it]  5%|▍         | 144/2888 [43:50<13:59:35, 18.36s/it]  5%|▌         | 145/2888 [44:07<13:39:36, 17.93s/it]  5%|▌         | 146/2888 [44:26<13:49:50, 18.16s/it]  5%|▌         | 147/2888 [44:43<13:44:31, 18.05s/it]  5%|▌         | 148/2888 [45:05<14:37:28, 19.21s/it]  5%|▌         | 149/2888 [45:24<14:35:37, 19.18s/it]  5%|▌         | 150/2888 [45:42<14:09:24, 18.61s/it]                                                       5%|▌         | 150/2888 [45:42<14:09:24, 18.61s/it]  5%|▌         | 151/2888 [45:59<13:48:43, 18.17s/it]  5%|▌         | 152/2888 [46:16<13:37:45, 17.93s/it]  5%|▌         | 153/2888 [46:33<13:17:12, 17.49s/it]  5%|▌         | 154/2888 [46:52<13:36:05, 17.91s/it]  5%|▌         | 155/2888 [47:11<13:50:14, 18.23s/it]  5%|▌         | 156/2888 [47:29<13:49:25, 18.22s/it]  5%|▌         | 157/2888 [47:46<13:28:42, 17.77s/it]  5%|▌         | 158/2888 [48:04<13:36:34, 17.95s/it]  6%|▌         | 159/2888 [48:22<13:33:31, 17.89s/it]  6%|▌         | 160/2888 [48:44<14:28:07, 19.09s/it]                                                       6%|▌         | 160/2888 [48:44<14:28:07, 19.09s/it]  6%|▌         | 161/2888 [49:03<14:30:49, 19.16s/it]  6%|▌     srun: Job step aborted: Waiting up to 47 seconds for job step to finish.
slurmstepd: error: *** JOB 1159106 ON della-j12g2 CANCELLED AT 2025-10-03T12:30:44 ***
    | 162/2888 [49:19<13:47:07, 18.21s/it]slurmstepd: error: *** STEP 1159106.0 ON della-j12g2 CANCELLED AT 2025-10-03T12:30:44 ***
W1003 12:30:44.576000 3712844 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 12:30:44.576000 1179174 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 12:30:44.576000 1594999 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 12:30:44.578000 3712844 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3712896 closing signal SIGTERM
W1003 12:30:44.578000 1594999 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1595054 closing signal SIGTERM
W1003 12:30:44.578000 1179174 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1179227 closing signal SIGTERM
W1003 12:30:44.576000 2397575 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 12:30:44.576000 3124052 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 12:30:44.576000 2328484 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 12:30:44.576000 3229170 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 12:30:44.580000 2397575 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2397634 closing signal SIGTERM
W1003 12:30:44.580000 2328484 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2328542 closing signal SIGTERM
W1003 12:30:44.581000 3229170 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3229220 closing signal SIGTERM
W1003 12:30:44.579000 3712844 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3712897 closing signal SIGTERM
W1003 12:30:44.580000 3124052 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3124108 closing signal SIGTERM
W1003 12:30:44.576000 2879060 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 12:30:44.581000 2879060 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2879109 closing signal SIGTERM
W1003 12:30:44.582000 1179174 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1179228 closing signal SIGTERM
W1003 12:30:44.585000 2397575 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2397635 closing signal SIGTERM
W1003 12:30:44.586000 2879060 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2879110 closing signal SIGTERM
W1003 12:30:44.586000 3124052 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3124109 closing signal SIGTERM
W1003 12:30:44.584000 3229170 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3229221 closing signal SIGTERM
