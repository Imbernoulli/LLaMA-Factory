+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=0 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset dxy --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 5.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=3 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset dxy --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 5.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=4 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset dxy --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 5.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=6 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset dxy --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 5.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=7 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset dxy --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 5.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=2 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset dxy --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 5.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=5 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset dxy --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 5.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
+ cd /scratch/gpfs/yl7690/projects/LLaMA-Factory-new
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ export DISABLE_VERSION_CHECK=1
+ DISABLE_VERSION_CHECK=1
++ scontrol show hostnames 'della-j12g2,della-j15g[1-3],della-j16g[1,3],della-j17g1,della-k11g1'
++ head -n 1
+ torchrun --nproc_per_node=4 --nnodes=8 --node_rank=1 --master_addr=della-j12g2 --master_port=29500 src/train.py --model_name_or_path /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16 --trust_remote_code true --stage sft --do_train --finetuning_type full --deepspeed /scratch/gpfs/yl7690/projects/LLaMA-Factory-new/examples/deepspeed/ds_z3_offload_config.json --dataset dxy --template gpt --cutoff_len 64000 --max_samples 100000000 --overwrite_cache true --preprocessing_num_workers 64 --dataloader_num_workers 64 --output_dir /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune --logging_steps 10 --save_steps 100 --plot_loss true --overwrite_output_dir true --save_only_model true --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --learning_rate 5.0e-5 --num_train_epochs 1.0 --lr_scheduler_type cosine --warmup_ratio 0.1 --bf16 true --ddp_timeout 180000000 --val_size 0.01 --per_device_eval_batch_size 1 --eval_strategy steps --eval_steps 500 --flash_attn fa2 --enable_liger_kernel true
W1003 09:52:13.103000 1143917 site-packages/torch/distributed/run.py:774] 
W1003 09:52:13.103000 1143917 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:13.103000 1143917 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 09:52:13.103000 1143917 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:13.167000 3677347 site-packages/torch/distributed/run.py:774] 
W1003 09:52:13.167000 3677347 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:13.167000 3677347 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 09:52:13.167000 3677347 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:13.217000 2365920 site-packages/torch/distributed/run.py:774] 
W1003 09:52:13.217000 2365920 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:13.217000 2365920 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 09:52:13.217000 2365920 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:13.274000 1562569 site-packages/torch/distributed/run.py:774] 
W1003 09:52:13.274000 1562569 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:13.274000 1562569 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 09:52:13.274000 1562569 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:14.550000 3091358 site-packages/torch/distributed/run.py:774] 
W1003 09:52:14.550000 3091358 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:14.550000 3091358 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 09:52:14.550000 3091358 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:14.550000 2847256 site-packages/torch/distributed/run.py:774] 
W1003 09:52:14.550000 2847256 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:14.550000 2847256 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 09:52:14.550000 2847256 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:14.551000 3196428 site-packages/torch/distributed/run.py:774] 
W1003 09:52:14.551000 3196428 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:14.551000 3196428 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 09:52:14.551000 3196428 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:14.632000 2289967 site-packages/torch/distributed/run.py:774] 
W1003 09:52:14.632000 2289967 site-packages/torch/distributed/run.py:774] *****************************************
W1003 09:52:14.632000 2289967 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1003 09:52:14.632000 2289967 site-packages/torch/distributed/run.py:774] *****************************************
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,747 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,747 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,747 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,747 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,747 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,747 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,748 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,748 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,748 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,748 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,748 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,748 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,748 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,748 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,748 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,748 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,748 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,748 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,749 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,749 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,749 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,749 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,749 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,749 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,751 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,751 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,751 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,751 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,751 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,751 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,757 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,757 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,757 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,757 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,757 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,757 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,769 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,769 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,769 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,769 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,770 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,770 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,772 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,772 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,772 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,772 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,772 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:41,772 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:42,395 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 09:52:42,396 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 09:52:42,405 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,405 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,405 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,405 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,405 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,405 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,405 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:42,410 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 09:52:42,410 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 09:52:42,416 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,417 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,417 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,417 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,417 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,417 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,417 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:42,417 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 09:52:42,418 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 09:52:42,421 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,422 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,422 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,422 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,422 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,422 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,422 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:42,426 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 09:52:42,427 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 09:52:42,429 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,429 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,429 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,429 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,429 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,429 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,429 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:42,432 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 09:52:42,432 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 09:52:42,435 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,435 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,435 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,435 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,435 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,435 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,435 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:42,437 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 09:52:42,438 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:42,439 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:839] 2025-10-03 09:52:42,440 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-03 09:52:42,440 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,440 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,440 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,440 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,440 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,440 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,440 >> loading file chat_template.jinja
[INFO|configuration_utils.py:839] 2025-10-03 09:52:42,443 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,443 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,443 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,443 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,443 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,443 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,443 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:42,449 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-10-03 09:52:42,450 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 09:52:42,452 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,453 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,453 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,453 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,453 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,453 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2066] 2025-10-03 09:52:42,453 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:43,063 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:43,114 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:43,117 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:43,136 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:43,148 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:43,149 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:43,158 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2337] 2025-10-03 09:52:43,158 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Converting format of dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 12102 examples [00:01,  1.77 examples/s]            Converting format of dataset (num_proc=64): 12861 examples [00:01, 840.50 examples/s]Converting format of dataset (num_proc=64): 18497 examples [00:01, 8411.46 examples/s]Converting format of dataset (num_proc=64): 24152 examples [00:01, 16123.79 examples/s]Converting format of dataset (num_proc=64): 24200 examples [00:02, 4193.64 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 12101 examples [00:01,  1.23s/ examples]            Converting format of dataset (num_proc=64): 13241 examples [00:01, 1161.50 examples/s]Converting format of dataset (num_proc=64): 19259 examples [00:01, 8655.64 examples/s]Converting format of dataset (num_proc=64): 24200 examples [00:01, 14835.88 examples/s]Converting format of dataset (num_proc=64): 24200 examples [00:02, 4137.03 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 12128 examples [00:01, 23.74 examples/s]            Converting format of dataset (num_proc=64): 15896 examples [00:01, 4088.70 examples/s]Converting format of dataset (num_proc=64): 18909 examples [00:01, 6751.74 examples/s]Converting format of dataset (num_proc=64): 20997 examples [00:01, 8373.03 examples/s]Converting format of dataset (num_proc=64): 23257 examples [00:01, 10509.37 examples/s]Converting format of dataset (num_proc=64): 24200 examples [00:02, 4092.06 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 12103 examples [00:01,  2.47 examples/s]            Converting format of dataset (num_proc=64): 13807 examples [00:01, 1788.44 examples/s]Converting format of dataset (num_proc=64): 19691 examples [00:01, 9203.63 examples/s]Converting format of dataset (num_proc=64): 22878 examples [00:01, 12639.40 examples/s]Converting format of dataset (num_proc=64): 24200 examples [00:03, 3928.91 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 12105 examples [00:01,  3.66 examples/s]            Converting format of dataset (num_proc=64): 17210 examples [00:01, 4825.26 examples/s]Converting format of dataset (num_proc=64): 20233 examples [00:01, 6202.44 examples/s]Converting format of dataset (num_proc=64): 22440 examples [00:02, 6715.55 examples/s]Converting format of dataset (num_proc=64): 24200 examples [00:03, 3834.02 examples/s]
Converting format of dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 12104 examples [00:01,  3.38 examples/s]            Converting format of dataset (num_proc=64): 15702 examples [00:01, 3869.01 examples/s]Converting format of dataset (num_proc=64): 21177 examples [00:01, 10637.13 examples/s]Converting format of dataset (num_proc=64): 24200 examples [00:03, 3831.31 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 12103 examples [00:01,  2.01 examples/s]            Converting format of dataset (num_proc=64): 13055 examples [00:01, 828.97 examples/s]Converting format of dataset (num_proc=64): 14752 examples [00:01, 2599.84 examples/s]Converting format of dataset (num_proc=64): 15913 examples [00:01, 3589.00 examples/s]Converting format of dataset (num_proc=64): 23340 examples [00:01, 14381.84 examples/s]Converting format of dataset (num_proc=64): 24200 examples [00:03, 3801.88 examples/s] 
Converting format of dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64): 12101 examples [00:01,  1.21s/ examples]            Converting format of dataset (num_proc=64): 15515 examples [00:01, 3585.07 examples/s]Converting format of dataset (num_proc=64): 20988 examples [00:01, 9874.90 examples/s]Converting format of dataset (num_proc=64): 24012 examples [00:02, 4265.46 examples/s]Converting format of dataset (num_proc=64): 24200 examples [00:03, 3623.11 examples/s]
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Running tokenizer on dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 12290 examples [00:08, 22.70 examples/s]            Running tokenizer on dataset (num_proc=64): 12669 examples [00:08, 83.89 examples/s]Running tokenizer on dataset (num_proc=64): 12859 examples [00:08, 121.54 examples/s]Running tokenizer on dataset (num_proc=64): 13049 examples [00:09, 167.58 examples/s]Running tokenizer on dataset (num_proc=64): 13238 examples [00:09, 224.15 examples/s]Running tokenizer on dataset (num_proc=64): 13427 examples [00:09, 290.99 examples/s]Running tokenizer on dataset (num_proc=64): 13616 examples [00:10, 317.08 examples/s]Running tokenizer on dataset (num_proc=64): 13805 examples [00:10, 387.25 examples/s]Running tokenizer on dataset (num_proc=64): 14183 examples [00:10, 597.51 examples/s]Running tokenizer on dataset (num_proc=64): 14561 examples [00:10, 795.91 examples/s]Running tokenizer on dataset (num_prRunning tokenizer on dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 12290 examples [00:08, 22.88 examples/s]            Running tokenizer on dataset (num_proc=64): 12669 examples [00:08, 84.79 examples/s]Running tokenizer on dataset (num_proc=64): 12859 examples [00:08, 122.98 examples/s]Running tokenizer on dataset (num_proc=64): 13049 examples [00:09, 170.33 examples/s]Running tokenizer on dataset (num_proc=64): 13238 examples [00:09, 207.43 examples/s]Running tokenizer on dataset (num_proc=64): 13616 examples [00:09, 318.74 examples/s]Running tokenizer on dataset (num_proc=64): 13805 examples [00:10, 374.41 examples/s]Running tokenizer on dataset (num_proc=64): 14183 examples [00:10, 549.37 examples/s]Running tokenizer on dataset (num_proc=64): 14372 examples [00:10, 589.51 examples/s]Running tokenizer on dataset (num_proc=64): 14750 examples [00:10, 782.00 examples/s]Running tokenizer on dataset (num_prRunning tokenizer on dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 12290 examples [00:08, 22.87 examples/s]            Running tokenizer on dataset (num_proc=64): 12670 examples [00:08, 84.64 examples/s]Running tokenizer on dataset (num_proc=64): 12859 examples [00:08, 122.12 examples/s]Running tokenizer on dataset (num_proc=64): 13049 examples [00:09, 169.31 examples/s]Running tokenizer on dataset (num_proc=64): 13238 examples [00:09, 224.51 examples/s]Running tokenizer on dataset (num_proc=64): 13427 examples [00:09, 290.31 examples/s]Running tokenizer on dataset (num_proc=64): 13616 examples [00:10, 313.54 examples/s]Running tokenizer on dataset (num_proc=64): 13805 examples [00:10, 382.89 examples/s]Running tokenizer on dataset (num_proc=64): 14183 examples [00:10, 588.06 examples/s]Running tokenizer on dataset (num_proc=64): 14561 examples [00:11, 645.33 examples/s]Running tokenizer on dataset (num_prRunning tokenizer on dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 12290 examples [00:08, 22.83 examples/s]            Running tokenizer on dataset (num_proc=64): 12480 examples [00:08, 53.28 examples/s]Running tokenizer on dataset (num_proc=64): 12859 examples [00:08, 134.57 examples/s]Running tokenizer on dataset (num_proc=64): 13049 examples [00:09, 168.28 examples/s]Running tokenizer on dataset (num_proc=64): 13238 examples [00:09, 220.42 examples/s]Running tokenizer on dataset (num_proc=64): 13427 examples [00:09, 280.59 examples/s]Running tokenizer on dataset (num_proc=64): 13616 examples [00:10, 302.93 examples/s]Running tokenizer on dataset (num_proc=64): 14183 examples [00:10, 591.97 examples/s]Running tokenizer on dataset (num_proc=64): 14372 examples [00:10, 621.61 examples/s]Running tokenizer on dataset (num_proc=64): 14750 examples [00:11, 798.12 examples/s]Running tokenizer on dataset (num_prRunning tokenizer on dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 12290 examples [00:07, 24.37 examples/s]            Running tokenizer on dataset (num_proc=64): 12670 examples [00:08, 86.66 examples/s]Running tokenizer on dataset (num_proc=64): 12859 examples [00:08, 125.20 examples/s]Running tokenizer on dataset (num_proc=64): 13048 examples [00:08, 162.16 examples/s]Running tokenizer on dataset (num_proc=64): 13616 examples [00:09, 329.89 examples/s]Running tokenizer on dataset (num_proc=64): 13805 examples [00:09, 344.59 examples/s]Running tokenizer on dataset (num_proc=64): 14183 examples [00:10, 493.36 examples/s]Running tokenizer on dataset (num_proc=64): 14561 examples [00:10, 655.32 examples/s]Running tokenizer on dataset (num_proc=64): 14939 examples [00:10, 821.49 examples/s]Running tokenizer on dataset (num_proc=64): 15317 examples [00:10, 980.03 examples/s]Running tokenizer on dataset (num_prRunning tokenizer on dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 12290 examples [00:08, 23.25 examples/s]            Running tokenizer on dataset (num_proc=64): 12480 examples [00:08, 54.28 examples/s]Running tokenizer on dataset (num_proc=64): 12859 examples [00:08, 137.37 examples/s]Running tokenizer on dataset (num_proc=64): 13049 examples [00:08, 184.44 examples/s]Running tokenizer on dataset (num_proc=64): 13238 examples [00:09, 219.66 examples/s]Running tokenizer on dataset (num_proc=64): 13616 examples [00:10, 299.96 examples/s]Running tokenizer on dataset (num_proc=64): 14183 examples [00:10, 530.84 examples/s]Running tokenizer on dataset (num_proc=64): 14372 examples [00:10, 565.66 examples/s]Running tokenizer on dataset (num_proc=64): 14750 examples [00:10, 730.64 examples/s]Running tokenizer on dataset (num_proc=64): 15128 examples [00:11, 890.48 examples/s]Running tokenizer on dataset (num_prRunning tokenizer on dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 12290 examples [00:08, 22.67 examples/s]            Running tokenizer on dataset (num_proc=64): 12669 examples [00:08, 83.76 examples/s]Running tokenizer on dataset (num_proc=64): 13049 examples [00:09, 154.66 examples/s]Running tokenizer on dataset (num_proc=64): 13238 examples [00:09, 183.93 examples/s]Running tokenizer on dataset (num_proc=64): 13427 examples [00:09, 232.68 examples/s]Running tokenizer on dataset (num_proc=64): 13616 examples [00:10, 289.46 examples/s]Running tokenizer on dataset (num_proc=64): 13805 examples [00:10, 351.89 examples/s]Running tokenizer on dataset (num_proc=64): 14372 examples [00:10, 659.26 examples/s]Running tokenizer on dataset (num_proc=64): 14561 examples [00:10, 682.30 examples/s]Running tokenizer on dataset (num_proc=64): 14939 examples [00:11, 861.64 examples/s]Running tokenizer on dataset (num_prRunning tokenizer on dataset (num_proc=64): 100%|██████████| 12100/12100 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64): 12290 examples [00:07, 23.78 examples/s]            Running tokenizer on dataset (num_proc=64): 12669 examples [00:08, 86.80 examples/s]Running tokenizer on dataset (num_proc=64): 13048 examples [00:08, 156.82 examples/s]Running tokenizer on dataset (num_proc=64): 13616 examples [00:09, 279.22 examples/s]Running tokenizer on dataset (num_proc=64): 13805 examples [00:09, 314.44 examples/s]Running tokenizer on dataset (num_proc=64): 14372 examples [00:10, 512.24 examples/s]Running tokenizer on dataset (num_proc=64): 14750 examples [00:10, 627.68 examples/s]Running tokenizer on dataset (num_proc=64): 15128 examples [00:10, 744.02 examples/s]Running tokenizer on dataset (num_proc=64): 15506 examples [00:11, 705.49 examples/s]Running tokenizer on dataset (num_proc=64): 15695 examples [00:11, 703.29 examples/s]Running tokenizer on dataset (num_proc=64): 14939 examples [00:11, 969.80 examples/s]Running tokenizer on dataset (num_proc=64): 15128 examples [00:11, 931.11 examples/s]Running tokenizer on dataset (num_proc=64): 15317 examples [00:11, 869.41 examples/s]Running tokenizer on dataset (num_proc=64): 15506 examples [00:12, 569.51 examples/s]Running tokenizer on dataset (num_proc=64): 15884 examples [00:12, 773.13 examples/s]Running tokenizer on dataset (num_proc=64): 16073 examples [00:12, 777.38 examples/s]Running tokenizer on dataset (num_proc=64): 16451 examples [00:12, 974.53 examples/s]Running tokenizer on dataset (num_proc=64): 16640 examples [00:13, 932.17 examples/s]Running tokenizer on dataset (num_proc=64): 16829 examples [00:13, 895.81 examples/s]Running tokenizer on dataset (num_proc=64): 17207 examples [00:13, 1088.81 examples/s]Running tokenizer on dataset (num_proc=64): 17396 examples [00:14, 785.62 examples/s] Running tokenizer on dataset (num_proc=64): 17585 examples [00:15, 456.74 examples/s]Running tokenizer on datasoc=64): 15128 examples [00:11, 954.15 examples/s]Running tokenizer on dataset (num_proc=64): 15317 examples [00:11, 736.23 examples/s]Running tokenizer on dataset (num_proc=64): 15506 examples [00:12, 612.03 examples/s]Running tokenizer on dataset (num_proc=64): 15695 examples [00:12, 644.42 examples/s]Running tokenizer on dataset (num_proc=64): 16073 examples [00:12, 847.67 examples/s]Running tokenizer on dataset (num_proc=64): 16451 examples [00:12, 1018.51 examples/s]Running tokenizer on dataset (num_proc=64): 16640 examples [00:13, 955.66 examples/s] Running tokenizer on dataset (num_proc=64): 16829 examples [00:13, 909.24 examples/s]Running tokenizer on dataset (num_proc=64): 17207 examples [00:13, 1091.04 examples/s]Running tokenizer on dataset (num_proc=64): 17396 examples [00:14, 783.02 examples/s] Running tokenizer on dataset (num_proc=64): 17585 examples [00:15, 453.15 examples/s]Running tokenizer on dataset (num_proc=64): 18152 examples [00:15, 766.19 examples/s]Running tokenizer on datoc=64): 15506 examples [00:11, 663.80 examples/s]Running tokenizer on dataset (num_proc=64): 15884 examples [00:11, 829.16 examples/s]Running tokenizer on dataset (num_proc=64): 16073 examples [00:11, 843.64 examples/s]Running tokenizer on dataset (num_proc=64): 16262 examples [00:12, 842.24 examples/s]Running tokenizer on dataset (num_proc=64): 16640 examples [00:12, 842.01 examples/s]Running tokenizer on dataset (num_proc=64): 16829 examples [00:12, 839.22 examples/s]Running tokenizer on dataset (num_proc=64): 17207 examples [00:13, 1032.76 examples/s]Running tokenizer on dataset (num_proc=64): 17396 examples [00:13, 649.35 examples/s] Running tokenizer on dataset (num_proc=64): 17585 examples [00:14, 572.26 examples/s]Running tokenizer on dataset (num_proc=64): 17774 examples [00:14, 619.29 examples/s]Running tokenizer on dataset (num_proc=64): 18341 examples [00:14, 833.15 examples/s]Running tokenizer on dataset (num_proc=64): 18908 examples [00:15, 1161.05 examples/s]Running tokenizer on dataoc=64): 15128 examples [00:11, 966.36 examples/s]Running tokenizer on dataset (num_proc=64): 15317 examples [00:11, 763.91 examples/s]Running tokenizer on dataset (num_proc=64): 15506 examples [00:12, 634.74 examples/s]Running tokenizer on dataset (num_proc=64): 15695 examples [00:12, 661.20 examples/s]Running tokenizer on dataset (num_proc=64): 16073 examples [00:12, 851.20 examples/s]Running tokenizer on dataset (num_proc=64): 16451 examples [00:12, 1014.15 examples/s]Running tokenizer on dataset (num_proc=64): 16640 examples [00:13, 952.23 examples/s] Running tokenizer on dataset (num_proc=64): 16829 examples [00:13, 904.54 examples/s]Running tokenizer on dataset (num_proc=64): 17207 examples [00:13, 1080.80 examples/s]Running tokenizer on dataset (num_proc=64): 17396 examples [00:14, 776.29 examples/s] Running tokenizer on dataset (num_proc=64): 17585 examples [00:14, 518.90 examples/s]Running tokenizer on dataset (num_proc=64): 17774 examples [00:15, 567.06 examples/s]Running tokenizer on datoc=64): 15317 examples [00:11, 831.61 examples/s]Running tokenizer on dataset (num_proc=64): 15506 examples [00:12, 676.36 examples/s]Running tokenizer on dataset (num_proc=64): 15695 examples [00:12, 694.32 examples/s]Running tokenizer on dataset (num_proc=64): 16073 examples [00:12, 881.01 examples/s]Running tokenizer on dataset (num_proc=64): 16262 examples [00:12, 852.49 examples/s]Running tokenizer on dataset (num_proc=64): 16451 examples [00:13, 831.57 examples/s]Running tokenizer on dataset (num_proc=64): 16829 examples [00:13, 1025.36 examples/s]Running tokenizer on dataset (num_proc=64): 17207 examples [00:13, 1175.36 examples/s]Running tokenizer on dataset (num_proc=64): 17396 examples [00:14, 824.63 examples/s] Running tokenizer on dataset (num_proc=64): 17585 examples [00:15, 475.03 examples/s]Running tokenizer on dataset (num_proc=64): 17963 examples [00:15, 652.62 examples/s]Running tokenizer on dataset (num_proc=64): 18341 examples [00:15, 824.68 examples/s]Running tokenizer on dataoc=64): 15317 examples [00:11, 717.35 examples/s]Running tokenizer on dataset (num_proc=64): 15506 examples [00:12, 608.59 examples/s]Running tokenizer on dataset (num_proc=64): 15695 examples [00:12, 642.94 examples/s]Running tokenizer on dataset (num_proc=64): 16073 examples [00:12, 692.18 examples/s]Running tokenizer on dataset (num_proc=64): 16451 examples [00:12, 868.75 examples/s]Running tokenizer on dataset (num_proc=64): 16829 examples [00:13, 1024.94 examples/s]Running tokenizer on dataset (num_proc=64): 17207 examples [00:13, 1157.11 examples/s]Running tokenizer on dataset (num_proc=64): 17396 examples [00:13, 840.61 examples/s] Running tokenizer on dataset (num_proc=64): 17585 examples [00:14, 492.03 examples/s]Running tokenizer on dataset (num_proc=64): 17963 examples [00:15, 662.62 examples/s]Running tokenizer on dataset (num_proc=64): 18341 examples [00:15, 828.79 examples/s]Running tokenizer on dataset (num_proc=64): 18530 examples [00:15, 818.91 examples/s]Running tokenizer on dataoc=64): 15128 examples [00:11, 958.54 examples/s]Running tokenizer on dataset (num_proc=64): 15317 examples [00:11, 737.26 examples/s]Running tokenizer on dataset (num_proc=64): 15506 examples [00:12, 609.17 examples/s]Running tokenizer on dataset (num_proc=64): 15695 examples [00:12, 638.25 examples/s]Running tokenizer on dataset (num_proc=64): 16073 examples [00:12, 835.27 examples/s]Running tokenizer on dataset (num_proc=64): 16451 examples [00:13, 1002.57 examples/s]Running tokenizer on dataset (num_proc=64): 16640 examples [00:13, 940.72 examples/s] Running tokenizer on dataset (num_proc=64): 16829 examples [00:13, 894.69 examples/s]Running tokenizer on dataset (num_proc=64): 17207 examples [00:13, 1073.00 examples/s]Running tokenizer on dataset (num_proc=64): 17396 examples [00:14, 772.83 examples/s] Running tokenizer on dataset (num_proc=64): 17585 examples [00:15, 443.34 examples/s]Running tokenizer on dataset (num_proc=64): 18341 examples [00:15, 748.63 examples/s]Running tokenizer on datoc=64): 16073 examples [00:11, 832.50 examples/s]Running tokenizer on dataset (num_proc=64): 16262 examples [00:12, 792.22 examples/s]Running tokenizer on dataset (num_proc=64): 16451 examples [00:12, 758.35 examples/s]Running tokenizer on dataset (num_proc=64): 17207 examples [00:12, 1256.04 examples/s]Running tokenizer on dataset (num_proc=64): 17396 examples [00:13, 860.47 examples/s] Running tokenizer on dataset (num_proc=64): 17585 examples [00:13, 658.10 examples/s]Running tokenizer on dataset (num_proc=64): 17774 examples [00:14, 658.67 examples/s]Running tokenizer on dataset (num_proc=64): 18341 examples [00:14, 968.95 examples/s]Running tokenizer on dataset (num_proc=64): 18530 examples [00:14, 894.73 examples/s]Running tokenizer on dataset (num_proc=64): 18719 examples [00:14, 837.16 examples/s]Running tokenizer on dataset (num_proc=64): 19097 examples [00:15, 766.89 examples/s]Running tokenizer on dataset (num_proc=64): 19475 examples [00:15, 899.08 examples/s]Running tokenizer on dataset (num_proc=64): 18341 examples [00:15, 777.00 examples/s]Running tokenizer on dataset (num_proc=64): 18719 examples [00:15, 914.45 examples/s]Running tokenizer on dataset (num_proc=64): 18908 examples [00:15, 982.17 examples/s]Running tokenizer on dataset (num_proc=64): 19286 examples [00:16, 964.27 examples/s]Running tokenizer on dataset (num_proc=64): 19475 examples [00:16, 981.91 examples/s]Running tokenizer on dataset (num_proc=64): 19664 examples [00:16, 1051.05 examples/s]Running tokenizer on dataset (num_proc=64): 19853 examples [00:17, 789.65 examples/s] Running tokenizer on dataset (num_proc=64): 20042 examples [00:17, 489.43 examples/s]Running tokenizer on dataset (num_proc=64): 20420 examples [00:18, 689.50 examples/s]Running tokenizer on dataset (num_proc=64): 20609 examples [00:18, 773.64 examples/s]Running tokenizer on dataset (num_proc=64): 20798 examples [00:18, 550.40 examples/s]Running tokenizer on dataset (num_proc=64): 21176 examples [00:19, 559.38 examples/s]Running tokenizeaset (num_proc=64): 18341 examples [00:15, 766.75 examples/s]Running tokenizer on dataset (num_proc=64): 18719 examples [00:15, 937.73 examples/s]Running tokenizer on dataset (num_proc=64): 18908 examples [00:16, 900.18 examples/s]Running tokenizer on dataset (num_proc=64): 19286 examples [00:16, 995.12 examples/s]Running tokenizer on dataset (num_proc=64): 19475 examples [00:16, 1076.15 examples/s]Running tokenizer on dataset (num_proc=64): 19664 examples [00:16, 1047.92 examples/s]Running tokenizer on dataset (num_proc=64): 19853 examples [00:17, 794.22 examples/s] Running tokenizer on dataset (num_proc=64): 20042 examples [00:17, 478.01 examples/s]Running tokenizer on dataset (num_proc=64): 20420 examples [00:18, 636.47 examples/s]Running tokenizer on dataset (num_proc=64): 20609 examples [00:18, 729.97 examples/s]Running tokenizer on dataset (num_proc=64): 20798 examples [00:19, 523.65 examples/s]Running tokenizer on dataset (num_proc=64): 21176 examples [00:19, 533.84 examples/s]Running tokenset (num_proc=64): 18719 examples [00:15, 981.19 examples/s]Running tokenizer on dataset (num_proc=64): 18908 examples [00:16, 887.03 examples/s]Running tokenizer on dataset (num_proc=64): 19286 examples [00:16, 919.99 examples/s]Running tokenizer on dataset (num_proc=64): 19475 examples [00:16, 969.00 examples/s]Running tokenizer on dataset (num_proc=64): 19664 examples [00:16, 1018.99 examples/s]Running tokenizer on dataset (num_proc=64): 19853 examples [00:17, 746.97 examples/s] Running tokenizer on dataset (num_proc=64): 20042 examples [00:18, 465.54 examples/s]Running tokenizer on dataset (num_proc=64): 20420 examples [00:18, 643.54 examples/s]Running tokenizer on dataset (num_proc=64): 20609 examples [00:18, 686.50 examples/s]Running tokenizer on dataset (num_proc=64): 20798 examples [00:19, 559.16 examples/s]Running tokenizer on dataset (num_proc=64): 21176 examples [00:19, 541.35 examples/s]Running tokenizer on dataset (num_proc=64): 21554 examples [00:20, 729.86 examples/s]Running tokenizaset (num_proc=64): 18719 examples [00:16, 879.10 examples/s]Running tokenizer on dataset (num_proc=64): 18908 examples [00:16, 840.79 examples/s]Running tokenizer on dataset (num_proc=64): 19286 examples [00:16, 900.22 examples/s]Running tokenizer on dataset (num_proc=64): 19664 examples [00:16, 975.46 examples/s]Running tokenizer on dataset (num_proc=64): 19853 examples [00:17, 808.16 examples/s]Running tokenizer on dataset (num_proc=64): 20042 examples [00:18, 525.14 examples/s]Running tokenizer on dataset (num_proc=64): 20231 examples [00:18, 610.64 examples/s]Running tokenizer on dataset (num_proc=64): 20420 examples [00:18, 685.69 examples/s]Running tokenizer on dataset (num_proc=64): 20609 examples [00:18, 807.49 examples/s]Running tokenizer on dataset (num_proc=64): 20798 examples [00:19, 516.26 examples/s]Running tokenizer on dataset (num_proc=64): 21176 examples [00:20, 546.26 examples/s]Running tokenizer on dataset (num_proc=64): 21365 examples [00:20, 656.44 examples/s]Running tokenizeset (num_proc=64): 19286 examples [00:15, 1049.63 examples/s]Running tokenizer on dataset (num_proc=64): 19475 examples [00:15, 902.10 examples/s] Running tokenizer on dataset (num_proc=64): 19664 examples [00:16, 992.09 examples/s]Running tokenizer on dataset (num_proc=64): 19853 examples [00:16, 960.10 examples/s]Running tokenizer on dataset (num_proc=64): 20042 examples [00:17, 514.11 examples/s]Running tokenizer on dataset (num_proc=64): 20420 examples [00:17, 638.75 examples/s]Running tokenizer on dataset (num_proc=64): 20798 examples [00:17, 732.34 examples/s]Running tokenizer on dataset (num_proc=64): 21176 examples [00:18, 624.76 examples/s]Running tokenizer on dataset (num_proc=64): 21743 examples [00:18, 963.49 examples/s]Running tokenizer on dataset (num_proc=64): 21932 examples [00:19, 637.91 examples/s]Running tokenizer on dataset (num_proc=64): 22310 examples [00:19, 867.82 examples/s]Running tokenizer on dataset (num_proc=64): 22877 examples [00:20, 1102.31 examples/s]Running tokeniaset (num_proc=64): 18341 examples [00:15, 769.00 examples/s]Running tokenizer on dataset (num_proc=64): 18908 examples [00:16, 897.86 examples/s]Running tokenizer on dataset (num_proc=64): 19286 examples [00:16, 1045.36 examples/s]Running tokenizer on dataset (num_proc=64): 19475 examples [00:16, 1028.85 examples/s]Running tokenizer on dataset (num_proc=64): 19664 examples [00:16, 1056.84 examples/s]Running tokenizer on dataset (num_proc=64): 19853 examples [00:17, 787.37 examples/s] Running tokenizer on dataset (num_proc=64): 20042 examples [00:18, 481.47 examples/s]Running tokenizer on dataset (num_proc=64): 20420 examples [00:18, 668.02 examples/s]Running tokenizer on dataset (num_proc=64): 20798 examples [00:19, 560.27 examples/s]Running tokenizer on dataset (num_proc=64): 21176 examples [00:19, 567.63 examples/s]Running tokenizer on dataset (num_proc=64): 21365 examples [00:20, 617.82 examples/s]Running tokenizer on dataset (num_proc=64): 21743 examples [00:20, 851.23 examples/s]Running tokeset (num_proc=64): 18908 examples [00:15, 981.37 examples/s]Running tokenizer on dataset (num_proc=64): 19286 examples [00:16, 1025.09 examples/s]Running tokenizer on dataset (num_proc=64): 19475 examples [00:16, 1033.97 examples/s]Running tokenizer on dataset (num_proc=64): 19664 examples [00:16, 1025.45 examples/s]Running tokenizer on dataset (num_proc=64): 19853 examples [00:17, 763.57 examples/s] Running tokenizer on dataset (num_proc=64): 20042 examples [00:17, 477.31 examples/s]Running tokenizer on dataset (num_proc=64): 20420 examples [00:18, 667.80 examples/s]Running tokenizer on dataset (num_proc=64): 20798 examples [00:18, 594.52 examples/s]Running tokenizer on dataset (num_proc=64): 21176 examples [00:19, 572.36 examples/s]Running tokenizer on dataset (num_proc=64): 21365 examples [00:19, 646.46 examples/s]Running tokenizer on dataset (num_proc=64): 21743 examples [00:19, 848.17 examples/s]Running tokenizer on dataset (num_proc=64): 21932 examples [00:20, 597.79 examples/s]Running tokenet (num_proc=64): 19853 examples [00:16, 804.60 examples/s]Running tokenizer on dataset (num_proc=64): 20042 examples [00:17, 595.13 examples/s]Running tokenizer on dataset (num_proc=64): 20231 examples [00:17, 612.67 examples/s]Running tokenizer on dataset (num_proc=64): 20420 examples [00:17, 694.25 examples/s]Running tokenizer on dataset (num_proc=64): 20609 examples [00:17, 597.73 examples/s]Running tokenizer on dataset (num_proc=64): 20798 examples [00:18, 556.18 examples/s]Running tokenizer on dataset (num_proc=64): 20987 examples [00:18, 683.47 examples/s]Running tokenizer on dataset (num_proc=64): 21176 examples [00:19, 462.71 examples/s]Running tokenizer on dataset (num_proc=64): 21365 examples [00:19, 450.04 examples/s]Running tokenizer on dataset (num_proc=64): 21743 examples [00:19, 669.90 examples/s]Running tokenizer on dataset (num_proc=64): 21932 examples [00:20, 671.35 examples/s]Running tokenizer on dataset (num_proc=64): 22121 examples [00:20, 522.63 examples/s]Running tokenizer zer on dataset (num_proc=64): 23066 examples [00:20, 1091.08 examples/s]Running tokenizer on dataset (num_proc=64): 23255 examples [00:20, 1183.73 examples/s]Running tokenizer on dataset (num_proc=64): 23633 examples [00:20, 1293.00 examples/s]Running tokenizer on dataset (num_proc=64): 23822 examples [00:20, 1141.67 examples/s]Running tokenizer on dataset (num_proc=64): 24011 examples [00:21, 1097.88 examples/s]Running tokenizer on dataset (num_proc=64): 24200 examples [00:21, 1091.32 examples/s]Running tokenizer on dataset (num_proc=64): 24200 examples [00:21, 558.80 examples/s] 
r on dataset (num_proc=64): 21365 examples [00:19, 620.21 examples/s]Running tokenizer on dataset (num_proc=64): 21743 examples [00:19, 867.12 examples/s]Running tokenizer on dataset (num_proc=64): 21932 examples [00:20, 628.60 examples/s]Running tokenizer on dataset (num_proc=64): 22310 examples [00:20, 890.11 examples/s]Running tokenizer on dataset (num_proc=64): 22877 examples [00:20, 1210.19 examples/s]Running tokenizer on dataset (num_proc=64): 23066 examples [00:21, 1130.91 examples/s]Running tokenizer on dataset (num_proc=64): 23255 examples [00:21, 1142.92 examples/s]Running tokenizer on dataset (num_proc=64): 23633 examples [00:21, 1522.85 examples/s]Running tokenizer on dataset (num_proc=64): 24011 examples [00:22, 1014.83 examples/s]Running tokenizer on dataset (num_proc=64): 24200 examples [00:22, 937.71 examples/s] Running tokenizer on dataset (num_proc=64): 24200 examples [00:22, 531.22 examples/s]
izer on dataset (num_proc=64): 22499 examples [00:20, 943.33 examples/s]Running tokenizer on dataset (num_proc=64): 22877 examples [00:21, 1162.18 examples/s]Running tokenizer on dataset (num_proc=64): 23066 examples [00:21, 1032.39 examples/s]Running tokenizer on dataset (num_proc=64): 23255 examples [00:21, 1045.36 examples/s]Running tokenizer on dataset (num_proc=64): 23633 examples [00:21, 1403.70 examples/s]Running tokenizer on dataset (num_proc=64): 24011 examples [00:22, 997.15 examples/s] Running tokenizer on dataset (num_proc=64): 24200 examples [00:22, 876.50 examples/s]Running tokenizer on dataset (num_proc=64): 24200 examples [00:22, 526.45 examples/s]
izer on dataset (num_proc=64): 21554 examples [00:20, 704.11 examples/s]Running tokenizer on dataset (num_proc=64): 21743 examples [00:20, 788.56 examples/s]Running tokenizer on dataset (num_proc=64): 21932 examples [00:20, 607.98 examples/s]Running tokenizer on dataset (num_proc=64): 22121 examples [00:20, 698.64 examples/s]Running tokenizer on dataset (num_proc=64): 22499 examples [00:20, 1049.23 examples/s]Running tokenizer on dataset (num_proc=64): 22877 examples [00:21, 1311.76 examples/s]Running tokenizer on dataset (num_proc=64): 23255 examples [00:21, 1093.91 examples/s]Running tokenizer on dataset (num_proc=64): 23633 examples [00:21, 1370.96 examples/s]Running tokenizer on dataset (num_proc=64): 24011 examples [00:22, 1029.46 examples/s]Running tokenizer on dataset (num_proc=64): 24200 examples [00:22, 957.11 examples/s] Running tokenizer on dataset (num_proc=64): 24200 examples [00:22, 526.10 examples/s]
er on dataset (num_proc=64): 21743 examples [00:20, 823.96 examples/s]Running tokenizer on dataset (num_proc=64): 21932 examples [00:20, 591.46 examples/s]Running tokenizer on dataset (num_proc=64): 22499 examples [00:20, 966.50 examples/s]Running tokenizer on dataset (num_proc=64): 22877 examples [00:21, 1177.65 examples/s]Running tokenizer on dataset (num_proc=64): 23066 examples [00:21, 1090.19 examples/s]Running tokenizer on dataset (num_proc=64): 23255 examples [00:21, 1069.20 examples/s]Running tokenizer on dataset (num_proc=64): 23633 examples [00:21, 1399.22 examples/s]Running tokenizer on dataset (num_proc=64): 23822 examples [00:22, 996.53 examples/s] Running tokenizer on dataset (num_proc=64): 24011 examples [00:22, 1027.37 examples/s]Running tokenizer on dataset (num_proc=64): 24200 examples [00:22, 878.65 examples/s] Running tokenizer on dataset (num_proc=64): 24200 examples [00:23, 524.83 examples/s]
on dataset (num_proc=64): 22310 examples [00:20, 619.16 examples/s]Running tokenizer on dataset (num_proc=64): 22877 examples [00:21, 1118.94 examples/s]Running tokenizer on dataset (num_proc=64): 23066 examples [00:21, 1043.56 examples/s]Running tokenizer on dataset (num_proc=64): 23255 examples [00:21, 1092.93 examples/s]Running tokenizer on dataset (num_proc=64): 23633 examples [00:21, 1373.76 examples/s]Running tokenizer on dataset (num_proc=64): 23822 examples [00:21, 988.13 examples/s] Running tokenizer on dataset (num_proc=64): 24011 examples [00:22, 790.63 examples/s]Running tokenizer on dataset (num_proc=64): 24200 examples [00:22, 655.74 examples/s]Running tokenizer on dataset (num_proc=64): 24200 examples [00:23, 523.23 examples/s]
nizer on dataset (num_proc=64): 21932 examples [00:20, 634.79 examples/s]Running tokenizer on dataset (num_proc=64): 22121 examples [00:20, 690.84 examples/s]Running tokenizer on dataset (num_proc=64): 22688 examples [00:21, 1206.38 examples/s]Running tokenizer on dataset (num_proc=64): 23066 examples [00:21, 1140.82 examples/s]Running tokenizer on dataset (num_proc=64): 23444 examples [00:21, 1200.34 examples/s]Running tokenizer on dataset (num_proc=64): 23822 examples [00:22, 1048.42 examples/s]Running tokenizer on dataset (num_proc=64): 24011 examples [00:22, 1019.73 examples/s]Running tokenizer on dataset (num_proc=64): 24200 examples [00:22, 925.23 examples/s] Running tokenizer on dataset (num_proc=64): 24200 examples [00:23, 522.44 examples/s]
r on dataset (num_proc=64): 21554 examples [00:20, 772.99 examples/s]Running tokenizer on dataset (num_proc=64): 21743 examples [00:20, 860.68 examples/s]Running tokenizer on dataset (num_proc=64): 21932 examples [00:20, 613.15 examples/s]Running tokenizer on dataset (num_proc=64): 22121 examples [00:21, 685.71 examples/s]Running tokenizer on dataset (num_proc=64): 22688 examples [00:21, 1318.34 examples/s]Running tokenizer on dataset (num_proc=64): 23066 examples [00:21, 1222.88 examples/s]Running tokenizer on dataset (num_proc=64): 23444 examples [00:21, 1214.84 examples/s]Running tokenizer on dataset (num_proc=64): 23822 examples [00:22, 1090.70 examples/s]Running tokenizer on dataset (num_proc=64): 24011 examples [00:22, 1078.91 examples/s]Running tokenizer on dataset (num_proc=64): 24200 examples [00:22, 948.85 examples/s] Running tokenizer on dataset (num_proc=64): 24200 examples [00:23, 518.81 examples/s]
[INFO|configuration_utils.py:763] 2025-10-03 09:53:11,828 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 09:53:11,828 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 09:53:11,828 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 09:53:11,828 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 09:53:11,828 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 09:53:11,828 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:763] 2025-10-03 09:53:11,828 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 09:53:11,829 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-03 09:53:11,829 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|configuration_utils.py:839] 2025-10-03 09:53:11,829 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|configuration_utils.py:839] 2025-10-03 09:53:11,829 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|configuration_utils.py:839] 2025-10-03 09:53:11,829 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:839] 2025-10-03 09:53:11,829 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
[INFO|configuration_utils.py:839] 2025-10-03 09:53:11,829 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|configuration_utils.py:763] 2025-10-03 09:53:11,829 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/config.json
[INFO|configuration_utils.py:839] 2025-10-03 09:53:11,830 >> Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "use_cache": true,
  "vocab_size": 201088
}

[INFO|modeling_utils.py:1277] 2025-10-03 09:53:12,279 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 09:53:12,280 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-03 09:53:12,293 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|modeling_utils.py:1277] 2025-10-03 09:53:12,343 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 09:53:12,344 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-03 09:53:12,345 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 09:53:12,345 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-03 09:53:12,356 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|modeling_utils.py:1277] 2025-10-03 09:53:12,364 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 09:53:12,365 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-03 09:53:12,369 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 09:53:12,390 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|modeling_utils.py:1277] 2025-10-03 09:53:12,431 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 09:53:12,432 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-03 09:53:12,451 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 09:53:12,452 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-03 09:53:12,456 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|modeling_utils.py:1277] 2025-10-03 09:53:12,461 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 09:53:12,461 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|modeling_utils.py:1277] 2025-10-03 09:53:12,463 >> loading weights file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/model.safetensors.index.json
[INFO|modeling_utils.py:4492] 2025-10-03 09:53:12,463 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1055] 2025-10-03 09:53:12,464 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 09:53:12,475 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

[INFO|configuration_utils.py:1055] 2025-10-03 09:53:12,487 >> Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999,
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.11s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:43,  6.16s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.00s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.01s/it]Loading checkpoint shards:  11%|█         | 1/9 [00:06<00:48,  6.00s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.12s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:42,  6.11s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]L37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]L37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]L37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.22s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:24<00:29,  5.91s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]L37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]L37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]L37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]L37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:18<00:37,  6.21s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:23<00:29,  5.90s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:30<00:24,  6.05s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██oading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
████████| 9/9 [00:51<00:00,  5.76s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 09:54:05,918 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
[INFO|modeling_utils.py:5732] 2025-10-03 09:54:05,918 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 09:54:05,919 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 09:54:05,919 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██oading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]
████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
oading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██[INFO|modeling_utils.py:5724] 2025-10-03 09:54:05,919 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
[INFO|modeling_utils.py:5732] 2025-10-03 09:54:05,919 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]

oading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 09:54:05,920 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

oading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:36<00:18,  6.15s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:42<00:12,  6.18s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:49<00:06,  6.23s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██[INFO|modeling_utils.py:5732] 2025-10-03 09:54:05,920 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 09:54:05,920 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.11s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 09:54:05,920 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
[INFO|modeling_utils.py:5732] 2025-10-03 09:54:05,920 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|modeling_utils.py:5732] 2025-10-03 09:54:05,920 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.

Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 09:54:05,920 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 09:54:05,920 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1008] 2025-10-03 09:54:05,920 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1008] 2025-10-03 09:54:05,920 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 09:54:05,920 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1055] 2025-10-03 09:54:05,921 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-03 09:54:05,921 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1008] 2025-10-03 09:54:05,921 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 09:54:05,921 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-03 09:54:05,921 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1008] 2025-10-03 09:54:05,921 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 09:54:05,921 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1055] 2025-10-03 09:54:05,922 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1008] 2025-10-03 09:54:05,922 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 09:54:05,922 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|configuration_utils.py:1055] 2025-10-03 09:54:05,922 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 09:54:05,943 >> Using auto half precision backend
[INFO|trainer.py:757] 2025-10-03 09:54:05,943 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 09:54:05,943 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 09:54:05,943 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 09:54:05,944 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 09:54:05,945 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 09:54:05,947 >> Using auto half precision backend
[INFO|trainer.py:757] 2025-10-03 09:54:05,947 >> Using auto half precision backend
[INFO|trainer.py:757] 2025-10-03 09:54:05,947 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 09:54:05,948 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 09:54:05,948 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 09:54:05,948 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[INFO|trainer.py:757] 2025-10-03 09:54:05,949 >> Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
[WARNING|trainer.py:985] 2025-10-03 09:54:05,949 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.10s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:51<00:00,  5.76s/it]
[INFO|modeling_utils.py:5724] 2025-10-03 09:54:05,953 >> All model checkpoint weights were used when initializing GptOssForCausalLM.

[INFO|modeling_utils.py:5732] 2025-10-03 09:54:05,953 >> All the weights of GptOssForCausalLM were initialized from the model checkpoint at /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GptOssForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1008] 2025-10-03 09:54:05,954 >> loading configuration file /scratch/gpfs/PLI/yong/gpt-oss-20b-bf16/generation_config.json
[INFO|configuration_utils.py:1055] 2025-10-03 09:54:05,954 >> Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}

[INFO|trainer.py:757] 2025-10-03 09:54:05,973 >> Using auto half precision backend
[WARNING|trainer.py:985] 2025-10-03 09:54:05,976 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 09:54:06,214 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 09:54:06,219 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 09:54:06,225 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 09:54:06,235 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 09:54:06,238 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 09:54:06,242 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|deepspeed.py:380] 2025-10-03 09:54:06,243 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 2. Using DeepSpeed's value.
[INFO|deepspeed.py:380] 2025-10-03 09:54:06,260 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[rank14]:W1003 09:54:09.985000 1562620 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank14]:W1003 09:54:09.985000 1562620 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank13]:W1003 09:54:10.032000 1562619 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank13]:W1003 09:54:10.032000 1562619 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank2]:W1003 09:54:10.057000 2365986 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank2]:W1003 09:54:10.057000 2365986 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank18]:W1003 09:54:10.137000 2847312 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank18]:W1003 09:54:10.137000 2847312 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[rank5]:W1003 09:54:10.589000 2290019 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank5]:W1003 09:54:10.589000 2290019 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[INFO|trainer.py:2523] 2025-10-03 09:54:18,864 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 09:54:18,864 >>   Num examples = 11,979
[INFO|trainer.py:2525] 2025-10-03 09:54:18,864 >>   Num Epochs = 1
[INFO|trainer.py:2523] 2025-10-03 09:54:18,864 >> ***** Running training *****
[INFO|trainer.py:2526] 2025-10-03 09:54:18,864 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 09:54:18,864 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2530] 2025-10-03 09:54:18,864 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2524] 2025-10-03 09:54:18,864 >>   Num examples = 11,979
[INFO|trainer.py:2531] 2025-10-03 09:54:18,864 >>   Total optimization steps = 188
[INFO|trainer.py:2525] 2025-10-03 09:54:18,864 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 09:54:18,864 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 09:54:18,864 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2530] 2025-10-03 09:54:18,864 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2531] 2025-10-03 09:54:18,864 >>   Total optimization steps = 188
[INFO|trainer.py:2523] 2025-10-03 09:54:18,864 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 09:54:18,864 >>   Num examples = 11,979
[INFO|trainer.py:2525] 2025-10-03 09:54:18,864 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 09:54:18,864 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 09:54:18,864 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2530] 2025-10-03 09:54:18,864 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2531] 2025-10-03 09:54:18,864 >>   Total optimization steps = 188
[INFO|trainer.py:2532] 2025-10-03 09:54:18,864 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2532] 2025-10-03 09:54:18,865 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2523] 2025-10-03 09:54:18,864 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 09:54:18,865 >>   Num examples = 11,979
[INFO|trainer.py:2525] 2025-10-03 09:54:18,865 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 09:54:18,865 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 09:54:18,865 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2523] 2025-10-03 09:54:18,865 >> ***** Running training *****
[INFO|trainer.py:2530] 2025-10-03 09:54:18,865 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2524] 2025-10-03 09:54:18,865 >>   Num examples = 11,979
[INFO|trainer.py:2531] 2025-10-03 09:54:18,865 >>   Total optimization steps = 188
[INFO|trainer.py:2525] 2025-10-03 09:54:18,865 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 09:54:18,865 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 09:54:18,865 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2530] 2025-10-03 09:54:18,865 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2531] 2025-10-03 09:54:18,865 >>   Total optimization steps = 188
[INFO|trainer.py:2532] 2025-10-03 09:54:18,865 >>   Number of trainable parameters = 20,914,757,184
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2532] 2025-10-03 09:54:18,865 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2523] 2025-10-03 09:54:18,865 >> ***** Running training *****
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2524] 2025-10-03 09:54:18,865 >>   Num examples = 11,979
[INFO|trainer.py:2525] 2025-10-03 09:54:18,865 >>   Num Epochs = 1
[INFO|trainer.py:2532] 2025-10-03 09:54:18,865 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2526] 2025-10-03 09:54:18,865 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 09:54:18,865 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2530] 2025-10-03 09:54:18,865 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2531] 2025-10-03 09:54:18,865 >>   Total optimization steps = 188
[INFO|trainer.py:2523] 2025-10-03 09:54:18,865 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 09:54:18,865 >>   Num examples = 11,979
[INFO|trainer.py:2525] 2025-10-03 09:54:18,865 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 09:54:18,865 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 09:54:18,866 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2530] 2025-10-03 09:54:18,866 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2531] 2025-10-03 09:54:18,866 >>   Total optimization steps = 188
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2532] 2025-10-03 09:54:18,866 >>   Number of trainable parameters = 20,914,757,184
[INFO|trainer.py:2532] 2025-10-03 09:54:18,866 >>   Number of trainable parameters = 20,914,757,184
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:2523] 2025-10-03 09:54:19,092 >> ***** Running training *****
[INFO|trainer.py:2524] 2025-10-03 09:54:19,092 >>   Num examples = 11,979
[INFO|trainer.py:2525] 2025-10-03 09:54:19,092 >>   Num Epochs = 1
[INFO|trainer.py:2526] 2025-10-03 09:54:19,092 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2529] 2025-10-03 09:54:19,092 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2530] 2025-10-03 09:54:19,092 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2531] 2025-10-03 09:54:19,092 >>   Total optimization steps = 188
[INFO|trainer.py:2532] 2025-10-03 09:54:19,093 >>   Number of trainable parameters = 20,914,757,184
  0%|          | 0/188 [00:00<?, ?it/s]  1%|          | 1/188 [00:32<1:42:28, 32.88s/it]  1%|          | 2/188 [00:57<1:27:34, 28.25s/it]  2%|▏         | 3/188 [01:22<1:21:47, 26.52s/it]  2%|▏         | 4/188 [01:43<1:14:23, 24.26s/it]  3%|▎         | 5/188 [02:03<1:09:40, 22.84s/it]  3%|▎         | 6/188 [02:30<1:14:04, 24.42s/it]  4%|▎         | 7/188 [02:53<1:11:57, 23.85s/it]  4%|▍         | 8/188 [03:12<1:06:23, 22.13s/it]  5%|▍         | 9/188 [03:33<1:04:57, 21.77s/it]  5%|▌         | 10/188 [03:54<1:04:02, 21.59s/it]                                                    5%|▌         | 10/188 [03:54<1:04:02, 21.59s/it]  6%|▌         | 11/188 [04:16<1:04:37, 21.90s/it]  6%|▋         | 12/188 [04:37<1:03:24, 21.62s/it]  7%|▋         | 13/188 [05:02<1:05:20, 22.40s/it]  7%|▋         | 14/188 [05:26<1:07:10, 23.17s/it]  8%|▊         | 15/188 [05:46<1:03:52, 22.15s/it]  9%|▊         | 16/188 [06:06<1:01:01, 21.29s/it]  9%|▉         | 17/188 [06:28<1:01:45, 21.67s/it] 10%|▉         | 18/188 [06:49<1:01:00, 21.53s/it] 10%|█         | 19/188 [07:10<1:00:15, 21.40s/it] 11%|█         | 20/188 [07:30<58:35, 20.93s/it]                                                   11%|█         | 20/188 [07:30<58:35, 20.93s/it] 11%|█         | 21/188 [07:50<57:33, 20.68s/it] 12%|█▏        | 22/188 [08:12<57:58, 20.95s/it] 12%|█▏        | 23/188 [08:34<58:40, 21.33s/it] 13%|█▎        | 24/188 [08:53<56:37, 20.71s/it] 13%|█▎        | 25/188 [09:13<55:36, 20.47s/it] 14%|█▍        | 26/188 [09:36<56:41, 21.00s/it] 14%|█▍        | 27/188 [09:57<57:01, 21.25s/it] 15%|█▍        | 28/188 [10:17<55:33, 20.84s/it] 15%|█▌        | 29/188 [10:39<55:38, 21.00s/it] 16%|█▌        | 30/188 [10:56<52:44, 20.03s/it]                                                 16%|█▌        | 30/188 [10:56<52:44, 20.03s/it] 16%|█▋        | 31/188 [11:20<55:01, 21.03s/it] 17%|█▋        | 32/188 [11:40<53:54, 20.73s/it] 18%|█▊        | 33/188 [11:59<52:17, 20.24s/it] 18%|█▊        | 34/188 [12:21<53:33, 20.87s/it] 19%|█▊        | 35/188 [12:43<53:49, 21.10s/it] 19%|█▉        | 36/188 [13:04<53:50, 21.25s/it] 20%|█▉        | 37/188 [13:30<56:55, 22.62s/it] 20%|██        | 38/188 [13:51<54:51, 21.94s/it] 21%|██        | 39/188 [14:11<53:21, 21.48s/it] 21%|██▏       | 40/188 [14:32<52:53, 21.44s/it]                                                 21%|██▏       | 40/188 [14:32<52:53, 21.44s/it] 22%|██▏       | 41/188 [14:54<52:23, 21.39s/it] 22%|██▏       | 42/188 [15:18<54:00, 22.20s/it] 23%|██▎       | 43/188 [15:39<52:42, 21.81s/it] 23%|██▎       | 44/188 [15:59<51:38, 21.51s/it] 24%|██▍       | 45/188 [16:21<51:30, 21.61s/it] 24%|██▍       | 46/188 [16:45<52:26, 22.16s/it] 25%|██▌       | 47/188 [17:04<50:06, 21.32s/it] 26%|██▌       | 48/188 [17:24<48:37, 20.84s/it] 26%|██▌       | 49/188 [17:44<47:31, 20.51s/it] 27%|██▋       | 50/188 [18:07<49:19, 21.45s/it]                                                 27%|██▋       | 50/188 [18:07<49:19, 21.45s/it] 27%|██▋       | 51/188 [18:28<48:21, 21.18s/it] 28%|██▊       | 52/188 [18:48<47:25, 20.92s/it] 28%|██▊       | 53/188 [19:10<47:31, 21.12s/it] 29%|██▊       | 54/188 [19:33<48:22, 21.66s/it] 29%|██▉       | 55/188 [19:54<47:36, 21.48s/it] 30%|██▉       | 56/188 [20:19<49:33, 22.53s/it] 30%|███       | 57/188 [20:38<46:57, 21.51s/it] 31%|███       | 58/188 [21:00<47:17, 21.82s/it] 31%|███▏      | 59/188 [21:20<45:42, 21.26s/it] 32%|███▏      | 60/188 [21:45<47:22, 22.21s/it]                                                 32%|███▏      | 60/188 [21:45<47:22, 22.21s/it] 32%|███▏      | 61/188 [22:07<46:51, 22.14s/it] 33%|███▎      | 62/188 [22:29<46:40, 22.23s/it] 34%|███▎      | 63/188 [22:48<44:28, 21.35s/it] 34%|███▍      | 64/188 [23:08<42:57, 20.79s/it] 35%|███▍      | 65/188 [23:30<43:15, 21.10s/it] 35%|███▌      | 66/188 [23:49<41:56, 20.63s/it] 36%|███▌      | 67/188 [24:10<41:41, 20.67s/it] 36%|███▌      | 68/188 [24:32<41:53, 20.95s/it] 37%|███▋      | 69/188 [24:52<40:57, 20.65s/it] 37%|███▋      | 70/188 [25:12<40:23, 20.54s/it]                                                 37%|███▋      | 70/188 [25:12<40:23, 20.54s/it] 38%|███▊      | 71/188 [25:32<39:32, 20.28s/it] 38%|███▊      | 72/188 [25:57<42:03, 21.75s/it] 39%|███▉      | 73/188 [26:21<43:02, 22.46s/it] 39%|███▉      | 74/188 [26:41<41:16, 21.72s/it] 40%|███▉      | 75/188 [27:02<40:40, 21.59s/it] 40%|████      | 76/188 [27:23<39:47, 21.32s/it] 41%|████      | 77/188 [27:42<38:13, 20.66s/it] 41%|████▏     | 78/188 [28:04<38:50, 21.19s/it] 42%|████▏     | 79/188 [28:24<37:29, 20.64s/it] 43%|████▎     | 80/188 [28:45<37:36, 20.89s/it]                                                 43%|████▎     | 80/188 [28:45<37:36, 20.89s/it] 43%|████▎     | 81/188 [29:10<39:09, 21.96s/it] 44%|████▎     | 82/188 [29:32<38:46, 21.94s/it] 44%|████▍     | 83/188 [29:55<39:10, 22.39s/it] 45%|████▍     | 84/188 [30:14<37:07, 21.42s/it] 45%|████▌     | 85/188 [30:35<36:19, 21.16s/it] 46%|████▌     | 86/188 [30:53<34:41, 20.41s/it] 46%|████▋     | 87/188 [31:15<35:02, 20.82s/it] 47%|████▋     | 88/188 [31:40<36:53, 22.14s/it] 47%|████▋     | 89/188 [32:00<35:23, 21.44s/it] 48%|████▊     | 90/188 [32:21<34:45, 21.28s/it]                                                 48%|████▊     | 90/188 [32:21<34:45, 21.28s/it] 48%|████▊     | 91/188 [32:41<33:48, 20.92s/it] 49%|████▉     | 92/188 [33:04<34:18, 21.44s/it] 49%|████▉     | 93/188 [33:22<32:29, 20.52s/it] 50%|█████     | 94/188 [33:41<31:29, 20.10s/it] 51%|█████     | 95/188 [34:02<31:14, 20.16s/it] 51%|█████     | 96/188 [34:22<31:05, 20.28s/it] 52%|█████▏    | 97/188 [34:44<31:36, 20.84s/it] 52%|█████▏    | 98/188 [35:05<31:06, 20.73s/it] 53%|█████▎    | 99/188 [35:27<31:20, 21.13s/it] 53%|█████▎    | 100/188 [35:51<32:12, 21.96s/it]                                                  53%|█████▎    | 100/188 [35:51<32:12, 21.96s/it][INFO|trainer.py:4289] 2025-10-03 10:30:22,252 >> Saving model checkpoint to /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-100
[INFO|configuration_utils.py:491] 2025-10-03 10:30:22,259 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-100/config.json
[INFO|configuration_utils.py:826] 2025-10-03 10:30:22,259 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-100/generation_config.json
[INFO|modeling_utils.py:4308] 2025-10-03 10:30:41,739 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-100/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2394] 2025-10-03 10:30:41,740 >> chat template saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-100/chat_template.jinja
[INFO|tokenization_utils_base.py:2563] 2025-10-03 10:30:41,741 >> tokenizer config file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2572] 2025-10-03 10:30:41,742 >> Special tokens file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-100/special_tokens_map.json
 54%|█████▎    | 101/188 [36:45<45:47, 31.58s/it] 54%|█████▍    | 102/188 [37:08<41:43, 29.11s/it] 55%|█████▍    | 103/188 [37:29<37:48, 26.69s/it] 55%|█████▌    | 104/188 [37:51<35:25, 25.30s/it] 56%|█████▌    | 105/188 [38:11<32:41, 23.63s/it] 56%|█████▋    | 106/188 [38:31<30:59, 22.68s/it] 57%|█████▋    | 107/188 [38:51<29:11, 21.62s/it] 57%|█████▋    | 108/188 [39:18<31:04, 23.31s/it] 58%|█████▊    | 109/188 [39:38<29:19, 22.27s/it] 59%|█████▊    | 110/188 [40:00<28:53, 22.23s/it]                                                  59%|█████▊    | 110/188 [40:00<28:53, 22.23s/it] 59%|█████▉    | 111/188 [40:21<27:57, 21.79s/it] 60%|█████▉    | 112/188 [40:47<29:11, 23.04s/it] 60%|██████    | 113/188 [41:09<28:35, 22.87s/it] 61%|██████    | 114/188 [41:31<27:52, 22.60s/it] 61%|██████    | 115/188 [41:50<26:06, 21.46s/it] 62%|██████▏   | 116/188 [42:11<25:48, 21.50s/it] 62%|██████▏   | 117/188 [42:33<25:28, 21.52s/it] 63%|██████▎   | 118/188 [42:55<25:15, 21.65s/it] 63%|██████▎   | 119/188 [43:15<24:29, 21.30s/it] 64%|██████▍   | 120/188 [43:35<23:37, 20.84s/it]                                                  64%|██████▍   | 120/188 [43:35<23:37, 20.84s/it] 64%|██████▍   | 121/188 [43:56<23:25, 20.98s/it] 65%|██████▍   | 122/188 [44:25<25:36, 23.28s/it] 65%|██████▌   | 123/188 [44:46<24:36, 22.72s/it] 66%|██████▌   | 124/188 [45:06<23:12, 21.76s/it] 66%|██████▋   | 125/188 [45:26<22:15, 21.21s/it] 67%|██████▋   | 126/188 [45:50<22:42, 21.98s/it] 68%|██████▊   | 127/188 [46:11<22:07, 21.76s/it] 68%|██████▊   | 128/188 [46:30<20:57, 20.96s/it] 69%|██████▊   | 129/188 [46:53<21:11, 21.55s/it] 69%|██████▉   | 130/188 [47:16<21:09, 21.89s/it]                                                  69%|██████▉   | 130/188 [47:16<21:09, 21.89s/it] 70%|██████▉   | 131/188 [47:39<21:04, 22.19s/it] 70%|███████   | 132/188 [47:59<20:10, 21.62s/it] 71%|███████   | 133/188 [48:20<19:48, 21.61s/it] 71%|███████▏  | 134/188 [48:40<18:48, 20.90s/it] 72%|███████▏  | 135/188 [48:59<18:08, 20.54s/it] 72%|███████▏  | 136/188 [49:21<18:08, 20.94s/it] 73%|███████▎  | 137/188 [49:40<17:11, 20.23s/it] 73%|███████▎  | 138/188 [50:03<17:31, 21.03s/it] 74%|███████▍  | 139/188 [50:24<17:07, 20.97s/it] 74%|███████▍  | 140/188 [50:46<17:03, 21.33s/it]                                                  74%|███████▍  | 140/188 [50:46<17:03, 21.33s/it] 75%|███████▌  | 141/188 [51:05<16:18, 20.82s/it] 76%|███████▌  | 142/188 [51:23<15:18, 19.96s/it] 76%|███████▌  | 143/188 [51:47<15:43, 20.96s/it] 77%|███████▋  | 144/188 [52:05<14:44, 20.10s/it] 77%|███████▋  | 145/188 [52:29<15:14, 21.28s/it] 78%|███████▊  | 146/188 [52:49<14:42, 21.00s/it] 78%|███████▊  | 147/188 [53:10<14:14, 20.85s/it] 79%|███████▊  | 148/188 [53:33<14:23, 21.59s/it] 79%|███████▉  | 149/188 [53:52<13:30, 20.77s/it] 80%|███████▉  | 150/188 [54:11<12:53, 20.36s/it]                                                  80%|███████▉  | 150/188 [54:11<12:53, 20.36s/it] 80%|████████  | 151/188 [54:29<12:06, 19.63s/it] 81%|████████  | 152/188 [54:54<12:40, 21.14s/it] 81%|████████▏ | 153/188 [55:12<11:52, 20.34s/it] 82%|████████▏ | 154/188 [55:33<11:32, 20.36s/it] 82%|████████▏ | 155/188 [55:53<11:14, 20.43s/it] 83%|████████▎ | 156/188 [56:21<12:04, 22.63s/it] 84%|████████▎ | 157/188 [56:41<11:13, 21.72s/it] 84%|████████▍ | 158/188 [57:05<11:17, 22.60s/it] 85%|████████▍ | 159/188 [57:29<11:03, 22.87s/it] 85%|████████▌ | 160/188 [57:53<10:55, 23.42s/it]                                                  85%|████████▌ | 160/188 [57:53<10:55, 23.42s/it] 86%|████████▌ | 161/188 [58:15<10:20, 23.00s/it] 86%|████████▌ | 162/188 [58:37<09:47, 22.59s/it] 87%|████████▋ | 163/188 [58:58<09:15, 22.22s/it] 87%|████████▋ | 164/188 [59:21<08:54, 22.26s/it] 88%|████████▊ | 165/188 [59:43<08:28, 22.12s/it] 88%|████████▊ | 166/188 [1:00:06<08:12, 22.40s/it] 89%|████████▉ | 167/188 [1:00:26<07:40, 21.92s/it] 89%|████████▉ | 168/188 [1:00:49<07:22, 22.15s/it] 90%|████████▉ | 169/188 [1:01:14<07:15, 22.90s/it] 90%|█████████ | 170/188 [1:01:32<06:24, 21.36s/it]                                                    90%|█████████ | 170/188 [1:01:32<06:24, 21.36s/it] 91%|█████████ | 171/188 [1:01:52<05:58, 21.06s/it] 91%|█████████▏| 172/188 [1:02:14<05:40, 21.27s/it] 92%|█████████▏| 173/188 [1:02:34<05:17, 21.14s/it] 93%|█████████▎| 174/188 [1:02:56<04:58, 21.34s/it] 93%|█████████▎| 175/188 [1:03:21<04:51, 22.43s/it] 94%|█████████▎| 176/188 [1:03:41<04:20, 21.71s/it] 94%|█████████▍| 177/188 [1:04:02<03:53, 21.26s/it] 95%|█████████▍| 178/188 [1:04:21<03:27, 20.79s/it] 95%|█████████▌| 179/188 [1:04:45<03:15, 21.69s/it] 96%|█████████▌| 180/188 [1:05:07<02:53, 21.65s/it]                                    [INFO|trainer.py:2808] 2025-10-03 11:02:23,938 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2808] 2025-10-03 11:02:23,938 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2808] 2025-10-03 11:02:23,938 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2808] 2025-10-03 11:02:23,938 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2808] 2025-10-03 11:02:23,938 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2808] 2025-10-03 11:02:23,938 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2808] 2025-10-03 11:02:23,938 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                96%|█████████▌| 180/188 [1:05:07<02:53, 21.65s/it] 96%|█████████▋| 181/188 [1:05:26<02:27, 21.07s/it] 97%|█████████▋| 182/188 [1:05:46<02:04, 20.77s/it] 97%|█████████▋| 183/188 [1:06:10<01:48, 21.63s/it] 98%|█████████▊| 184/188 [1:06:30<01:25, 21.27s/it] 98%|█████████▊| 185/188 [1:06:51<01:03, 21.13s/it] 99%|█████████▉| 186/188 [1:07:15<00:43, 21.94s/it] 99%|█████████▉| 187/188 [1:07:40<00:22, 22.98s/it]100%|██████████| 188/188 [1:07:53<00:00, 19.79s/it][INFO|trainer.py:4289] 2025-10-03 11:02:23,939 >> Saving model checkpoint to /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-188
[INFO|configuration_utils.py:491] 2025-10-03 11:02:23,942 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-188/config.json
[INFO|configuration_utils.py:826] 2025-10-03 11:02:23,943 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-188/generation_config.json
[INFO|modeling_utils.py:4308] 2025-10-03 11:02:43,973 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-188/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2394] 2025-10-03 11:02:43,973 >> chat template saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-188/chat_template.jinja
[INFO|tokenization_utils_base.py:2563] 2025-10-03 11:02:43,974 >> tokenizer config file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:2572] 2025-10-03 11:02:43,974 >> Special tokens file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/checkpoint-188/special_tokens_map.json
[INFO|trainer.py:2808] 2025-10-03 11:02:44,253 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 188/188 [1:08:25<00:00, 19.79s/it]100%|██████████| 188/188 [1:08:25<00:00, 21.84s/it]
[INFO|trainer.py:4289] 2025-10-03 11:02:55,801 >> Saving model checkpoint to /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|configuration_utils.py:491] 2025-10-03 11:02:55,806 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/config.json
[INFO|configuration_utils.py:826] 2025-10-03 11:02:55,807 >> Configuration saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/generation_config.json
[INFO|trainer.py:4623] 2025-10-03 11:02:55,807 >> 
***** Running Evaluation *****
[INFO|trainer.py:4625] 2025-10-03 11:02:55,807 >>   Num examples = 121
[INFO|trainer.py:4628] 2025-10-03 11:02:55,807 >>   Batch size = 1
[INFO|trainer.py:4623] 2025-10-03 11:02:55,808 >> 
***** Running Evaluation *****
[INFO|trainer.py:4625] 2025-10-03 11:02:55,808 >>   Num examples = 121
[INFO|trainer.py:4628] 2025-10-03 11:02:55,808 >>   Batch size = 1
[INFO|trainer.py:4623] 2025-10-03 11:02:55,809 >> 
***** Running Evaluation *****
[INFO|trainer.py:4625] 2025-10-03 11:02:55,809 >>   Num examples = 121
[INFO|trainer.py:4628] 2025-10-03 11:02:55,809 >>   Batch size = 1
[INFO|trainer.py:4623] 2025-10-03 11:02:55,809 >> 
***** Running Evaluation *****
[INFO|trainer.py:4625] 2025-10-03 11:02:55,810 >>   Num examples = 121
[INFO|trainer.py:4628] 2025-10-03 11:02:55,810 >>   Batch size = 1
[INFO|trainer.py:4623] 2025-10-03 11:02:55,811 >> 
***** Running Evaluation *****
[INFO|trainer.py:4625] 2025-10-03 11:02:55,811 >>   Num examples = 121
[INFO|trainer.py:4628] 2025-10-03 11:02:55,811 >>   Batch size = 1
[INFO|trainer.py:4623] 2025-10-03 11:02:55,811 >> 
***** Running Evaluation *****
[INFO|trainer.py:4625] 2025-10-03 11:02:55,811 >>   Num examples = 121
[INFO|trainer.py:4628] 2025-10-03 11:02:55,811 >>   Batch size = 1
[INFO|trainer.py:4623] 2025-10-03 11:02:55,811 >> 
***** Running Evaluation *****
[INFO|trainer.py:4625] 2025-10-03 11:02:55,812 >>   Num examples = 121
[INFO|trainer.py:4628] 2025-10-03 11:02:55,812 >>   Batch size = 1
[INFO|modeling_utils.py:4308] 2025-10-03 11:03:14,600 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2394] 2025-10-03 11:03:14,601 >> chat template saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/chat_template.jinja
[INFO|tokenization_utils_base.py:2563] 2025-10-03 11:03:14,602 >> tokenizer config file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/tokenizer_config.json
[INFO|tokenization_utils_base.py:2572] 2025-10-03 11:03:14,602 >> Special tokens file saved in /scratch/gpfs/CHIJ/yong/trained_models/train_dxy_gpt-oss-20b-bf16_lr5.0e-5_cosine_dxy_full_finetune/special_tokens_map.json
/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 40, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[INFO|trainer.py:4623] 2025-10-03 11:03:15,236 >> 
***** Running Evaluation *****
[INFO|trainer.py:4625] 2025-10-03 11:03:15,236 >>   Num examples = 121
[INFO|trainer.py:4628] 2025-10-03 11:03:15,236 >>   Batch size = 1
[rank22]: Traceback (most recent call last):
[rank22]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/train.py", line 27, in <module>
[rank22]:     main()
[rank22]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/train.py", line 18, in main
[rank22]:     run_exp()
[rank22]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank22]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank22]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank22]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank22]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/sft/workflow.py", line 122, in run_sft
[rank22]:     metrics = trainer.evaluate(metric_key_prefix="eval", **gen_kwargs)
[rank22]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
[rank22]:     return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 4469, in evaluate
[rank22]:     output = eval_loop(
[rank22]:              ^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 4665, in evaluation_loop
[rank22]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank22]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/sft/trainer.py", line 137, in prediction_step
[rank22]:     loss, generated_tokens, _ = super().prediction_step(
[rank22]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 289, in prediction_step
[rank22]:     return super().prediction_step(
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 4881, in prediction_step
[rank22]:     loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
[rank22]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/sft/trainer.py", line 117, in compute_loss
[rank22]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 4099, in compute_loss
[rank22]:     outputs = model(**inputs)
[rank22]:               ^^^^^^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank22]:     return self._call_impl(*args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank22]:     return inner()
[rank22]:            ^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank22]:     result = forward_call(*args, **kwargs)
[rank22]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/Liger-Kernel/src/liger_kernel/transformers/model/gpt_oss.py", line 92, in lce_forward
[rank22]:     loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 67, in ForCausalLMLoss
[rank22]:     loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 36, in fixed_cross_entropy
[rank22]:     loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/nn/functional.py", line 3462, in cross_entropy
[rank22]:     return torch._C._nn.cross_entropy_loss(
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.51 GiB. GPU 2 has a total capacity of 79.18 GiB of which 23.62 GiB is free. Including non-PyTorch memory, this process has 55.53 GiB memory in use. Of the allocated memory 46.77 GiB is allocated by PyTorch, and 5.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank24]: Traceback (most recent call last):
[rank24]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/train.py", line 27, in <module>
[rank24]:     main()
[rank24]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/train.py", line 18, in main
[rank24]:     run_exp()
[rank24]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank24]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank24]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank24]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank24]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/sft/workflow.py", line 122, in run_sft
[rank24]:     metrics = trainer.evaluate(metric_key_prefix="eval", **gen_kwargs)
[rank24]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
[rank24]:     return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 4469, in evaluate
[rank24]:     output = eval_loop(
[rank24]:              ^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 4665, in evaluation_loop
[rank24]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank24]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/sft/trainer.py", line 137, in prediction_step
[rank24]:     loss, generated_tokens, _ = super().prediction_step(
[rank24]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer_seq2seq.py", line 289, in prediction_step
[rank24]:     return super().prediction_step(
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 4881, in prediction_step
[rank24]:     loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
[rank24]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/src/llamafactory/train/sft/trainer.py", line 117, in compute_loss
[rank24]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/trainer.py", line 4099, in compute_loss
[rank24]:     outputs = model(**inputs)
[rank24]:               ^^^^^^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank24]:     return self._call_impl(*args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank24]:     return inner()
[rank24]:            ^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank24]:     result = forward_call(*args, **kwargs)
[rank24]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/projects/LLaMA-Factory-new/Liger-Kernel/src/liger_kernel/transformers/model/gpt_oss.py", line 92, in lce_forward
[rank24]:     loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 67, in ForCausalLMLoss
[rank24]:     loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 36, in fixed_cross_entropy
[rank24]:     loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/nn/functional.py", line 3462, in cross_entropy
[rank24]:     return torch._C._nn.cross_entropy_loss(
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.30 GiB. GPU 0 has a total capacity of 79.18 GiB of which 15.14 GiB is free. Including non-PyTorch memory, this process has 64.01 GiB memory in use. Of the allocated memory 60.01 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W1003 11:03:31.308000 1143917 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1143974 closing signal SIGTERM
W1003 11:03:31.310000 1143917 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1143975 closing signal SIGTERM
W1003 11:03:31.311000 1143917 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1143977 closing signal SIGTERM
E1003 11:03:31.406000 1143917 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 2 (pid: 1143976) of binary: /scratch/gpfs/yl7690/.conda/envs/oss/bin/python3.1
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
src/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-03_11:03:31
  host      : della-j16g3
  rank      : 22 (local_rank: 2)
  exitcode  : 1 (pid: 1143976)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W1003 11:03:32.735000 3091358 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3091411 closing signal SIGTERM
W1003 11:03:32.738000 3091358 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3091412 closing signal SIGTERM
W1003 11:03:32.739000 3091358 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3091413 closing signal SIGTERM
E1003 11:03:32.823000 3091358 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 3091410) of binary: /scratch/gpfs/yl7690/.conda/envs/oss/bin/python3.1
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
src/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-03_11:03:32
  host      : della-j17g1
  rank      : 24 (local_rank: 0)
  exitcode  : 1 (pid: 3091410)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: della-j16g3: task 5: Exited with exit code 1
srun: Terminating StepId=1156417.0
slurmstepd: error: *** STEP 1156417.0 ON della-j12g2 CANCELLED AT 2025-10-03T11:03:34 ***
W1003 11:03:34.830000 2365920 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 11:03:34.831000 2365920 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2365984 closing signal SIGTERM
W1003 11:03:34.830000 2847256 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 11:03:34.833000 2365920 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2365985 closing signal SIGTERM
W1003 11:03:34.830000 3196428 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 11:03:34.831000 2289967 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 11:03:34.830000 1562569 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 11:03:34.830000 3677347 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1003 11:03:34.833000 3196428 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3196477 closing signal SIGTERM
W1003 11:03:34.833000 2847256 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2847310 closing signal SIGTERM
W1003 11:03:34.833000 2289967 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2290018 closing signal SIGTERM
W1003 11:03:34.833000 1562569 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1562618 closing signal SIGTERM
W1003 11:03:34.833000 3677347 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3677398 closing signal SIGTERM
W1003 11:03:34.835000 3196428 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3196478 closing signal SIGTERM
W1003 11:03:34.835000 2847256 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2847311 closing signal SIGTERM
W1003 11:03:34.834000 1562569 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1562619 closing signal SIGTERM
W1003 11:03:34.835000 1562569 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1562620 closing signal SIGTERM
W1003 11:03:34.836000 1562569 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1562621 closing signal SIGTERM
W1003 11:03:34.847000 2365920 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2365986 closing signal SIGTERM
W1003 11:03:34.849000 3677347 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3677399 closing signal SIGTERM
W1003 11:03:34.856000 2847256 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2847312 closing signal SIGTERM
W1003 11:03:34.912000 2289967 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2290019 closing signal SIGTERM
W1003 11:03:34.913000 2289967 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2290020 closing signal SIGTERM
W1003 11:03:34.923000 3196428 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3196479 closing signal SIGTERM
W1003 11:03:34.926000 3196428 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3196480 closing signal SIGTERM
W1003 11:03:34.937000 2365920 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2365987 closing signal SIGTERM
W1003 11:03:34.938000 2289967 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2290021 closing signal SIGTERM
W1003 11:03:34.939000 2847256 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2847313 closing signal SIGTERM
W1003 11:03:34.950000 3677347 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3677400 closing signal SIGTERM
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2847256 got signal: 15
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2365920 got signal: 15
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1562569 got signal: 15
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
W1003 11:03:35.036000 3677347 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3677401 closing signal SIGTERM
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3196428 got signal: 15
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2289967 got signal: 15
Traceback (most recent call last):
  File "/scratch/gpfs/yl7690/.conda/envs/oss/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/scratch/gpfs/yl7690/.conda/envs/oss/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3677347 got signal: 15
srun: error: della-j17g1: task 6: Exited with exit code 1
srun: error: della-j16g1: task 4: Exited with exit code 1
srun: error: della-j12g2: task 0: Exited with exit code 1
srun: error: della-k11g1: task 7: Exited with exit code 1
srun: error: della-j15g3: task 3: Exited with exit code 1
srun: error: della-j15g2: task 2: Exited with exit code 1
srun: error: della-j15g1: task 1: Exited with exit code 1
