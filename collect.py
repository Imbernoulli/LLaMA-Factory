import os
import json
from typing import Dict, List, Any
from jload import jsave

all_train = []

def fix_code_block_ending(text):
    if text.endswith('```') and not text.endswith('\n```'):
        text = text[:-3] + '\n```'
    return text

def extract_last_lean4_block(text: str) -> tuple[str, str]:
    marker = "```lean4"
    start_positions = []
    pos = 0

    while True:
        pos = text.find(marker, pos)
        if pos == -1:
            break
        start_positions.append(pos)
        pos += len(marker)

    if not start_positions:
        return (text, "")

    last_start = start_positions[-1]

    end_marker = "```"
    search_from = last_start + len(marker)
    end_pos = text.find(end_marker, search_from)

    if end_pos == -1:
        lean4_block = text[last_start:]
    else:
        lean4_block = text[last_start:end_pos + len(end_marker)]

    before_content = text[:last_start].strip()

    before_lines = before_content.splitlines()
    if before_lines and before_lines[-1].strip().startswith('#'):
        before_lines = before_lines[:-1]
    before_content = '\n'.join(before_lines)

    return before_content, fix_code_block_ending(lean4_block.strip())

def process_data(
    data1: List[Dict[str, Any]],
    data2: Dict[str, Any],
    metadata: Dict[str, str]
) -> None:
    print("â”€" * 50)
    print(f"ğŸš€ é–‹å§‹è™•ç†ä¸€çµ„æ–°çš„æ•¸æ“š...")
    print(f"   - ä¾†æºæª”æ¡ˆ 1 (data1): {metadata['code_compilation_path']}")
    print(f"   - ä¾†æºæª”æ¡ˆ 2 (data2): {metadata['full_records_path']}")
    
    
    correct_names = set()
    for d1 in data1:
        if d1["compilation_result"]["pass"] and d1["compilation_result"]["complete"]:
            correct_names.add(d1["name"])
    for d2 in data2:
        if d2["problem_id"] in correct_names:
            train_entry = d2["messages_history_for_this_attempt"]
            train_entry.append({
                "role": "assistant",
                "content": d2["model_output"]
            })
            for t in train_entry:
                if t["role"] is "assistant":
                    a, b = extract_last_lean4_block(t["content"])
                    t["content"] = f"<think>\n{a}\n</think>\n{b}"
            all_train.append({"messages": train_entry})

def find_and_load_correction_files(base_dir: str) -> None:
    """
    éæ­·æŒ‡å®šçš„åŸºæœ¬ç›®éŒ„ï¼Œå°‹æ‰¾æ‰€æœ‰ _corr* æ–‡ä»¶å°ä¸¦é€²è¡Œè™•ç†ã€‚

    Args:
        base_dir (str): è¦æœç´¢çš„æ ¹ç›®éŒ„è·¯å¾‘ã€‚
    """
    if not os.path.isdir(base_dir):
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ç›®éŒ„ '{base_dir}'")
        return

    print(f"æ­£åœ¨æƒæç›®éŒ„: {base_dir}\n")
    
    # os.walk æœƒéæ­¸åœ°éæ­·æ‰€æœ‰å­ç›®éŒ„
    for root, _, files in os.walk(base_dir):
        # æ‰¾å‡ºæ‰€æœ‰ code_compilation çš„ä¿®æ­£æª”æ¡ˆ
        code_files = [f for f in files if f.startswith('code_compilation_repl') and f.endswith('.json')]
        
        if not code_files:
            continue

        for code_file in code_files:
            # å¾ 'code_compilation_repl_corr1.json' ä¸­æå–å¾Œç¶´ '_corr1.json'
            suffix = code_file.replace('code_compilation_repl', '')
            
            # æ§‹å»ºå°æ‡‰çš„ full_records æª”æ¡ˆå
            records_file = f'full_records{suffix}'
            
            # æª¢æŸ¥é…å°çš„æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            if records_file in files:
                code_file_path = os.path.join(root, code_file)
                records_file_path = os.path.join(root, records_file)
                
                try:
                    # è®€å– data1
                    with open(code_file_path, 'r', encoding='utf-8') as f1:
                        data1 = json.load(f1)
                    
                    # è®€å– data2
                    with open(records_file_path, 'r', encoding='utf-8') as f2:
                        data2 = json.load(f2)

                    # æº–å‚™å…ƒæ•¸æ“š
                    metadata = {
                        "base_directory": root,
                        "code_compilation_path": code_file_path,
                        "full_records_path": records_file_path,
                        "correction_suffix": suffix
                    }
                    
                    # å°‡å…©å€‹æ•¸æ“šå‚³éçµ¦è™•ç†å‡½æ•¸
                    process_data(data1, data2, metadata)
                
                except json.JSONDecodeError as e:
                    print(f"JSON è§£ç¢¼éŒ¯èª¤ï¼Œæª”æ¡ˆ: {code_file_path} æˆ– {records_file_path} -> {e}")
                except Exception as e:
                    print(f"è®€å–æˆ–è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")


if __name__ == "__main__":
    base_directory = "/scratch/gpfs/yl7690/projects/DeepSeek-Prover-V1.5/results_unified/OMR_statements_iter1_filtered"
    
    find_and_load_correction_files(base_directory)
    
    jsave(all_train, "omni32_train.jsonl")